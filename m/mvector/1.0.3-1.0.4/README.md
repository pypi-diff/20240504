# Comparing `tmp/mvector-1.0.3-py3-none-any.whl.zip` & `tmp/mvector-1.0.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,34 +1,34 @@
-Zip file size: 61126 bytes, number of entries: 32
--rw-rw-rw-  2.0 fat      134 b- defN 24-May-03 03:44 mvector/__init__.py
--rw-rw-rw-  2.0 fat    17513 b- defN 24-May-02 06:53 mvector/predict.py
--rw-rw-rw-  2.0 fat    38106 b- defN 24-May-02 07:13 mvector/trainer.py
+Zip file size: 61531 bytes, number of entries: 32
+-rw-rw-rw-  2.0 fat      148 b- defN 24-May-04 05:15 mvector/__init__.py
+-rw-rw-rw-  2.0 fat    17697 b- defN 24-May-03 14:21 mvector/predict.py
+-rw-rw-rw-  2.0 fat    38271 b- defN 24-May-03 14:22 mvector/trainer.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Jan-06 05:59 mvector/data_utils/__init__.py
 -rw-rw-rw-  2.0 fat    22220 b- defN 24-Apr-27 05:32 mvector/data_utils/audio.py
 -rw-rw-rw-  2.0 fat      930 b- defN 24-May-02 05:56 mvector/data_utils/collate_fn.py
 -rw-rw-rw-  2.0 fat     3696 b- defN 24-May-02 06:00 mvector/data_utils/featurizer.py
 -rw-rw-rw-  2.0 fat     7407 b- defN 24-May-02 07:33 mvector/data_utils/reader.py
 -rw-rw-rw-  2.0 fat     1572 b- defN 24-Jan-06 05:59 mvector/data_utils/spec_aug.py
 -rw-rw-rw-  2.0 fat     4712 b- defN 24-Jan-06 05:59 mvector/data_utils/utils.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Jan-06 05:59 mvector/metric/__init__.py
 -rw-rw-rw-  2.0 fat     1517 b- defN 24-Jan-06 05:59 mvector/metric/metrics.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Jan-06 05:59 mvector/models/__init__.py
 -rw-rw-rw-  2.0 fat    13511 b- defN 24-Jan-06 05:59 mvector/models/campplus.py
 -rw-rw-rw-  2.0 fat     5293 b- defN 24-Jan-06 05:59 mvector/models/ecapa_tdnn.py
--rw-rw-rw-  2.0 fat    10340 b- defN 24-Jan-06 05:59 mvector/models/eres2net.py
+-rw-rw-rw-  2.0 fat    18261 b- defN 24-May-03 14:46 mvector/models/eres2net.py
 -rw-rw-rw-  2.0 fat     3641 b- defN 24-Jan-10 12:54 mvector/models/fc.py
 -rw-rw-rw-  2.0 fat     9448 b- defN 24-Jan-10 12:54 mvector/models/loss.py
--rw-rw-rw-  2.0 fat     3795 b- defN 24-Jan-06 05:59 mvector/models/pooling.py
+-rw-rw-rw-  2.0 fat     3799 b- defN 24-May-03 14:23 mvector/models/pooling.py
 -rw-rw-rw-  2.0 fat     6687 b- defN 24-Jan-06 05:59 mvector/models/res2net.py
 -rw-rw-rw-  2.0 fat     5522 b- defN 24-Jan-06 05:59 mvector/models/resnet_se.py
 -rw-rw-rw-  2.0 fat     2963 b- defN 24-Jan-06 05:59 mvector/models/tdnn.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Jan-06 05:59 mvector/utils/__init__.py
 -rw-rw-rw-  2.0 fat     2839 b- defN 24-Jan-06 05:59 mvector/utils/logger.py
 -rw-rw-rw-  2.0 fat     1385 b- defN 24-Jan-15 12:44 mvector/utils/record.py
 -rw-rw-rw-  2.0 fat     3538 b- defN 24-Jan-06 05:59 mvector/utils/scheduler.py
 -rw-rw-rw-  2.0 fat     2789 b- defN 24-Jan-06 05:59 mvector/utils/utils.py
--rw-rw-rw-  2.0 fat    11558 b- defN 24-May-03 03:44 mvector-1.0.3.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    33611 b- defN 24-May-03 03:44 mvector-1.0.3.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-May-03 03:44 mvector-1.0.3.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        8 b- defN 24-May-03 03:44 mvector-1.0.3.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     2595 b- defN 24-May-03 03:44 mvector-1.0.3.dist-info/RECORD
-32 files, 217422 bytes uncompressed, 56998 bytes compressed:  73.8%
+-rw-rw-rw-  2.0 fat    11558 b- defN 24-May-04 05:15 mvector-1.0.4.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    33928 b- defN 24-May-04 05:15 mvector-1.0.4.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-May-04 05:15 mvector-1.0.4.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        8 b- defN 24-May-04 05:15 mvector-1.0.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     2595 b- defN 24-May-04 05:15 mvector-1.0.4.dist-info/RECORD
+32 files, 226027 bytes uncompressed, 57403 bytes compressed:  74.6%
```

## zipnote {}

```diff
@@ -75,23 +75,23 @@
 
 Filename: mvector/utils/scheduler.py
 Comment: 
 
 Filename: mvector/utils/utils.py
 Comment: 
 
-Filename: mvector-1.0.3.dist-info/LICENSE
+Filename: mvector-1.0.4.dist-info/LICENSE
 Comment: 
 
-Filename: mvector-1.0.3.dist-info/METADATA
+Filename: mvector-1.0.4.dist-info/METADATA
 Comment: 
 
-Filename: mvector-1.0.3.dist-info/WHEEL
+Filename: mvector-1.0.4.dist-info/WHEEL
 Comment: 
 
-Filename: mvector-1.0.3.dist-info/top_level.txt
+Filename: mvector-1.0.4.dist-info/top_level.txt
 Comment: 
 
-Filename: mvector-1.0.3.dist-info/RECORD
+Filename: mvector-1.0.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## mvector/__init__.py

```diff
@@ -1,3 +1,3 @@
-__version__ = "1.0.3"
+__version__ = "1.0.4"
 # 项目支持的模型
-SUPPORT_MODEL = ['ERes2Net', 'CAMPPlus', 'EcapaTdnn', 'Res2Net', 'ResNetSE', 'TDNN']
+SUPPORT_MODEL = ['ERes2Net', 'ERes2NetV2', 'CAMPPlus', 'EcapaTdnn', 'Res2Net', 'ResNetSE', 'TDNN']
```

## mvector/predict.py

```diff
@@ -11,15 +11,15 @@
 from tqdm import tqdm
 
 from mvector import SUPPORT_MODEL
 from mvector.data_utils.audio import AudioSegment
 from mvector.data_utils.featurizer import AudioFeaturizer
 from mvector.models.campplus import CAMPPlus
 from mvector.models.ecapa_tdnn import EcapaTdnn
-from mvector.models.eres2net import ERes2Net
+from mvector.models.eres2net import ERes2Net, ERes2NetV2
 from mvector.models.res2net import Res2Net
 from mvector.models.resnet_se import ResNetSE
 from mvector.models.tdnn import TDNN
 from mvector.utils.logger import setup_logger
 from mvector.utils.utils import dict_to_object, print_arguments
 
 logger = setup_logger(__name__)
@@ -57,14 +57,16 @@
         self.configs = dict_to_object(configs)
         assert self.configs.use_model in SUPPORT_MODEL, f'没有该模型：{self.configs.use_model}'
         self._audio_featurizer = AudioFeaturizer(feature_method=self.configs.preprocess_conf.feature_method,
                                                  method_args=self.configs.preprocess_conf.get('method_args', {}))
         # 获取模型
         if self.configs.use_model == 'ERes2Net':
             backbone = ERes2Net(input_size=self._audio_featurizer.feature_dim, **self.configs.model_conf.backbone)
+        elif self.configs.use_model == 'ERes2NetV2':
+            backbone = ERes2NetV2(input_size=self._audio_featurizer.feature_dim, **self.configs.model_conf.backbone)
         elif self.configs.use_model == 'CAMPPlus':
             backbone = CAMPPlus(input_size=self._audio_featurizer.feature_dim, **self.configs.model_conf.backbone)
         elif self.configs.use_model == 'EcapaTdnn':
             backbone = EcapaTdnn(input_size=self._audio_featurizer.feature_dim, **self.configs.model_conf.backbone)
         elif self.configs.use_model == 'Res2Net':
             backbone = Res2Net(input_size=self._audio_featurizer.feature_dim, **self.configs.model_conf.backbone)
         elif self.configs.use_model == 'ResNetSE':
```

## mvector/trainer.py

```diff
@@ -22,15 +22,15 @@
 from mvector.data_utils.collate_fn import collate_fn
 from mvector.data_utils.featurizer import AudioFeaturizer
 from mvector.data_utils.reader import MVectorDataset
 from mvector.data_utils.spec_aug import SpecAug
 from mvector.metric.metrics import compute_fnr_fpr, compute_eer, compute_dcf, accuracy
 from mvector.models.campplus import CAMPPlus
 from mvector.models.ecapa_tdnn import EcapaTdnn
-from mvector.models.eres2net import ERes2Net
+from mvector.models.eres2net import ERes2Net, ERes2NetV2
 from mvector.models.fc import SpeakerIdentification
 from mvector.models.loss import AAMLoss, CELoss, AMLoss, ARMLoss, SubCenterLoss, SphereFace2
 from mvector.models.res2net import Res2Net
 from mvector.models.resnet_se import ResNetSE
 from mvector.models.tdnn import TDNN
 from mvector.utils.logger import setup_logger
 from mvector.utils.scheduler import WarmupCosineSchedulerLR, MarginScheduler
@@ -169,14 +169,16 @@
                     f.write(f'{save_path}\t{label}\n')
             logger.info(f'{data_list}列表中的数据已提取特征完成，新列表为：{save_data_list}')
 
     def __setup_model(self, input_size, is_train=False):
         # 获取模型
         if self.configs.use_model == 'ERes2Net':
             self.backbone = ERes2Net(input_size=input_size, **self.configs.model_conf.backbone)
+        elif self.configs.use_model == 'ERes2NetV2':
+            self.backbone = ERes2NetV2(input_size=input_size, **self.configs.model_conf.backbone)
         elif self.configs.use_model == 'CAMPPlus':
             self.backbone = CAMPPlus(input_size=input_size, **self.configs.model_conf.backbone)
         elif self.configs.use_model == 'EcapaTdnn':
             self.backbone = EcapaTdnn(input_size=input_size, **self.configs.model_conf.backbone)
         elif self.configs.use_model == 'Res2Net':
             self.backbone = Res2Net(input_size=input_size, **self.configs.model_conf.backbone)
         elif self.configs.use_model == 'ResNetSE':
```

## mvector/models/eres2net.py

```diff
@@ -1,14 +1,16 @@
 import math
 
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
-from mvector.models.pooling import TemporalAveragePooling, TemporalStatsPool
+from mvector.models.pooling import TemporalAveragePooling, TemporalStatsPool, AttentiveStatsPool
+
+__all__ = ['ERes2Net', 'ERes2NetV2']
 
 
 class ReLU(nn.Hardtanh):
 
     def __init__(self, inplace=False):
         super(ReLU, self).__init__(0, 20, inplace)
 
@@ -77,16 +79,14 @@
                 nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),
                 nn.BatchNorm2d(self.expansion * planes))
         self.stride = stride
         self.width = width
         self.scale = scale
 
     def forward(self, x):
-        residual = x
-
         out = self.conv1(x)
         out = self.bn1(out)
         out = self.relu(out)
         spx = torch.split(out, self.width, 1)
         for i in range(self.nums):
             if i == 0:
                 sp = spx[i]
@@ -94,15 +94,14 @@
                 sp = sp + spx[i]
             sp = self.convs[i](sp)
             sp = self.relu(self.bns[i](sp))
             if i == 0:
                 out = sp
             else:
                 out = torch.cat((out, sp), 1)
-
         out = self.conv3(out)
         out = self.bn3(out)
 
         residual = self.shortcut(x)
         out += residual
         out = self.relu(out)
 
@@ -142,33 +141,29 @@
                 nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),
                 nn.BatchNorm2d(self.expansion * planes))
         self.stride = stride
         self.width = width
         self.scale = scale
 
     def forward(self, x):
-        residual = x
-
         out = self.conv1(x)
         out = self.bn1(out)
         out = self.relu(out)
         spx = torch.split(out, self.width, 1)
         for i in range(self.nums):
             if i == 0:
                 sp = spx[i]
             else:
                 sp = self.fuse_models[i - 1](sp, spx[i])
-
             sp = self.convs[i](sp)
             sp = self.relu(self.bns[i](sp))
             if i == 0:
                 out = sp
             else:
                 out = torch.cat((out, sp), 1)
-
         out = self.conv3(out)
         out = self.bn3(out)
 
         residual = self.shortcut(x)
         out += residual
         out = self.relu(out)
 
@@ -267,9 +262,211 @@
 
         embed_a = self.seg_1(stats)
         if self.two_emb_layer:
             out = F.relu(embed_a)
             out = self.seg_bn_1(out)
             embed_b = self.seg_2(out)
             return embed_b
+        else:
+            return embed_a
+
+
+class BasicBlockERes2NetV2(nn.Module):
+
+    def __init__(self, expansion, in_planes, planes, stride=1, base_width=26, scale=2):
+        super(BasicBlockERes2NetV2, self).__init__()
+        self.expansion = expansion
+        width = int(math.floor(planes * (base_width / 64.0)))
+        self.conv1 = nn.Conv2d(in_planes, width * scale, kernel_size=1, stride=stride, bias=False)
+        self.bn1 = nn.BatchNorm2d(width * scale)
+        self.nums = scale
+
+        convs = []
+        bns = []
+        for i in range(self.nums):
+            convs.append(nn.Conv2d(width, width, kernel_size=3, padding=1, bias=False))
+            bns.append(nn.BatchNorm2d(width))
+        self.convs = nn.ModuleList(convs)
+        self.bns = nn.ModuleList(bns)
+        self.relu = ReLU(inplace=True)
+
+        self.conv3 = nn.Conv2d(width * scale, planes * self.expansion, kernel_size=1, bias=False)
+        self.bn3 = nn.BatchNorm2d(planes * self.expansion)
+        self.shortcut = nn.Sequential()
+        if stride != 1 or in_planes != self.expansion * planes:
+            self.shortcut = nn.Sequential(
+                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),
+                nn.BatchNorm2d(self.expansion * planes))
+        self.stride = stride
+        self.width = width
+        self.scale = scale
+
+    def forward(self, x):
+        out = self.conv1(x)
+        out = self.bn1(out)
+        out = self.relu(out)
+        spx = torch.split(out, self.width, 1)
+        for i in range(self.nums):
+            if i == 0:
+                sp = spx[i]
+            else:
+                sp = sp + spx[i]
+            sp = self.convs[i](sp)
+            sp = self.relu(self.bns[i](sp))
+            if i == 0:
+                out = sp
+            else:
+                out = torch.cat((out, sp), 1)
+        out = self.conv3(out)
+        out = self.bn3(out)
+
+        residual = self.shortcut(x)
+        out += residual
+        out = self.relu(out)
+
+        return out
+
+
+class BasicBlockERes2NetV2_AFF(nn.Module):
+
+    def __init__(self, expansion, in_planes, planes, stride=1, base_width=26, scale=2):
+        super(BasicBlockERes2NetV2_AFF, self).__init__()
+        self.expansion = expansion
+        width = int(math.floor(planes * (base_width / 64.0)))
+        self.conv1 = nn.Conv2d(in_planes, width * scale, kernel_size=1, stride=stride, bias=False)
+        self.bn1 = nn.BatchNorm2d(width * scale)
+        self.nums = scale
+
+        convs = []
+        fuse_models = []
+        bns = []
+        for i in range(self.nums):
+            convs.append(nn.Conv2d(width, width, kernel_size=3, padding=1, bias=False))
+            bns.append(nn.BatchNorm2d(width))
+        for j in range(self.nums - 1):
+            fuse_models.append(AFF(channels=width, r=4))
+
+        self.convs = nn.ModuleList(convs)
+        self.bns = nn.ModuleList(bns)
+        self.fuse_models = nn.ModuleList(fuse_models)
+        self.relu = ReLU(inplace=True)
+
+        self.conv3 = nn.Conv2d(width * scale, planes * self.expansion, kernel_size=1, bias=False)
+        self.bn3 = nn.BatchNorm2d(planes * self.expansion)
+        self.shortcut = nn.Sequential()
+        if stride != 1 or in_planes != self.expansion * planes:
+            self.shortcut = nn.Sequential(
+                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),
+                nn.BatchNorm2d(self.expansion * planes))
+        self.stride = stride
+        self.width = width
+        self.scale = scale
+
+    def forward(self, x):
+        out = self.conv1(x)
+        out = self.bn1(out)
+        out = self.relu(out)
+        spx = torch.split(out, self.width, 1)
+        for i in range(self.nums):
+            if i == 0:
+                sp = spx[i]
+            else:
+                sp = self.fuse_models[i - 1](sp, spx[i])
+            sp = self.convs[i](sp)
+            sp = self.relu(self.bns[i](sp))
+            if i == 0:
+                out = sp
+            else:
+                out = torch.cat((out, sp), 1)
+        out = self.conv3(out)
+        out = self.bn3(out)
+
+        residual = self.shortcut(x)
+        out += residual
+        out = self.relu(out)
+
+        return out
+
+
+class ERes2NetV2(nn.Module):
+    def __init__(self,
+                 input_size,
+                 block=BasicBlockERes2NetV2,
+                 block_fuse=BasicBlockERes2NetV2_AFF,
+                 num_blocks=[3, 4, 6, 3],
+                 m_channels=32,
+                 expansion=2,
+                 base_width=26,
+                 scale=2,
+                 embd_dim=192,
+                 pooling_type='TSTP',
+                 two_emb_layer=False):
+        super(ERes2NetV2, self).__init__()
+        self.in_planes = m_channels
+        self.expansion = expansion
+        self.embd_dim = embd_dim
+        self.stats_dim = int(input_size / 8) * m_channels * 8
+        self.two_emb_layer = two_emb_layer
+
+        self.conv1 = nn.Conv2d(1, m_channels, kernel_size=3, stride=1, padding=1, bias=False)
+        self.bn1 = nn.BatchNorm2d(m_channels)
+        self.layer1 = self._make_layer(block, m_channels, num_blocks[0], stride=1,
+                                       base_width=base_width, scale=scale)
+        self.layer2 = self._make_layer(block, m_channels * 2, num_blocks[1], stride=2,
+                                       base_width=base_width, scale=scale)
+        self.layer3 = self._make_layer(block_fuse, m_channels * 4, num_blocks[2], stride=2,
+                                       base_width=base_width, scale=scale)
+        self.layer4 = self._make_layer(block_fuse, m_channels * 8, num_blocks[3], stride=2,
+                                       base_width=base_width, scale=scale)
+
+        # Downsampling module
+        self.layer3_ds = nn.Conv2d(m_channels * 8, m_channels * 16, kernel_size=3, padding=1, stride=2, bias=False)
+
+        # Bottom-up fusion module
+        self.fuse34 = AFF(channels=m_channels * 16, r=4)
+
+        self.n_stats = 1 if pooling_type == 'TAP' else 2
+        if pooling_type == "TAP":
+            self.pooling = TemporalAveragePooling()
+        elif pooling_type == "ASP":
+            self.pooling = AttentiveStatsPool(in_dim=self.stats_dim * self.expansion)
+        elif pooling_type == "TSTP":
+            self.pooling = TemporalStatsPool()
+        else:
+            raise Exception(f'没有{pooling_type}池化层！')
+
+        self.seg_1 = nn.Linear(self.stats_dim * self.expansion * self.n_stats, embd_dim)
+        if self.two_emb_layer:
+            self.seg_bn_1 = nn.BatchNorm1d(embd_dim, affine=False)
+            self.seg_2 = nn.Linear(embd_dim, embd_dim)
+        else:
+            self.seg_bn_1 = nn.Identity()
+            self.seg_2 = nn.Identity()
+
+    def _make_layer(self, block, planes, num_blocks, stride, base_width, scale):
+        strides = [stride] + [1] * (num_blocks - 1)
+        layers = []
+        for stride in strides:
+            layers.append(block(self.expansion, self.in_planes, planes, stride, base_width, scale))
+            self.in_planes = planes * self.expansion
+        return nn.Sequential(*layers)
+
+    def forward(self, x):
+        x = x.permute(0, 2, 1)  # (B,T,F) => (B,F,T)
+        x = x.unsqueeze_(1)
+        out = F.relu(self.bn1(self.conv1(x)))
+        out1 = self.layer1(out)
+        out2 = self.layer2(out1)
+        out3 = self.layer3(out2)
+        out4 = self.layer4(out3)
+        out3_ds = self.layer3_ds(out3)
+        fuse_out34 = self.fuse34(out4, out3_ds)
+        stats = self.pooling(fuse_out34)
+
+        embed_a = self.seg_1(stats)
+        if self.two_emb_layer:
+            out = F.relu(embed_a)
+            out = self.seg_bn_1(out)
+            embed_b = self.seg_2(out)
+            return embed_b
         else:
             return embed_a
```

## mvector/models/pooling.py

```diff
@@ -42,14 +42,15 @@
         var = torch.var(x, dim=2)
         x = torch.cat((mean, var), dim=1)
         return x
 
 
 class SelfAttentivePooling(nn.Module):
     """SAP"""
+
     def __init__(self, in_dim, bottleneck_dim=128):
         # Use Conv1d with stride == 1 rather than Linear, then we don't need to transpose inputs.
         # attention dim = 128
         super(SelfAttentivePooling, self).__init__()
         self.linear1 = nn.Conv1d(in_dim, bottleneck_dim, kernel_size=1)  # equals W and b in the paper
         self.linear2 = nn.Conv1d(bottleneck_dim, in_dim, kernel_size=1)  # equals V and k in the paper
 
@@ -59,14 +60,15 @@
         alpha = torch.softmax(self.linear2(alpha), dim=2)
         mean = torch.sum(alpha * x, dim=2)
         return mean
 
 
 class AttentiveStatsPool(nn.Module):
     """ASP"""
+
     def __init__(self, in_dim, bottleneck_dim=128):
         super().__init__()
         # Use Conv1d with stride == 1 rather than Linear, then we don't need to transpose inputs.
         self.linear1 = nn.Conv1d(in_dim, bottleneck_dim, kernel_size=1)  # equals W and b in the paper
         self.linear2 = nn.Conv1d(bottleneck_dim, in_dim, kernel_size=1)  # equals V and k in the paper
 
     def forward(self, x):
```

## Comparing `mvector-1.0.3.dist-info/LICENSE` & `mvector-1.0.4.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `mvector-1.0.3.dist-info/METADATA` & `mvector-1.0.4.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mvector
-Version: 1.0.3
+Version: 1.0.4
 Summary: Voice Print Recognition toolkit on Pytorch
 Home-page: https://github.com/yeyupiaoling/VoiceprintRecognition_Pytorch
 Download-URL: https://github.com/yeyupiaoling/VoiceprintRecognition_Pytorch.git
 Author: yeyupiaoling
 License: Apache License 2.0
 Keywords: Voice,Pytorch
 Classifier: Intended Audience :: Developers
@@ -80,45 +80,47 @@
 - ERes2Net：[An Enhanced Res2Net with Local and Global Feature Fusion for Speaker Verification](https://arxiv.org/abs/2305.12838v1)
 
 
 # 模型下载
 
 ### 训练CN-Celeb数据，共有2796个说话人。
 
-|    模型     | Params(M) | 预处理方法 |                数据集                 | train speakers | threshold |   EER   | MinDCF  |   模型下载   |
-|:---------:|:---------:|:-----:|:----------------------------------:|:--------------:|:---------:|:-------:|:-------:|:--------:|
-|   CAM++   |    6.8    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.26    | 0.09557 | 0.53516 | 加入知识星球获取 |
-| ERes2Net  |    6.6    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.19    | 0.09980 | 0.52352 | 加入知识星球获取 |
-| ResNetSE  |    7.8    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.20    | 0.10149 | 0.55185 | 加入知识星球获取 |
-| EcapaTdnn |    6.1    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.24    | 0.10163 | 0.56543 | 加入知识星球获取 |
-|   TDNN    |    2.6    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.23    | 0.12182 | 0.62141 | 加入知识星球获取 |
-|  Res2Net  |    5.0    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.22    | 0.14390 | 0.67961 | 加入知识星球获取 |
-|   CAM++   |    6.8    | Fbank |               更大数据集                |      2W+       |   0.33    | 0.07874 | 0.52524 | 加入知识星球获取 |
-| ERes2Net  |   55.1    | Fbank |               其他数据集                |      20W+      |   0.36    | 0.02936 | 0.18355 | 加入知识星球获取 |
-|   CAM++   |    6.8    | Fbank |               其他数据集                |      20W+      |   0.29    | 0.04765 | 0.31436 | 加入知识星球获取 |
+|     模型     | Params(M) | 预处理方法 |                数据集                 | train speakers | threshold |   EER   | MinDCF  |   模型下载   |
+|:----------:|:---------:|:-----:|:----------------------------------:|:--------------:|:---------:|:-------:|:-------:|:--------:|
+|   CAM++    |    6.8    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.26    | 0.09557 | 0.53516 | 加入知识星球获取 |
+|  ERes2Net  |    6.6    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.19    | 0.09980 | 0.52352 | 加入知识星球获取 |
+|  ResNetSE  |    7.8    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.20    | 0.10149 | 0.55185 | 加入知识星球获取 |
+| EcapaTdnn  |    6.1    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.24    | 0.10163 | 0.56543 | 加入知识星球获取 |
+|    TDNN    |    2.6    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.23    | 0.12182 | 0.62141 | 加入知识星球获取 |
+|  Res2Net   |    5.0    | Fbank | [CN-Celeb](http://openslr.org/82/) |      2796      |   0.22    | 0.14390 | 0.67961 | 加入知识星球获取 |
+|   CAM++    |    6.8    | Fbank |               更大数据集                |      2W+       |   0.33    | 0.07874 | 0.52524 | 加入知识星球获取 |
+|  ERes2Net  |   55.1    | Fbank |               其他数据集                |      20W+      |   0.36    | 0.02936 | 0.18355 | 加入知识星球获取 |
+| ERes2NetV2 |   56.2    | Fbank |               其他数据集                |      20W+      |   0.36    | 0.03847 | 0.24301 | 加入知识星球获取 |
+|   CAM++    |    6.8    | Fbank |               其他数据集                |      20W+      |   0.29    | 0.04765 | 0.31436 | 加入知识星球获取 |
 
 说明：
 1. 评估的测试集为[CN-Celeb的测试集](https://aistudio.baidu.com/aistudio/datasetdetail/233361)，包含196个说话人。
 2. 使用语速增强分类大小翻三倍`speed_perturb_3_class: True`。
 3. 参数数量不包含了分类器的参数数量。
 
 
 ### 训练VoxCeleb1&2数据，共有7205个说话人。
 
-|    模型     | Params(M) | 预处理方法 |     数据集     | train speakers | threshold |   EER   | MinDCF  |   模型下载   |
-|:---------:|:---------:|:-----:|:-----------:|:--------------:|:---------:|:-------:|:-------:|:--------:|
-|   CAM++   |    6.8    | Fbank | VoxCeleb1&2 |      7205      |   0.23    | 0.02659 | 0.18604 | 加入知识星球获取 |
-| ERes2Net  |    6.6    | Fbank | VoxCeleb1&2 |      7205      |   0.23    | 0.03648 | 0.25508 | 加入知识星球获取 |
-| ResNetSE  |    7.8    | Fbank | VoxCeleb1&2 |      7205      |   0.23    | 0.03668 | 0.27881 | 加入知识星球获取 |
-| EcapaTdnn |    6.1    | Fbank | VoxCeleb1&2 |      7205      |   0.26    | 0.02610 | 0.18008 | 加入知识星球获取 |
-|   TDNN    |    2.6    | Fbank | VoxCeleb1&2 |      7205      |   0.26    | 0.03963 | 0.31433 | 加入知识星球获取 |
-|  Res2Net  |    5.0    | Fbank | VoxCeleb1&2 |      7205      |   0.20    | 0.04290 | 0.41416 | 加入知识星球获取 |
-|   CAM++   |    6.8    | Fbank |    更大数据集    |      2W+       |   0.28    | 0.03182 | 0.23731 | 加入知识星球获取 |
-| ERes2Net  |   55.1    | Fbank |    其他数据集    |      20W+      |   0.53    | 0.08904 | 0.62130 | 加入知识星球获取 |
-|   CAM++   |    6.8    | Fbank |    其他数据集    |      20W+      |   0.49    | 0.10334 | 0.71200 | 加入知识星球获取 |
+|     模型     | Params(M) | 预处理方法 |     数据集     | train speakers | threshold |   EER   | MinDCF  |   模型下载   |
+|:----------:|:---------:|:-----:|:-----------:|:--------------:|:---------:|:-------:|:-------:|:--------:|
+|   CAM++    |    6.8    | Fbank | VoxCeleb1&2 |      7205      |   0.23    | 0.02659 | 0.18604 | 加入知识星球获取 |
+|  ERes2Net  |    6.6    | Fbank | VoxCeleb1&2 |      7205      |   0.23    | 0.03648 | 0.25508 | 加入知识星球获取 |
+|  ResNetSE  |    7.8    | Fbank | VoxCeleb1&2 |      7205      |   0.23    | 0.03668 | 0.27881 | 加入知识星球获取 |
+| EcapaTdnn  |    6.1    | Fbank | VoxCeleb1&2 |      7205      |   0.26    | 0.02610 | 0.18008 | 加入知识星球获取 |
+|    TDNN    |    2.6    | Fbank | VoxCeleb1&2 |      7205      |   0.26    | 0.03963 | 0.31433 | 加入知识星球获取 |
+|  Res2Net   |    5.0    | Fbank | VoxCeleb1&2 |      7205      |   0.20    | 0.04290 | 0.41416 | 加入知识星球获取 |
+|   CAM++    |    6.8    | Fbank |    更大数据集    |      2W+       |   0.28    | 0.03182 | 0.23731 | 加入知识星球获取 |
+|  ERes2Net  |   55.1    | Fbank |    其他数据集    |      20W+      |   0.53    | 0.08904 | 0.62130 | 加入知识星球获取 |
+| ERes2NetV2 |   56.2    | Fbank |    其他数据集    |      20W+      |           |         |         | 加入知识星球获取 |
+|   CAM++    |    6.8    | Fbank |    其他数据集    |      20W+      |   0.49    | 0.10334 | 0.71200 | 加入知识星球获取 |
 
 说明：
 
 1. 评估的测试集为[VoxCeleb1&2的测试集](https://aistudio.baidu.com/aistudio/datasetdetail/255977)，包含158个说话人。
 2. 使用语速增强分类大小翻三倍`speed_perturb_3_class: True`。
 3. 参数数量不包含了分类器的参数数量。
```

## Comparing `mvector-1.0.3.dist-info/RECORD` & `mvector-1.0.4.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,32 +1,32 @@
-mvector/__init__.py,sha256=1Nqnj-dBG8NZOTjUbPp9NWTy0ddKc5o7d-vkgz53aw0,134
-mvector/predict.py,sha256=Io_wxBYRfalhOmSezKf6iJ84Xbf6p67JFFbqzROabT4,17513
-mvector/trainer.py,sha256=CJCp4SOyctHk-5lMunrG1tozw--L7LYV-oz2mOL3LO8,38106
+mvector/__init__.py,sha256=1zetSbdKZkGIeDAhU5LjUXdAVng_ORH7ucvFUC60Ios,148
+mvector/predict.py,sha256=6tQIAWYqt6WT13v7Gmj1XBoSqQaXk24L1FjKCzFkqiM,17697
+mvector/trainer.py,sha256=Ijg0wuLzjhh4u263Yo-oh9rUhn3YoQNlc1nApG-iouQ,38271
 mvector/data_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 mvector/data_utils/audio.py,sha256=NQ4wf0SiUR20n76cg20LFPPwFCvESqbUmZrBhEWyyAM,22220
 mvector/data_utils/collate_fn.py,sha256=khDNN1B0xZpoXz1dLHJE4JBbXnOLPqcQvVlruVnRBv0,930
 mvector/data_utils/featurizer.py,sha256=dgiKRMuvAApNZBt36mdd3RDZJp3z0JTMwYBiHd6nRUg,3696
 mvector/data_utils/reader.py,sha256=pZz8HV3LVsuCXuRfOze3oVwUPF3lbiTWoj2hwFKCkSg,7407
 mvector/data_utils/spec_aug.py,sha256=Sr9-Ss6I7LA3TGMqzML_X-nb_nnMfUxt5KbsA6HJpzM,1572
 mvector/data_utils/utils.py,sha256=EmcQ5kvot_gIpmAyKALnxGWyqN0COYERp6nOKMoVK30,4712
 mvector/metric/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 mvector/metric/metrics.py,sha256=59HlEhkSuV3_MKV41EkE9sLGQIfbHxtlWcUQezhJano,1517
 mvector/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 mvector/models/campplus.py,sha256=1m5Y1F8ExpUeH-FTZd9WoKQ2Twd9EiuE3zn79ZRyvoc,13511
 mvector/models/ecapa_tdnn.py,sha256=U9EI5QlrMSQNgtIJ47eEmh2-p8QlttoE4-c_sSFqbCQ,5293
-mvector/models/eres2net.py,sha256=J9qXuDO8ksdSMu_UffvwZurHJHfQoSRmBxH8iYUGIqU,10340
+mvector/models/eres2net.py,sha256=CdCOl0gIbSuB76F85JyoVU_7hbtoa4ofh39GiaJl_YY,18261
 mvector/models/fc.py,sha256=4oNVcOB7DkjwXXI6uEZUfp1Dub27Fdcf8kPb7o1BQUQ,3641
 mvector/models/loss.py,sha256=jqtr-UIOrpN0wvyCQvvA5wMGCOKI4YiDoe7vn9rNxLI,9448
-mvector/models/pooling.py,sha256=d71FTmOT-n5yR3ozd6MR7RpxoONzlnFPyOLkIoVIrcM,3795
+mvector/models/pooling.py,sha256=zrt7DUhFv4T8Xg1ZNdk8iJn6RZMZO7atRmyNEVF1IdU,3799
 mvector/models/res2net.py,sha256=U__18Nocnfwms8aeEzVMJSx4Ni41i_BskuNNhT9OlGE,6687
 mvector/models/resnet_se.py,sha256=aPElXGIlPjGNXJT5p8Z-942EsQ_n2eGEoGdlCkDdbSs,5522
 mvector/models/tdnn.py,sha256=O_YO-wUknUJPV991Ol3fOvq6T5okGjazaYCVSnEL2Ck,2963
 mvector/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 mvector/utils/logger.py,sha256=-ssorx8FlHA_wrd2Eq6f4HkOqaOG2YseBGvYAo8NXN8,2839
 mvector/utils/record.py,sha256=S2sGoLPJrdRsrG7_ojNt4kwL05VNrOxnmnMOwNOZ9-0,1385
 mvector/utils/scheduler.py,sha256=3qgF4hNkOn-vE7kgN4vvj8Ou_6Y8gsPJjQoiWI1okOk,3538
 mvector/utils/utils.py,sha256=TprSWPMLDodFmxVJtGLRb0MmQ9xI16En7ZZvMJ9yuj0,2789
-mvector-1.0.3.dist-info/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
-mvector-1.0.3.dist-info/METADATA,sha256=rIvhD-5mccMmqxyDD6ZgsOT2dZabVjeuQgLGfQ2CZvc,33611
-mvector-1.0.3.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
-mvector-1.0.3.dist-info/top_level.txt,sha256=biohDrNo0DJzZdjePn_XJZLCyGMA9w0u2mzmVVXM3cE,8
-mvector-1.0.3.dist-info/RECORD,,
+mvector-1.0.4.dist-info/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
+mvector-1.0.4.dist-info/METADATA,sha256=lfBZHAnJfH82QOpMOFW4-_OTppPUv6D-bi4nRSeALsE,33928
+mvector-1.0.4.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
+mvector-1.0.4.dist-info/top_level.txt,sha256=biohDrNo0DJzZdjePn_XJZLCyGMA9w0u2mzmVVXM3cE,8
+mvector-1.0.4.dist-info/RECORD,,
```

