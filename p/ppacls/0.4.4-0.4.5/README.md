# Comparing `tmp/ppacls-0.4.4-py3-none-any.whl.zip` & `tmp/ppacls-0.4.5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,32 +1,32 @@
-Zip file size: 57134 bytes, number of entries: 30
--rw-rw-rw-  2.0 fat      196 b- defN 24-May-03 03:45 ppacls/__init__.py
--rw-rw-rw-  2.0 fat     8988 b- defN 24-May-02 05:00 ppacls/predict.py
--rw-rw-rw-  2.0 fat    28974 b- defN 24-May-02 05:49 ppacls/trainer.py
+Zip file size: 57457 bytes, number of entries: 30
+-rw-rw-rw-  2.0 fat      210 b- defN 24-May-04 04:27 ppacls/__init__.py
+-rw-rw-rw-  2.0 fat     9169 b- defN 24-May-03 14:03 ppacls/predict.py
+-rw-rw-rw-  2.0 fat    29127 b- defN 24-May-03 14:03 ppacls/trainer.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Nov-04 14:44 ppacls/data_utils/__init__.py
 -rw-rw-rw-  2.0 fat    22219 b- defN 24-Apr-27 05:31 ppacls/data_utils/audio.py
 -rw-rw-rw-  2.0 fat      947 b- defN 24-May-02 05:13 ppacls/data_utils/collate_fn.py
 -rw-rw-rw-  2.0 fat     3919 b- defN 24-May-02 04:53 ppacls/data_utils/featurizer.py
 -rw-rw-rw-  2.0 fat     7042 b- defN 24-May-02 09:38 ppacls/data_utils/reader.py
 -rw-rw-rw-  2.0 fat     1581 b- defN 23-Aug-07 14:51 ppacls/data_utils/spec_aug.py
 -rw-rw-rw-  2.0 fat     4698 b- defN 24-Jan-28 04:01 ppacls/data_utils/utils.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Nov-05 11:08 ppacls/models/__init__.py
 -rw-rw-rw-  2.0 fat    12884 b- defN 23-Aug-09 10:07 ppacls/models/campplus.py
 -rw-rw-rw-  2.0 fat    11092 b- defN 24-May-02 04:42 ppacls/models/ecapa_tdnn.py
--rw-rw-rw-  2.0 fat    10451 b- defN 23-Aug-10 10:24 ppacls/models/eres2net.py
+-rw-rw-rw-  2.0 fat    17955 b- defN 24-May-03 14:47 ppacls/models/eres2net.py
 -rw-rw-rw-  2.0 fat     9863 b- defN 23-Aug-07 14:51 ppacls/models/panns.py
 -rw-rw-rw-  2.0 fat     5277 b- defN 23-Aug-10 10:24 ppacls/models/pooling.py
 -rw-rw-rw-  2.0 fat     6941 b- defN 23-Aug-09 09:51 ppacls/models/res2net.py
 -rw-rw-rw-  2.0 fat     5760 b- defN 23-Aug-09 09:51 ppacls/models/resnet_se.py
 -rw-rw-rw-  2.0 fat     3557 b- defN 23-Aug-09 09:51 ppacls/models/tdnn.py
 -rw-rw-rw-  2.0 fat     5049 b- defN 23-Mar-18 10:41 ppacls/models/utils.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Nov-04 14:44 ppacls/utils/__init__.py
 -rw-rw-rw-  2.0 fat     2839 b- defN 22-Nov-04 14:44 ppacls/utils/logger.py
 -rw-rw-rw-  2.0 fat     1067 b- defN 23-Mar-23 11:45 ppacls/utils/record.py
 -rw-rw-rw-  2.0 fat     3399 b- defN 23-Aug-05 12:19 ppacls/utils/scheduler.py
 -rw-rw-rw-  2.0 fat     4301 b- defN 24-Apr-27 04:49 ppacls/utils/utils.py
--rw-rw-rw-  2.0 fat    11558 b- defN 24-May-03 03:45 ppacls-0.4.4.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    23734 b- defN 24-May-03 03:45 ppacls-0.4.4.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-May-03 03:45 ppacls-0.4.4.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        7 b- defN 24-May-03 03:45 ppacls-0.4.4.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     2407 b- defN 24-May-03 03:45 ppacls-0.4.4.dist-info/RECORD
-30 files, 198842 bytes uncompressed, 53312 bytes compressed:  73.2%
+-rw-rw-rw-  2.0 fat    11558 b- defN 24-May-04 04:27 ppacls-0.4.5.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    23840 b- defN 24-May-04 04:27 ppacls-0.4.5.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-May-04 04:27 ppacls-0.4.5.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        7 b- defN 24-May-04 04:27 ppacls-0.4.5.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     2407 b- defN 24-May-04 04:27 ppacls-0.4.5.dist-info/RECORD
+30 files, 206800 bytes uncompressed, 53635 bytes compressed:  74.1%
```

## zipnote {}

```diff
@@ -69,23 +69,23 @@
 
 Filename: ppacls/utils/scheduler.py
 Comment: 
 
 Filename: ppacls/utils/utils.py
 Comment: 
 
-Filename: ppacls-0.4.4.dist-info/LICENSE
+Filename: ppacls-0.4.5.dist-info/LICENSE
 Comment: 
 
-Filename: ppacls-0.4.4.dist-info/METADATA
+Filename: ppacls-0.4.5.dist-info/METADATA
 Comment: 
 
-Filename: ppacls-0.4.4.dist-info/WHEEL
+Filename: ppacls-0.4.5.dist-info/WHEEL
 Comment: 
 
-Filename: ppacls-0.4.4.dist-info/top_level.txt
+Filename: ppacls-0.4.5.dist-info/top_level.txt
 Comment: 
 
-Filename: ppacls-0.4.4.dist-info/RECORD
+Filename: ppacls-0.4.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## ppacls/__init__.py

```diff
@@ -1,4 +1,4 @@
-__version__ = "0.4.4"
+__version__ = "0.4.5"
 # 项目支持的模型
-SUPPORT_MODEL = ['EcapaTdnn', 'PANNS_CNN6', 'PANNS_CNN10', 'PANNS_CNN14', 'Res2Net', 'ResNetSE', 'TDNN', 'ERes2Net',
-                 'CAMPPlus']
+SUPPORT_MODEL = ['EcapaTdnn', 'PANNS_CNN6', 'PANNS_CNN10', 'PANNS_CNN14', 'Res2Net', 'ResNetSE', 'TDNN',
+                 'ERes2Net', 'ERes2NetV2', 'CAMPPlus']
```

## ppacls/predict.py

```diff
@@ -6,15 +6,15 @@
 import yaml
 
 from ppacls import SUPPORT_MODEL
 from ppacls.data_utils.audio import AudioSegment
 from ppacls.data_utils.featurizer import AudioFeaturizer
 from ppacls.models.campplus import CAMPPlus
 from ppacls.models.ecapa_tdnn import EcapaTdnn
-from ppacls.models.eres2net import ERes2Net
+from ppacls.models.eres2net import ERes2Net, ERes2NetV2
 from ppacls.models.panns import PANNS_CNN6, PANNS_CNN10, PANNS_CNN14
 from ppacls.models.res2net import Res2Net
 from ppacls.models.resnet_se import ResNetSE
 from ppacls.models.tdnn import TDNN
 from ppacls.utils.logger import setup_logger
 from ppacls.utils.utils import dict_to_object, print_arguments
 
@@ -68,14 +68,16 @@
             self.predictor = Res2Net(input_size=self._audio_featurizer.feature_dim, **self.configs.model_conf)
         elif self.configs.use_model == 'ResNetSE':
             self.predictor = ResNetSE(input_size=self._audio_featurizer.feature_dim, **self.configs.model_conf)
         elif self.configs.use_model == 'TDNN':
             self.predictor = TDNN(input_size=self._audio_featurizer.feature_dim, **self.configs.model_conf)
         elif self.configs.use_model == 'ERes2Net':
             self.predictor = ERes2Net(input_size=self._audio_featurizer.feature_dim, **self.configs.model_conf)
+        elif self.configs.use_model == 'ERes2NetV2':
+            self.predictor = ERes2NetV2(input_size=self._audio_featurizer.feature_dim, **self.configs.model_conf)
         elif self.configs.use_model == 'CAMPPlus':
             self.predictor = CAMPPlus(input_size=self._audio_featurizer.feature_dim, **self.configs.model_conf)
         else:
             raise Exception(f'{self.configs.use_model} 模型不存在！')
         # 加载模型
         if os.path.isdir(model_path):
             model_path = os.path.join(model_path, 'model.pdparams')
```

## ppacls/trainer.py

```diff
@@ -20,15 +20,15 @@
 from ppacls import SUPPORT_MODEL, __version__
 from ppacls.data_utils.collate_fn import collate_fn
 from ppacls.data_utils.featurizer import AudioFeaturizer
 from ppacls.data_utils.reader import PPAClsDataset
 from ppacls.data_utils.spec_aug import SpecAug
 from ppacls.models.campplus import CAMPPlus
 from ppacls.models.ecapa_tdnn import EcapaTdnn
-from ppacls.models.eres2net import ERes2Net
+from ppacls.models.eres2net import ERes2Net, ERes2NetV2
 from ppacls.models.panns import PANNS_CNN6, PANNS_CNN10, PANNS_CNN14
 from ppacls.models.res2net import Res2Net
 from ppacls.models.resnet_se import ResNetSE
 from ppacls.models.tdnn import TDNN
 from ppacls.utils.logger import setup_logger
 from ppacls.utils.scheduler import cosine_decay_with_warmup
 from ppacls.utils.utils import dict_to_object, plot_confusion_matrix, print_arguments
@@ -164,14 +164,16 @@
             self.model = Res2Net(input_size=input_size, **self.configs.model_conf)
         elif self.configs.use_model == 'ResNetSE':
             self.model = ResNetSE(input_size=input_size, **self.configs.model_conf)
         elif self.configs.use_model == 'TDNN':
             self.model = TDNN(input_size=input_size, **self.configs.model_conf)
         elif self.configs.use_model == 'ERes2Net':
             self.model = ERes2Net(input_size=input_size, **self.configs.model_conf)
+        elif self.configs.use_model == 'ERes2NetV2':
+            self.model = ERes2NetV2(input_size=input_size, **self.configs.model_conf)
         elif self.configs.use_model == 'CAMPPlus':
             self.model = CAMPPlus(input_size=input_size, **self.configs.model_conf)
         else:
             raise Exception(f'{self.configs.use_model} 模型不存在！')
         summary(self.model, (1, 98, input_size))
         # print(self.model)
         # 获取损失函数
```

## ppacls/models/eres2net.py

```diff
@@ -1,15 +1,16 @@
 import math
 
 import paddle
 import paddle.nn as nn
 import paddle.nn.functional as F
 
-from ppacls.models.pooling import AttentiveStatisticsPooling, TemporalAveragePooling, TemporalStatsPool
-from ppacls.models.pooling import SelfAttentivePooling, TemporalStatisticsPooling
+from ppacls.models.pooling import TemporalStatsPool
+
+__all__ = ['ERes2Net', 'ERes2NetV2']
 
 
 class ReLU(nn.Hardtanh):
 
     def __init__(self, inplace=False):
         super(ReLU, self).__init__(0, 20, inplace)
 
@@ -78,32 +79,29 @@
                 nn.Conv2D(in_planes, self.expansion * planes, kernel_size=1, stride=stride),
                 nn.BatchNorm2D(self.expansion * planes))
         self.stride = stride
         self.width = width
         self.scale = scale
 
     def forward(self, x):
-        residual = x
-
         out = self.conv1(x)
         out = self.bn1(out)
         out = self.relu(out)
-        spx = paddle.split(out, int(out.shape[1]/self.width), 1)
+        spx = paddle.split(out, int(out.shape[1] / self.width), 1)
         for i in range(self.nums):
             if i == 0:
                 sp = spx[i]
             else:
                 sp = sp + spx[i]
             sp = self.convs[i](sp)
             sp = self.relu(self.bns[i](sp))
             if i == 0:
                 out = sp
             else:
                 out = paddle.concat((out, sp), 1)
-
         out = self.conv3(out)
         out = self.bn3(out)
 
         residual = self.shortcut(x)
         out += residual
         out = self.relu(out)
 
@@ -143,33 +141,29 @@
                 nn.Conv2D(in_planes, self.expansion * planes, kernel_size=1, stride=stride),
                 nn.BatchNorm2D(self.expansion * planes))
         self.stride = stride
         self.width = width
         self.scale = scale
 
     def forward(self, x):
-        residual = x
-
         out = self.conv1(x)
         out = self.bn1(out)
         out = self.relu(out)
-        spx = paddle.split(out, int(out.shape[1]/self.width), 1)
+        spx = paddle.split(out, int(out.shape[1] / self.width), 1)
         for i in range(self.nums):
             if i == 0:
                 sp = spx[i]
             else:
                 sp = self.fuse_models[i - 1](sp, spx[i])
-
             sp = self.convs[i](sp)
             sp = self.relu(self.bns[i](sp))
             if i == 0:
                 out = sp
             else:
                 out = paddle.concat((out, sp), 1)
-
         out = self.conv3(out)
         out = self.bn3(out)
 
         residual = self.shortcut(x)
         out += residual
         out = self.relu(out)
 
@@ -195,19 +189,15 @@
         self.in_planes = m_channels
         self.expansion = expansion
         self.feat_dim = input_size
         self.embd_dim = embd_dim
         self.stats_dim = int(input_size / 8) * m_channels * 8
         self.two_emb_layer = two_emb_layer
 
-        self.conv1 = nn.Conv2D(1,
-                               m_channels,
-                               kernel_size=3,
-                               stride=1,
-                               padding=1)
+        self.conv1 = nn.Conv2D(1, m_channels, kernel_size=3, stride=1, padding=1)
         self.bn1 = nn.BatchNorm2D(m_channels)
         self.layer1 = self._make_layer(block, m_channels, num_blocks[0], stride=1,
                                        base_width=base_width, scale=scale)
         self.layer2 = self._make_layer(block, m_channels * 2, num_blocks[1], stride=2,
                                        base_width=base_width, scale=scale)
         self.layer3 = self._make_layer(block_fuse, m_channels * 4, num_blocks[2], stride=2,
                                        base_width=base_width, scale=scale)
@@ -273,7 +263,210 @@
             embed_b = self.seg_2(out)
             out = self.output(embed_b)
             return out
         else:
             out = self.output(embed_a)
             return out
 
+
+class BasicBlockERes2NetV2(nn.Layer):
+
+    def __init__(self, expansion, in_planes, planes, stride=1, base_width=26, scale=2):
+        super(BasicBlockERes2NetV2, self).__init__()
+        self.expansion = expansion
+        width = int(math.floor(planes * (base_width / 64.0)))
+        self.conv1 = nn.Conv2D(in_planes, width * scale, kernel_size=1, stride=stride)
+        self.bn1 = nn.BatchNorm2D(width * scale)
+        self.nums = scale
+
+        convs = []
+        bns = []
+        for i in range(self.nums):
+            convs.append(nn.Conv2D(width, width, kernel_size=3, padding=1))
+            bns.append(nn.BatchNorm2D(width))
+        self.convs = nn.LayerList(convs)
+        self.bns = nn.LayerList(bns)
+        self.relu = ReLU(inplace=True)
+
+        self.conv3 = nn.Conv2D(width * scale, planes * self.expansion, kernel_size=1)
+        self.bn3 = nn.BatchNorm2D(planes * self.expansion)
+        self.shortcut = nn.Sequential()
+        if stride != 1 or in_planes != self.expansion * planes:
+            self.shortcut = nn.Sequential(
+                nn.Conv2D(in_planes, self.expansion * planes, kernel_size=1, stride=stride),
+                nn.BatchNorm2D(self.expansion * planes))
+        self.stride = stride
+        self.width = width
+        self.scale = scale
+
+    def forward(self, x):
+        out = self.conv1(x)
+        out = self.bn1(out)
+        out = self.relu(out)
+        spx = paddle.split(out, int(out.shape[1] / self.width), 1)
+        for i in range(self.nums):
+            if i == 0:
+                sp = spx[i]
+            else:
+                sp = sp + spx[i]
+            sp = self.convs[i](sp)
+            sp = self.relu(self.bns[i](sp))
+            if i == 0:
+                out = sp
+            else:
+                out = paddle.concat((out, sp), 1)
+        out = self.conv3(out)
+        out = self.bn3(out)
+
+        residual = self.shortcut(x)
+        out += residual
+        out = self.relu(out)
+
+        return out
+
+
+class BasicBlockERes2NetV2_AFF(nn.Layer):
+
+    def __init__(self, expansion, in_planes, planes, stride=1, base_width=26, scale=2):
+        super(BasicBlockERes2NetV2_AFF, self).__init__()
+        self.expansion = expansion
+        width = int(math.floor(planes * (base_width / 64.0)))
+        self.conv1 = nn.Conv2D(in_planes, width * scale, kernel_size=1, stride=stride)
+        self.bn1 = nn.BatchNorm2D(width * scale)
+        self.nums = scale
+
+        convs = []
+        fuse_models = []
+        bns = []
+        for i in range(self.nums):
+            convs.append(nn.Conv2D(width, width, kernel_size=3, padding=1))
+            bns.append(nn.BatchNorm2D(width))
+        for j in range(self.nums - 1):
+            fuse_models.append(AFF(channels=width, r=4))
+
+        self.convs = nn.LayerList(convs)
+        self.bns = nn.LayerList(bns)
+        self.fuse_models = nn.LayerList(fuse_models)
+        self.relu = ReLU(inplace=True)
+
+        self.conv3 = nn.Conv2D(width * scale, planes * self.expansion, kernel_size=1)
+        self.bn3 = nn.BatchNorm2D(planes * self.expansion)
+        self.shortcut = nn.Sequential()
+        if stride != 1 or in_planes != self.expansion * planes:
+            self.shortcut = nn.Sequential(
+                nn.Conv2D(in_planes, self.expansion * planes, kernel_size=1, stride=stride),
+                nn.BatchNorm2D(self.expansion * planes))
+        self.stride = stride
+        self.width = width
+        self.scale = scale
+
+    def forward(self, x):
+        out = self.conv1(x)
+        out = self.bn1(out)
+        out = self.relu(out)
+        spx = paddle.split(out, int(out.shape[1] / self.width), 1)
+        for i in range(self.nums):
+            if i == 0:
+                sp = spx[i]
+            else:
+                sp = self.fuse_models[i - 1](sp, spx[i])
+            sp = self.convs[i](sp)
+            sp = self.relu(self.bns[i](sp))
+            if i == 0:
+                out = sp
+            else:
+                out = paddle.concat((out, sp), 1)
+        out = self.conv3(out)
+        out = self.bn3(out)
+
+        residual = self.shortcut(x)
+        out += residual
+        out = self.relu(out)
+
+        return out
+
+
+class ERes2NetV2(nn.Layer):
+    def __init__(self,
+                 num_class,
+                 input_size,
+                 block=BasicBlockERes2NetV2,
+                 block_fuse=BasicBlockERes2NetV2_AFF,
+                 num_blocks=[3, 4, 6, 3],
+                 m_channels=32,
+                 expansion=2,
+                 base_width=26,
+                 scale=2,
+                 embd_dim=192,
+                 pooling_type='TSTP',
+                 two_emb_layer=False):
+        super(ERes2NetV2, self).__init__()
+        self.in_planes = m_channels
+        self.expansion = expansion
+        self.embd_dim = embd_dim
+        self.stats_dim = int(input_size / 8) * m_channels * 8
+        self.two_emb_layer = two_emb_layer
+
+        self.conv1 = nn.Conv2D(1, m_channels, kernel_size=3, stride=1, padding=1)
+        self.bn1 = nn.BatchNorm2D(m_channels)
+        self.layer1 = self._make_layer(block, m_channels, num_blocks[0], stride=1,
+                                       base_width=base_width, scale=scale)
+        self.layer2 = self._make_layer(block, m_channels * 2, num_blocks[1], stride=2,
+                                       base_width=base_width, scale=scale)
+        self.layer3 = self._make_layer(block_fuse, m_channels * 4, num_blocks[2], stride=2,
+                                       base_width=base_width, scale=scale)
+        self.layer4 = self._make_layer(block_fuse, m_channels * 8, num_blocks[3], stride=2,
+                                       base_width=base_width, scale=scale)
+
+        # Downsampling module
+        self.layer3_ds = nn.Conv2D(m_channels * 8, m_channels * 16, kernel_size=3, padding=1, stride=2)
+
+        # Bottom-up fusion module
+        self.fuse34 = AFF(channels=m_channels * 16, r=4)
+
+        self.n_stats = 1 if pooling_type == 'TAP' else 2
+        if pooling_type == "TSTP":
+            self.pooling = TemporalStatsPool()
+        else:
+            raise Exception(f'没有{pooling_type}池化层！')
+
+        self.seg_1 = nn.Linear(self.stats_dim * self.expansion * self.n_stats, embd_dim)
+        if self.two_emb_layer:
+            self.seg_bn_1 = nn.BatchNorm1D(embd_dim)
+            self.seg_2 = nn.Linear(embd_dim, embd_dim)
+        else:
+            self.seg_bn_1 = nn.Identity()
+            self.seg_2 = nn.Identity()
+        # 分类层
+        self.fc = nn.Linear(embd_dim, num_class)
+
+    def _make_layer(self, block, planes, num_blocks, stride, base_width, scale):
+        strides = [stride] + [1] * (num_blocks - 1)
+        layers = []
+        for stride in strides:
+            layers.append(block(self.expansion, self.in_planes, planes, stride, base_width, scale))
+            self.in_planes = planes * self.expansion
+        return nn.Sequential(*layers)
+
+    def forward(self, x):
+        x = x.transpose((0, 2, 1))  # (B,T,F) => (B,F,T)
+
+        x = x.unsqueeze_(1)
+        out = F.relu(self.bn1(self.conv1(x)))
+        out1 = self.layer1(out)
+        out2 = self.layer2(out1)
+        out3 = self.layer3(out2)
+        out4 = self.layer4(out3)
+        out3_ds = self.layer3_ds(out3)
+        fuse_out34 = self.fuse34(out4, out3_ds)
+        stats = self.pooling(fuse_out34)
+
+        embed_a = self.seg_1(stats)
+        if self.two_emb_layer:
+            out = F.relu(embed_a)
+            out = self.seg_bn_1(out)
+            embed_b = self.seg_2(out)
+            out = self.fc(embed_b)
+            return out
+        else:
+            out = self.fc(embed_a)
+            return out
```

## Comparing `ppacls-0.4.4.dist-info/LICENSE` & `ppacls-0.4.5.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `ppacls-0.4.4.dist-info/METADATA` & `ppacls-0.4.5.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: ppacls
-Version: 0.4.4
+Version: 0.4.5
 Summary: Audio Classification toolkit on PaddlePaddle
 Home-page: https://github.com/yeyupiaoling/AudioClassification_PaddlePaddle
 Download-URL: https://github.com/yeyupiaoling/AudioClassification_PaddlePaddle.git
 Author: yeyupiaoling
 License: Apache License 2.0
 Keywords: audio,paddle
 Classifier: Intended Audience :: Developers
@@ -85,14 +85,15 @@
 
 # 模型测试表
 
 |      模型      | Params(M) | 预处理方法 |        数据集        |   类别数量   |   准确率   |   获取模型   |
 |:------------:|:---------:|:-----:|:-----------------:|:--------:|:-------:|:--------:|
 |   CAMPPlus   |    7.2    | Flank |   UrbanSound8K    |    10    | 0.96590 | 加入知识星球获取 |
 | PANNS（CNN10） |    4.9    | Flank |   UrbanSound8K    |    10    | 0.95454 | 加入知识星球获取 |
+|  ERes2NetV2  |    5.4    | Flank |   UrbanSound8K    |    10    | 0.93295 | 加入知识星球获取 |
 |   ResNetSE   |    9.1    | Flank |   UrbanSound8K    |    10    | 0.92219 | 加入知识星球获取 |
 |     TDNN     |    2.7    | Flank |   UrbanSound8K    |    10    | 0.92045 | 加入知识星球获取 |
 |   ERes2Net   |    6.6    | Flank |   UrbanSound8K    |    10    | 0.90909 | 加入知识星球获取 |
 |  EcapaTdnn   |    6.2    | Flank |   UrbanSound8K    |    10    | 0.90503 | 加入知识星球获取 |
 |   Res2Net    |    5.6    | Flank |   UrbanSound8K    |    10    | 0.85812 | 加入知识星球获取 |
 |   CAMPPlus   |    6.1    | Flank | CN-Celeb和VoxCeleb | 2(中英文语种) | 0.99254 | 加入知识星球获取 |
 |   ResNetSE   |    9.8    | Flank | CN-Celeb和VoxCeleb | 2(中英文语种) | 0.99545 | 加入知识星球获取 |
```

