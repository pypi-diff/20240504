# Comparing `tmp/colossalai-nightly-2024.3.9.tar.gz` & `tmp/colossalai-nightly-2024.5.4.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "colossalai-nightly-2024.3.9.tar", last modified: Sat Mar  9 00:13:38 2024, max compression
+gzip compressed data, was "colossalai-nightly-2024.5.4.tar", last modified: Sat May  4 00:14:38 2024, max compression
```

## Comparing `colossalai-nightly-2024.3.9.tar` & `colossalai-nightly-2024.5.4.tar`

### file list

```diff
@@ -1,1166 +1,1175 @@
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.465873 colossalai-nightly-2024.3.9/
--rw-r--r--   0 runner    (1001) docker     (127)    29471 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/LICENSE
--rw-r--r--   0 runner    (1001) docker     (127)      198 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (127)    34234 2024-03-09 00:13:38.465873 colossalai-nightly-2024.3.9/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    28872 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.313872 colossalai-nightly-2024.3.9/colossalai/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.313872 colossalai-nightly-2024.3.9/colossalai/_C/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_C/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      597 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.313872 colossalai-nightly-2024.3.9/colossalai/_analyzer/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.313872 colossalai-nightly-2024.3.9/colossalai/_analyzer/_subclasses/
--rw-r--r--   0 runner    (1001) docker     (127)      165 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/_subclasses/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    20868 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/_subclasses/_meta_registration.py
--rw-r--r--   0 runner    (1001) docker     (127)     2088 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/_subclasses/_monkey_patch.py
--rw-r--r--   0 runner    (1001) docker     (127)    18677 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/_subclasses/flop_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     7589 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/_subclasses/meta_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)      114 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/envs.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.313872 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/
--rw-r--r--   0 runner    (1001) docker     (127)      129 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    18984 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/codegen.py
--rw-r--r--   0 runner    (1001) docker     (127)     9947 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/graph_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     8482 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/node_util.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.313872 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/passes/
--rw-r--r--   0 runner    (1001) docker     (127)      106 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/passes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12708 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/passes/graph_profile.py
--rw-r--r--   0 runner    (1001) docker     (127)     9939 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/passes/shape_prop.py
--rw-r--r--   0 runner    (1001) docker     (127)      952 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/symbolic_profile.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.317872 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/tracer/
--rw-r--r--   0 runner    (1001) docker     (127)       63 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/tracer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5415 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/tracer/bias_addition.py
--rw-r--r--   0 runner    (1001) docker     (127)     1108 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/tracer/custom_leaf_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     3568 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/tracer/proxy.py
--rw-r--r--   0 runner    (1001) docker     (127)     5862 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/tracer/symbolic_trace.py
--rw-r--r--   0 runner    (1001) docker     (127)    15702 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/tracer/tracer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.317872 colossalai-nightly-2024.3.9/colossalai/accelerator/
--rw-r--r--   0 runner    (1001) docker     (127)      431 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/accelerator/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2149 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/accelerator/api.py
--rw-r--r--   0 runner    (1001) docker     (127)     9402 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/accelerator/base_accelerator.py
--rw-r--r--   0 runner    (1001) docker     (127)    10154 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/accelerator/cpu_accelerator.py
--rw-r--r--   0 runner    (1001) docker     (127)     9442 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/accelerator/cuda_accelerator.py
--rw-r--r--   0 runner    (1001) docker     (127)     9453 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/accelerator/npu_accelerator.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.317872 colossalai-nightly-2024.3.9/colossalai/amp/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/amp/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.317872 colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.317872 colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/grad_scaler/
--rw-r--r--   0 runner    (1001) docker     (127)      222 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/grad_scaler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2119 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/grad_scaler/base_grad_scaler.py
--rw-r--r--   0 runner    (1001) docker     (127)      735 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/grad_scaler/constant_grad_scaler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4977 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/grad_scaler/dynamic_grad_scaler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.321872 colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/mixed_precision_mixin/
--rw-r--r--   0 runner    (1001) docker     (127)      226 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/mixed_precision_mixin/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2523 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/mixed_precision_mixin/base.py
--rw-r--r--   0 runner    (1001) docker     (127)      504 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/mixed_precision_mixin/bf16.py
--rw-r--r--   0 runner    (1001) docker     (127)     2695 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/mixed_precision_mixin/fp16.py
--rw-r--r--   0 runner    (1001) docker     (127)     7959 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/mixed_precision_optimizer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.321872 colossalai-nightly-2024.3.9/colossalai/auto_parallel/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.321872 colossalai-nightly-2024.3.9/colossalai/auto_parallel/checkpoint/
--rw-r--r--   0 runner    (1001) docker     (127)      155 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/checkpoint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      377 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/checkpoint/build_c_ext.py
--rw-r--r--   0 runner    (1001) docker     (127)     7711 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/checkpoint/ckpt_solver_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     3468 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py
--rw-r--r--   0 runner    (1001) docker     (127)    18538 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py
--rw-r--r--   0 runner    (1001) docker     (127)     4921 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/checkpoint/operation.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.321872 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/
--rw-r--r--   0 runner    (1001) docker     (127)       95 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      296 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.325872 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/
--rw-r--r--   0 runner    (1001) docker     (127)      241 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3940 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/activation.py
--rw-r--r--   0 runner    (1001) docker     (127)     2840 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py
--rw-r--r--   0 runner    (1001) docker     (127)     7571 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/conv.py
--rw-r--r--   0 runner    (1001) docker     (127)     2512 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)    24482 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     1028 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py
--rw-r--r--   0 runner    (1001) docker     (127)     9318 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/norm.py
--rw-r--r--   0 runner    (1001) docker     (127)     7392 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/pooling.py
--rw-r--r--   0 runner    (1001) docker     (127)     3258 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     2827 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/where.py
--rw-r--r--   0 runner    (1001) docker     (127)      761 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     4748 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/shard_metainfo.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.325872 colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6770 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/amp_optimizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     3545 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/base_offload_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     2072 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/mem_optimize.py
--rw-r--r--   0 runner    (1001) docker     (127)     5140 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/region.py
--rw-r--r--   0 runner    (1001) docker     (127)    20031 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/region_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     9909 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/runtime.py
--rw-r--r--   0 runner    (1001) docker     (127)    18503 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/solver.py
--rw-r--r--   0 runner    (1001) docker     (127)    17919 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/training_simulator.py
--rw-r--r--   0 runner    (1001) docker     (127)     2793 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/util.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.325872 colossalai-nightly-2024.3.9/colossalai/auto_parallel/passes/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/passes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5111 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/passes/comm_metainfo_pass.py
--rw-r--r--   0 runner    (1001) docker     (127)      417 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/passes/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     6050 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/passes/meta_info_prop.py
--rw-r--r--   0 runner    (1001) docker     (127)    11328 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/passes/runtime_apply_pass.py
--rw-r--r--   0 runner    (1001) docker     (127)    22439 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/passes/runtime_preparation_pass.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.329872 colossalai-nightly-2024.3.9/colossalai/auto_parallel/pipeline_shard/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/pipeline_shard/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.329872 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2805 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)    16998 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/initialize.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.333872 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/
--rw-r--r--   0 runner    (1001) docker     (127)     2062 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3731 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/addmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3070 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/batch_norm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4988 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/binary_elementwise_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4805 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/bmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     5563 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/conv_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2798 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/default_reshape_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    11414 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/embedding_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1316 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/getattr_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1734 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/getitem_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1901 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/layer_norm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    13535 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/linear_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    20347 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/matmul_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    16308 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/node_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1762 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/normal_pooling_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2115 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/output_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2997 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/permute_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1466 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/placeholder_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)      744 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     1980 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/softmax_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2238 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/split_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.337872 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/
--rw-r--r--   0 runner    (1001) docker     (127)     2100 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    14274 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/batch_norm_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     5162 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/binary_elementwise_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    24078 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/conv_strategy_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    12070 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/embedding_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     3615 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getattr_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     7361 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getitem_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     9225 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/layer_norm_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    41664 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/matmul_strategy_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     5538 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/normal_pooling_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4597 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/output_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     3636 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/placeholder_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    18860 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/reshape_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4702 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/softmax_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)    13053 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/strategy_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4965 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/sum_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     2278 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/tensor_constructor_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     3981 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/unary_elementwise_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     4158 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/where_generator.py
--rw-r--r--   0 runner    (1001) docker     (127)     3053 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/sum_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1156 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/tensor_constructor_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2330 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/transpose_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1774 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/unary_elementwise_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1964 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/view_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3553 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/where_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1522 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/options.py
--rw-r--r--   0 runner    (1001) docker     (127)    10812 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/sharding_strategy.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.337872 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/solver/
--rw-r--r--   0 runner    (1001) docker     (127)      238 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/solver/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9987 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/solver/cost_graph.py
--rw-r--r--   0 runner    (1001) docker     (127)     5767 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/solver/graph_analysis.py
--rw-r--r--   0 runner    (1001) docker     (127)    20206 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/solver/solver.py
--rw-r--r--   0 runner    (1001) docker     (127)     8474 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/solver/strategies_constructor.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.341872 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/utils/
--rw-r--r--   0 runner    (1001) docker     (127)     1210 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5899 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/utils/broadcast.py
--rw-r--r--   0 runner    (1001) docker     (127)     8346 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/utils/factory.py
--rw-r--r--   0 runner    (1001) docker     (127)     3841 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/utils/misc.py
--rw-r--r--   0 runner    (1001) docker     (127)     9066 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/utils/reshape.py
--rw-r--r--   0 runner    (1001) docker     (127)     4408 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/utils/sharding.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.341872 colossalai-nightly-2024.3.9/colossalai/booster/
--rw-r--r--   0 runner    (1001) docker     (127)       93 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1481 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/accelerator.py
--rw-r--r--   0 runner    (1001) docker     (127)    16440 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/booster.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.341872 colossalai-nightly-2024.3.9/colossalai/booster/mixed_precision/
--rw-r--r--   0 runner    (1001) docker     (127)     1298 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/mixed_precision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      102 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/mixed_precision/bf16.py
--rw-r--r--   0 runner    (1001) docker     (127)     3218 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/mixed_precision/fp16_apex.py
--rw-r--r--   0 runner    (1001) docker     (127)      870 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/mixed_precision/fp16_naive.py
--rw-r--r--   0 runner    (1001) docker     (127)     4824 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/mixed_precision/fp16_torch.py
--rw-r--r--   0 runner    (1001) docker     (127)      101 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/mixed_precision/fp8.py
--rw-r--r--   0 runner    (1001) docker     (127)      565 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/mixed_precision/mixed_precision_base.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.345872 colossalai-nightly-2024.3.9/colossalai/booster/plugin/
--rw-r--r--   0 runner    (1001) docker     (127)      529 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/plugin/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3098 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/plugin/dp_plugin_base.py
--rw-r--r--   0 runner    (1001) docker     (127)    27857 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/plugin/gemini_plugin.py
--rw-r--r--   0 runner    (1001) docker     (127)    58657 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/plugin/hybrid_parallel_plugin.py
--rw-r--r--   0 runner    (1001) docker     (127)    14888 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/plugin/low_level_zero_plugin.py
--rw-r--r--   0 runner    (1001) docker     (127)    20674 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py
--rw-r--r--   0 runner    (1001) docker     (127)     2159 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/plugin/plugin_base.py
--rw-r--r--   0 runner    (1001) docker     (127)      559 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/plugin/pp_plugin_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     8120 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/plugin/torch_ddp_plugin.py
--rw-r--r--   0 runner    (1001) docker     (127)    14796 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/booster/plugin/torch_fsdp_plugin.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.345872 colossalai-nightly-2024.3.9/colossalai/checkpoint_io/
--rw-r--r--   0 runner    (1001) docker     (127)      318 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/checkpoint_io/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    14992 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/checkpoint_io/checkpoint_io_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     8613 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/checkpoint_io/general_checkpoint_io.py
--rw-r--r--   0 runner    (1001) docker     (127)    42849 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/checkpoint_io/hybrid_parallel_checkpoint_io.py
--rw-r--r--   0 runner    (1001) docker     (127)     5758 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/checkpoint_io/index_file.py
--rw-r--r--   0 runner    (1001) docker     (127)    29355 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/checkpoint_io/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.345872 colossalai-nightly-2024.3.9/colossalai/cli/
--rw-r--r--   0 runner    (1001) docker     (127)       40 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/cli/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.345872 colossalai-nightly-2024.3.9/colossalai/cli/check/
--rw-r--r--   0 runner    (1001) docker     (127)      396 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/cli/check/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8454 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/cli/check/check_installation.py
--rw-r--r--   0 runner    (1001) docker     (127)      310 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/cli/cli.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.345872 colossalai-nightly-2024.3.9/colossalai/cli/launcher/
--rw-r--r--   0 runner    (1001) docker     (127)     3654 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/cli/launcher/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3214 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/cli/launcher/hostinfo.py
--rw-r--r--   0 runner    (1001) docker     (127)     4286 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/cli/launcher/multinode_runner.py
--rw-r--r--   0 runner    (1001) docker     (127)    10530 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/cli/launcher/run.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.349872 colossalai-nightly-2024.3.9/colossalai/cluster/
--rw-r--r--   0 runner    (1001) docker     (127)      296 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/cluster/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4241 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/cluster/device_mesh_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     7233 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/cluster/dist_coordinator.py
--rw-r--r--   0 runner    (1001) docker     (127)     2356 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/cluster/process_group_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     9777 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/cluster/process_group_mesh.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.349872 colossalai-nightly-2024.3.9/colossalai/context/
--rw-r--r--   0 runner    (1001) docker     (127)       96 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/context/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3153 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/context/config.py
--rw-r--r--   0 runner    (1001) docker     (127)      801 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/context/singleton_meta.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.349872 colossalai-nightly-2024.3.9/colossalai/device/
--rw-r--r--   0 runner    (1001) docker     (127)      139 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/device/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    17443 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/device/alpha_beta_profiler.py
--rw-r--r--   0 runner    (1001) docker     (127)     5634 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/device/calc_pipeline_strategy.py
--rw-r--r--   0 runner    (1001) docker     (127)    23616 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/device/device_mesh.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.349872 colossalai-nightly-2024.3.9/colossalai/fx/
--rw-r--r--   0 runner    (1001) docker     (127)      217 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1548 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/_compatibility.py
--rw-r--r--   0 runner    (1001) docker     (127)    19328 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/_meta_regist_12.py
--rw-r--r--   0 runner    (1001) docker     (127)     1906 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/_meta_regist_13.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.349872 colossalai-nightly-2024.3.9/colossalai/fx/codegen/
--rw-r--r--   0 runner    (1001) docker     (127)       45 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/codegen/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    45217 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/codegen/activation_checkpoint_codegen.py
--rw-r--r--   0 runner    (1001) docker     (127)     7438 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/graph_module.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.353872 colossalai-nightly-2024.3.9/colossalai/fx/passes/
--rw-r--r--   0 runner    (1001) docker     (127)      266 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/passes/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    13496 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/passes/adding_split_node_pass.py
--rw-r--r--   0 runner    (1001) docker     (127)    12261 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/passes/concrete_info_prop.py
--rw-r--r--   0 runner    (1001) docker     (127)    14066 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/passes/meta_info_prop.py
--rw-r--r--   0 runner    (1001) docker     (127)    15886 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/passes/passes_for_gpt2_test.py
--rw-r--r--   0 runner    (1001) docker     (127)     6888 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/passes/shard_1d_pass.py
--rw-r--r--   0 runner    (1001) docker     (127)    13714 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/passes/split_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     6169 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/passes/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.353872 colossalai-nightly-2024.3.9/colossalai/fx/profiler/
--rw-r--r--   0 runner    (1001) docker     (127)      768 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      871 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     6285 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/dataflow.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.353872 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/
--rw-r--r--   0 runner    (1001) docker     (127)      282 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      782 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     7062 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.353872 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_function/
--rw-r--r--   0 runner    (1001) docker     (127)      211 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_function/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1304 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_function/activation_function.py
--rw-r--r--   0 runner    (1001) docker     (127)     3387 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_function/arithmetic.py
--rw-r--r--   0 runner    (1001) docker     (127)      632 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_function/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)      438 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_function/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     2027 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_function/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     1188 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_function/pooling.py
--rw-r--r--   0 runner    (1001) docker     (127)      402 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_function/python_ops.py
--rw-r--r--   0 runner    (1001) docker     (127)     2188 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_function/torch_ops.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.357872 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/
--rw-r--r--   0 runner    (1001) docker     (127)      252 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1055 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/activation_function.py
--rw-r--r--   0 runner    (1001) docker     (127)     2037 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/attention.py
--rw-r--r--   0 runner    (1001) docker     (127)     6708 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/convolution.py
--rw-r--r--   0 runner    (1001) docker     (127)      424 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/dropout.py
--rw-r--r--   0 runner    (1001) docker     (127)      431 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)      495 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     1582 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     1013 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/pooling.py
--rw-r--r--   0 runner    (1001) docker     (127)     3020 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/rnn.py
--rw-r--r--   0 runner    (1001) docker     (127)      271 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/torch_op.py
--rw-r--r--   0 runner    (1001) docker     (127)      603 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/registry.py
--rw-r--r--   0 runner    (1001) docker     (127)     1050 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/shard_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2232 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/memory_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    13214 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/opcount.py
--rw-r--r--   0 runner    (1001) docker     (127)    15377 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/profiler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4235 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/shard_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4990 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/profiler/tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     4010 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/proxy.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.357872 colossalai-nightly-2024.3.9/colossalai/fx/tracer/
--rw-r--r--   0 runner    (1001) docker     (127)      201 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4850 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/_meta_trace.py
--rw-r--r--   0 runner    (1001) docker     (127)     2197 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/_symbolic_trace.py
--rw-r--r--   0 runner    (1001) docker     (127)     1479 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/_tracer_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.357872 colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/
--rw-r--r--   0 runner    (1001) docker     (127)       90 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.357872 colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/
--rw-r--r--   0 runner    (1001) docker     (127)      193 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2798 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addbmm.py
--rw-r--r--   0 runner    (1001) docker     (127)     2171 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addmm.py
--rw-r--r--   0 runner    (1001) docker     (127)     4445 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/bias_addition_function.py
--rw-r--r--   0 runner    (1001) docker     (127)      745 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/linear.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.357872 colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/
--rw-r--r--   0 runner    (1001) docker     (127)       78 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4395 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/bias_addition_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     2370 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/conv.py
--rw-r--r--   0 runner    (1001) docker     (127)      503 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)    26431 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/experimental.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.357872 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/
--rw-r--r--   0 runner    (1001) docker     (127)       62 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.361873 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_function/
--rw-r--r--   0 runner    (1001) docker     (127)      167 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_function/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      217 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_function/activation_function.py
--rw-r--r--   0 runner    (1001) docker     (127)     3200 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_function/arithmetic.py
--rw-r--r--   0 runner    (1001) docker     (127)     5707 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_function/convolution.py
--rw-r--r--   0 runner    (1001) docker     (127)      339 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_function/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)      517 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_function/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     2047 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_function/python_ops.py
--rw-r--r--   0 runner    (1001) docker     (127)     5622 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_function/torch_ops.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.361873 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_module/
--rw-r--r--   0 runner    (1001) docker     (127)      180 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_module/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      428 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_module/activation_function.py
--rw-r--r--   0 runner    (1001) docker     (127)     4752 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_module/convolution.py
--rw-r--r--   0 runner    (1001) docker     (127)      254 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_module/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)      400 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_module/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     1129 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_module/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     6769 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_module/pooling.py
--rw-r--r--   0 runner    (1001) docker     (127)      644 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_module/rnn.py
--rw-r--r--   0 runner    (1001) docker     (127)      834 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/registry.py
--rw-r--r--   0 runner    (1001) docker     (127)    24642 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/fx/tracer/tracer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.361873 colossalai-nightly-2024.3.9/colossalai/inference/
--rw-r--r--   0 runner    (1001) docker     (127)      235 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.361873 colossalai-nightly-2024.3.9/colossalai/inference/engine/
--rw-r--r--   0 runner    (1001) docker     (127)       67 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/engine/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8315 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/engine/engine.py
--rw-r--r--   0 runner    (1001) docker     (127)     8949 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/engine/microbatch_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.361873 colossalai-nightly-2024.3.9/colossalai/inference/engine/modeling/
--rw-r--r--   0 runner    (1001) docker     (127)      225 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/engine/modeling/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/engine/modeling/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    19778 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/engine/modeling/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)    20627 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/engine/modeling/chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)    22054 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/engine/modeling/llama.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.361873 colossalai-nightly-2024.3.9/colossalai/inference/engine/policies/
--rw-r--r--   0 runner    (1001) docker     (127)      360 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/engine/policies/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5437 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/engine/policies/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)     3467 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/engine/policies/chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)     8514 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/engine/policies/llama.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.361873 colossalai-nightly-2024.3.9/colossalai/inference/kv_cache/
--rw-r--r--   0 runner    (1001) docker     (127)       90 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/kv_cache/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4483 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/kv_cache/batch_infer_state.py
--rw-r--r--   0 runner    (1001) docker     (127)     4582 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/kv_cache/kvcache_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.361873 colossalai-nightly-2024.3.9/colossalai/inference/quant/
--rw-r--r--   0 runner    (1001) docker     (127)       61 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/quant/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.361873 colossalai-nightly-2024.3.9/colossalai/inference/quant/gptq/
--rw-r--r--   0 runner    (1001) docker     (127)      155 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/quant/gptq/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.365872 colossalai-nightly-2024.3.9/colossalai/inference/quant/gptq/cai_gptq/
--rw-r--r--   0 runner    (1001) docker     (127)      372 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/quant/gptq/cai_gptq/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    15011 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/quant/gptq/cai_gptq/cai_quant_linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     1629 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/quant/gptq/cai_gptq/gptq_op.py
--rw-r--r--   0 runner    (1001) docker     (127)     2423 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/quant/gptq/gptq_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.365872 colossalai-nightly-2024.3.9/colossalai/inference/quant/smoothquant/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/quant/smoothquant/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.365872 colossalai-nightly-2024.3.9/colossalai/inference/quant/smoothquant/models/
--rw-r--r--   0 runner    (1001) docker     (127)      297 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/quant/smoothquant/models/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    20063 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/quant/smoothquant/models/base_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     6500 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/quant/smoothquant/models/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)    35519 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/quant/smoothquant/models/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)    10276 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/inference/quant/smoothquant/models/parallel_linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     6537 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/initialize.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.365872 colossalai-nightly-2024.3.9/colossalai/interface/
--rw-r--r--   0 runner    (1001) docker     (127)      152 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/interface/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      851 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/interface/model.py
--rw-r--r--   0 runner    (1001) docker     (127)     4120 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/interface/optimizer.py
--rw-r--r--   0 runner    (1001) docker     (127)      333 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/interface/pretrained.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.365872 colossalai-nightly-2024.3.9/colossalai/kernel/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.365872 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/
--rw-r--r--   0 runner    (1001) docker     (127)     1156 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2517 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/base_extension.py
--rw-r--r--   0 runner    (1001) docker     (127)     4730 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/cpp_extension.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.365872 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/cpu_adam/
--rw-r--r--   0 runner    (1001) docker     (127)      151 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/cpu_adam/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1043 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/cpu_adam/cpu_adam_arm.py
--rw-r--r--   0 runner    (1001) docker     (127)     1660 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/cpu_adam/cpu_adam_x86.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.365872 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/
--rw-r--r--   0 runner    (1001) docker     (127)      350 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.365872 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/arm/
--rw-r--r--   0 runner    (1001) docker     (127)    12939 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     5960 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.369872 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/
--rw-r--r--   0 runner    (1001) docker     (127)     2606 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/colossal_C_frontend.cpp
--rw-r--r--   0 runner    (1001) docker     (127)      214 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/compat.h
--rw-r--r--   0 runner    (1001) docker     (127)    18561 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/cpu_adam.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     5965 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/cpu_adam.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.369872 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/include/
--rw-r--r--   0 runner    (1001) docker     (127)     8585 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/include/block_reduce.h
--rw-r--r--   0 runner    (1001) docker     (127)     4878 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    25829 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     3906 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/moe_cuda.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    25627 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/moe_cuda_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     5046 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/multi_tensor_adam.cu
--rw-r--r--   0 runner    (1001) docker     (127)     5200 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/multi_tensor_apply.cuh
--rw-r--r--   0 runner    (1001) docker     (127)    13279 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    13115 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/multi_tensor_lamb.cu
--rw-r--r--   0 runner    (1001) docker     (127)     4440 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/multi_tensor_scale_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     6479 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     2684 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    21112 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.h
--rw-r--r--   0 runner    (1001) docker     (127)     3467 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax_cuda.cu
--rw-r--r--   0 runner    (1001) docker     (127)     2066 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    23530 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h
--rw-r--r--   0 runner    (1001) docker     (127)     2818 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu
--rw-r--r--   0 runner    (1001) docker     (127)    14851 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/type_shim.h
--rw-r--r--   0 runner    (1001) docker     (127)     6588 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/scaled_softmax.py
--rw-r--r--   0 runner    (1001) docker     (127)     3896 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/cuda_extension.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.369872 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/flash_attention/
--rw-r--r--   0 runner    (1001) docker     (127)      527 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/flash_attention/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3502 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/flash_attention/flash_attention_dao_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)     2360 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/flash_attention/flash_attention_npu.py
--rw-r--r--   0 runner    (1001) docker     (127)     3622 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/flash_attention/flash_attention_xformers_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.369872 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/layernorm/
--rw-r--r--   0 runner    (1001) docker     (127)       88 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/layernorm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      822 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/layernorm/layernorm_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.369872 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/moe/
--rw-r--r--   0 runner    (1001) docker     (127)       70 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/moe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      959 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/moe/moe_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.373872 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/optimizer/
--rw-r--r--   0 runner    (1001) docker     (127)      104 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/optimizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1104 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/optimizer/fused_optimizer_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.373872 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/softmax/
--rw-r--r--   0 runner    (1001) docker     (127)      271 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/softmax/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1027 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/softmax/scaled_masked_softmax_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)      589 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/triton_extension.py
--rw-r--r--   0 runner    (1001) docker     (127)     8284 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/extensions/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.373872 colossalai-nightly-2024.3.9/colossalai/kernel/jit/
--rw-r--r--   0 runner    (1001) docker     (127)      317 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/jit/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      670 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/jit/bias_dropout_add.py
--rw-r--r--   0 runner    (1001) docker     (127)     1358 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/jit/bias_gelu.py
--rw-r--r--   0 runner    (1001) docker     (127)     3591 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/jit/option.py
--rw-r--r--   0 runner    (1001) docker     (127)     3395 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/kernel_loader.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.373872 colossalai-nightly-2024.3.9/colossalai/kernel/triton/
--rw-r--r--   0 runner    (1001) docker     (127)     1096 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/triton/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    14518 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/triton/context_attention.py
--rw-r--r--   0 runner    (1001) docker     (127)     2349 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/triton/copy_kv_cache_dest.py
--rw-r--r--   0 runner    (1001) docker     (127)     7022 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/triton/custom_autotune.py
--rw-r--r--   0 runner    (1001) docker     (127)     2397 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/triton/flash_decoding.py
--rw-r--r--   0 runner    (1001) docker     (127)     2960 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/triton/fused_layernorm.py
--rw-r--r--   0 runner    (1001) docker     (127)    18024 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/triton/gptq_triton.py
--rw-r--r--   0 runner    (1001) docker     (127)     3280 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/triton/int8_rotary_embedding_kernel.py
--rw-r--r--   0 runner    (1001) docker     (127)     7353 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/triton/llama_act_combine_kernel.py
--rw-r--r--   0 runner    (1001) docker     (127)     4967 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/triton/qkv_matmul_kernel.py
--rw-r--r--   0 runner    (1001) docker     (127)     5947 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/triton/self_attention_nofusion.py
--rw-r--r--   0 runner    (1001) docker     (127)    21773 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/triton/smooth_attention.py
--rw-r--r--   0 runner    (1001) docker     (127)     3744 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/triton/softmax.py
--rw-r--r--   0 runner    (1001) docker     (127)     7858 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/kernel/triton/token_attention_kernel.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.373872 colossalai-nightly-2024.3.9/colossalai/lazy/
--rw-r--r--   0 runner    (1001) docker     (127)      107 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/lazy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2187 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/lazy/construction.py
--rw-r--r--   0 runner    (1001) docker     (127)    25011 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/lazy/lazy_init.py
--rw-r--r--   0 runner    (1001) docker     (127)    13895 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/lazy/pretrained.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.377873 colossalai-nightly-2024.3.9/colossalai/legacy/
--rw-r--r--   0 runner    (1001) docker     (127)      301 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.377873 colossalai-nightly-2024.3.9/colossalai/legacy/amp/
--rw-r--r--   0 runner    (1001) docker     (127)     2310 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/amp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      153 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/amp/amp_type.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.377873 colossalai-nightly-2024.3.9/colossalai/legacy/amp/apex_amp/
--rw-r--r--   0 runner    (1001) docker     (127)     1637 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/amp/apex_amp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1073 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/amp/apex_amp/apex_amp.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.377873 colossalai-nightly-2024.3.9/colossalai/legacy/amp/naive_amp/
--rw-r--r--   0 runner    (1001) docker     (127)     2453 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/amp/naive_amp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    13449 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/amp/naive_amp/_fp16_optimizer.py
--rw-r--r--   0 runner    (1001) docker     (127)     1741 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/amp/naive_amp/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     6081 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/amp/naive_amp/naive_amp.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.377873 colossalai-nightly-2024.3.9/colossalai/legacy/amp/torch_amp/
--rw-r--r--   0 runner    (1001) docker     (127)     1538 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/amp/torch_amp/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    26771 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/amp/torch_amp/_grad_scaler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3368 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/amp/torch_amp/torch_amp.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.377873 colossalai-nightly-2024.3.9/colossalai/legacy/builder/
--rw-r--r--   0 runner    (1001) docker     (127)      166 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/builder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3025 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/builder/builder.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.377873 colossalai-nightly-2024.3.9/colossalai/legacy/communication/
--rw-r--r--   0 runner    (1001) docker     (127)      868 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/communication/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11387 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/communication/collective.py
--rw-r--r--   0 runner    (1001) docker     (127)    17484 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/communication/p2p.py
--rw-r--r--   0 runner    (1001) docker     (127)     9047 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/communication/p2p_v2.py
--rw-r--r--   0 runner    (1001) docker     (127)     1945 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/communication/ring.py
--rw-r--r--   0 runner    (1001) docker     (127)     5151 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/communication/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      978 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.377873 colossalai-nightly-2024.3.9/colossalai/legacy/context/
--rw-r--r--   0 runner    (1001) docker     (127)      149 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    24229 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/parallel_context.py
--rw-r--r--   0 runner    (1001) docker     (127)     1142 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/parallel_mode.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.381873 colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/
--rw-r--r--   0 runner    (1001) docker     (127)      763 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2144 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_1d.py
--rw-r--r--   0 runner    (1001) docker     (127)     6269 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_2d.py
--rw-r--r--   0 runner    (1001) docker     (127)    12944 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_2p5d.py
--rw-r--r--   0 runner    (1001) docker     (127)    13290 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_3d.py
--rw-r--r--   0 runner    (1001) docker     (127)     2041 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_data.py
--rw-r--r--   0 runner    (1001) docker     (127)     2174 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     2652 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_pipeline.py
--rw-r--r--   0 runner    (1001) docker     (127)     4170 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_sequence.py
--rw-r--r--   0 runner    (1001) docker     (127)     2051 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     1180 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/process_group_initializer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.381873 colossalai-nightly-2024.3.9/colossalai/legacy/context/random/
--rw-r--r--   0 runner    (1001) docker     (127)      420 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/random/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5204 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/random/_helper.py
--rw-r--r--   0 runner    (1001) docker     (127)     3407 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/context/random/seed_manager.py
--rw-r--r--   0 runner    (1001) docker     (127)      149 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/core.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.381873 colossalai-nightly-2024.3.9/colossalai/legacy/engine/
--rw-r--r--   0 runner    (1001) docker     (127)       87 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7784 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/_base_engine.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.381873 colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_accumulation/
--rw-r--r--   0 runner    (1001) docker     (127)     2558 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_accumulation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10265 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_accumulation/_gradient_accumulation.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.381873 colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/
--rw-r--r--   0 runner    (1001) docker     (127)      537 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      750 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/_base_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1149 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/_data_parallel_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2138 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/_moe_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2488 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/_pipeline_parallel_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1148 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/_sequence_parallel_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)      749 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/_zero_gradient_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     1020 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.385872 colossalai-nightly-2024.3.9/colossalai/legacy/engine/schedule/
--rw-r--r--   0 runner    (1001) docker     (127)      315 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/schedule/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5816 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/schedule/_base_schedule.py
--rw-r--r--   0 runner    (1001) docker     (127)     3808 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/schedule/_non_pipeline_schedule.py
--rw-r--r--   0 runner    (1001) docker     (127)    40085 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/schedule/_pipeline_schedule.py
--rw-r--r--   0 runner    (1001) docker     (127)     7133 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/engine/schedule/_pipeline_schedule_v2.py
--rw-r--r--   0 runner    (1001) docker     (127)     1993 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/global_variables.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.385872 colossalai-nightly-2024.3.9/colossalai/legacy/inference/
--rw-r--r--   0 runner    (1001) docker     (127)      152 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4606 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/async_engine.py
--rw-r--r--   0 runner    (1001) docker     (127)     5935 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/async_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.385872 colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1345 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/get_tokenizer.py
--rw-r--r--   0 runner    (1001) docker     (127)    13595 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/infer_batch.py
--rw-r--r--   0 runner    (1001) docker     (127)     5332 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/io_struct.py
--rw-r--r--   0 runner    (1001) docker     (127)     6315 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/ray_dist_init.py
--rw-r--r--   0 runner    (1001) docker     (127)     1612 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/ray_init_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     2810 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/req_queue.py
--rw-r--r--   0 runner    (1001) docker     (127)     3282 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/sampling_params.py
--rw-r--r--   0 runner    (1001) docker     (127)     1514 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/stats.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.385872 colossalai-nightly-2024.3.9/colossalai/legacy/inference/hybridengine/
--rw-r--r--   0 runner    (1001) docker     (127)       65 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/hybridengine/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6981 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/hybridengine/engine.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.385872 colossalai-nightly-2024.3.9/colossalai/legacy/inference/hybridengine/modeling/
--rw-r--r--   0 runner    (1001) docker     (127)       80 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/hybridengine/modeling/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/hybridengine/modeling/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    21591 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/hybridengine/modeling/llama.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.385872 colossalai-nightly-2024.3.9/colossalai/legacy/inference/hybridengine/polices/
--rw-r--r--   0 runner    (1001) docker     (127)       78 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/hybridengine/polices/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5477 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/hybridengine/polices/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)    11605 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.385872 colossalai-nightly-2024.3.9/colossalai/legacy/inference/pipeline/
--rw-r--r--   0 runner    (1001) docker     (127)       83 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/pipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9019 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/pipeline/microbatch_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.389873 colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/
--rw-r--r--   0 runner    (1001) docker     (127)      123 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4483 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/batch_infer_state.py
--rw-r--r--   0 runner    (1001) docker     (127)    21286 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/engine.py
--rw-r--r--   0 runner    (1001) docker     (127)     4580 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/kvcache_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.389873 colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/modeling/
--rw-r--r--   0 runner    (1001) docker     (127)      225 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/modeling/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/modeling/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    23642 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/modeling/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)    22757 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/modeling/chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)    17832 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/modeling/llama.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.389873 colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/policies/
--rw-r--r--   0 runner    (1001) docker     (127)      209 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/policies/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4440 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/policies/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)     2987 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/policies/chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)     4898 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/policies/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)    19842 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/initialize.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.389873 colossalai-nightly-2024.3.9/colossalai/legacy/nn/
--rw-r--r--   0 runner    (1001) docker     (127)       63 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.389873 colossalai-nightly-2024.3.9/colossalai/legacy/nn/_ops/
--rw-r--r--   0 runner    (1001) docker     (127)       22 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/_ops/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8669 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/_ops/_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.389873 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/
--rw-r--r--   0 runner    (1001) docker     (127)      242 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2817 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/base_layer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.389873 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/colossalai_layer/
--rw-r--r--   0 runner    (1001) docker     (127)      300 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/colossalai_layer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1404 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/colossalai_layer/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      999 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/colossalai_layer/dropout.py
--rw-r--r--   0 runner    (1001) docker     (127)     6156 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/colossalai_layer/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)     5224 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/colossalai_layer/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     1718 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/colossalai_layer/normalization.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.393873 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_1d/
--rw-r--r--   0 runner    (1001) docker     (127)      459 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_1d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3724 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_1d/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)     5063 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_1d/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    44898 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_1d/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.393873 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_2d/
--rw-r--r--   0 runner    (1001) docker     (127)      458 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_2d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    35857 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_2d/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)      853 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_2d/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    49324 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_2d/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.393873 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_2p5d/
--rw-r--r--   0 runner    (1001) docker     (127)      494 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_2p5d/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    39868 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_2p5d/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)     1234 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_2p5d/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    49527 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_2p5d/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.393873 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_3d/
--rw-r--r--   0 runner    (1001) docker     (127)      498 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_3d/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    22832 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_3d/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)     3037 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_3d/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    49624 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_3d/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.393873 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_sequence/
--rw-r--r--   0 runner    (1001) docker     (127)      152 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_sequence/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6392 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_sequence/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)      483 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_sequence/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    10693 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_sequence/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.393873 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/utils/
--rw-r--r--   0 runner    (1001) docker     (127)      445 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2411 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/utils/common.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.393873 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/vanilla/
--rw-r--r--   0 runner    (1001) docker     (127)      345 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/vanilla/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    14693 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/vanilla/layers.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.393873 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/wrapper/
--rw-r--r--   0 runner    (1001) docker     (127)      101 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/wrapper/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2170 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/wrapper/pipeline_wrapper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.397873 colossalai-nightly-2024.3.9/colossalai/legacy/nn/loss/
--rw-r--r--   0 runner    (1001) docker     (127)     1592 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/loss/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4606 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/loss/loss_1d.py
--rw-r--r--   0 runner    (1001) docker     (127)     5732 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/loss/loss_2d.py
--rw-r--r--   0 runner    (1001) docker     (127)     5540 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/loss/loss_2p5d.py
--rw-r--r--   0 runner    (1001) docker     (127)     6436 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/loss/loss_3d.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.397873 colossalai-nightly-2024.3.9/colossalai/legacy/nn/metric/
--rw-r--r--   0 runner    (1001) docker     (127)      686 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/metric/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      148 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/metric/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      787 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/metric/accuracy_2d.py
--rw-r--r--   0 runner    (1001) docker     (127)      801 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/metric/accuracy_2p5d.py
--rw-r--r--   0 runner    (1001) docker     (127)     1263 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/metric/accuracy_3d.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.397873 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/
--rw-r--r--   0 runner    (1001) docker     (127)       65 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6469 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/data_parallel.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.397873 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/
--rw-r--r--   0 runner    (1001) docker     (127)      962 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.401872 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/
--rw-r--r--   0 runner    (1001) docker     (127)      742 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1165 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/base_embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)    27958 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/cache_mgr.py
--rw-r--r--   0 runner    (1001) docker     (127)     8710 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/cached_embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)     2041 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/copyer.py
--rw-r--r--   0 runner    (1001) docker     (127)      807 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/embedding_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     5690 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)     9962 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise.py
--rw-r--r--   0 runner    (1001) docker     (127)     7138 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise_split_cache.py
--rw-r--r--   0 runner    (1001) docker     (127)     1954 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/colo_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     1150 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)     1136 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     4780 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/module_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     3874 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/reducer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.401872 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/
--rw-r--r--   0 runner    (1001) docker     (127)      163 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1558 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/layer_spec.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.401872 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/middleware/
--rw-r--r--   0 runner    (1001) docker     (127)      149 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/middleware/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.401872 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/middleware/adaptor/
--rw-r--r--   0 runner    (1001) docker     (127)       79 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/middleware/adaptor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6151 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/middleware/adaptor/fx.py
--rw-r--r--   0 runner    (1001) docker     (127)     6818 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/middleware/topo.py
--rw-r--r--   0 runner    (1001) docker     (127)    11447 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/pipelinable.py
--rw-r--r--   0 runner    (1001) docker     (127)     5759 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/pipeline_process_group.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.401872 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/rpc/
--rw-r--r--   0 runner    (1001) docker     (127)      237 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/rpc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    59091 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/rpc/_pipeline_base.py
--rw-r--r--   0 runner    (1001) docker     (127)    14979 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/rpc/_pipeline_schedule.py
--rw-r--r--   0 runner    (1001) docker     (127)     5390 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/rpc/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     9017 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.401872 colossalai-nightly-2024.3.9/colossalai/legacy/registry/
--rw-r--r--   0 runner    (1001) docker     (127)      690 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/registry/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3052 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/registry/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.401872 colossalai-nightly-2024.3.9/colossalai/legacy/tensor/
--rw-r--r--   0 runner    (1001) docker     (127)      418 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      779 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/tensor/compute_spec.py
--rw-r--r--   0 runner    (1001) docker     (127)      101 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/tensor/const.py
--rw-r--r--   0 runner    (1001) docker     (127)     8692 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/tensor/dist_spec_mgr.py
--rw-r--r--   0 runner    (1001) docker     (127)     2714 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/tensor/distspec.py
--rw-r--r--   0 runner    (1001) docker     (127)     1678 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/tensor/op_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (127)    10505 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/tensor/process_group.py
--rw-r--r--   0 runner    (1001) docker     (127)      697 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/tensor/tensor_spec.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.405873 colossalai-nightly-2024.3.9/colossalai/legacy/trainer/
--rw-r--r--   0 runner    (1001) docker     (127)       53 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/trainer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    14773 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/trainer/_trainer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.405873 colossalai-nightly-2024.3.9/colossalai/legacy/trainer/hooks/
--rw-r--r--   0 runner    (1001) docker     (127)      648 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/trainer/hooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2712 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/trainer/hooks/_base_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)     3226 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/trainer/hooks/_checkpoint_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)      231 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/trainer/hooks/_commons_.py
--rw-r--r--   0 runner    (1001) docker     (127)    13085 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/trainer/hooks/_log_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)     2091 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/trainer/hooks/_lr_scheduler_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)    16242 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/trainer/hooks/_metric_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.405873 colossalai-nightly-2024.3.9/colossalai/legacy/utils/
--rw-r--r--   0 runner    (1001) docker     (127)     1438 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9872 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/activation_checkpoint.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.405873 colossalai-nightly-2024.3.9/colossalai/legacy/utils/checkpoint/
--rw-r--r--   0 runner    (1001) docker     (127)      114 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/checkpoint/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5297 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/checkpoint/module_checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (127)     2149 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/checkpoint/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    11338 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/checkpointing.py
--rw-r--r--   0 runner    (1001) docker     (127)    16595 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/common.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.405873 colossalai-nightly-2024.3.9/colossalai/legacy/utils/data_sampler/
--rw-r--r--   0 runner    (1001) docker     (127)      177 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/data_sampler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      339 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/data_sampler/base_sampler.py
--rw-r--r--   0 runner    (1001) docker     (127)     6704 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/data_sampler/data_parallel_sampler.py
--rw-r--r--   0 runner    (1001) docker     (127)     6416 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/memory.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.405873 colossalai-nightly-2024.3.9/colossalai/legacy/utils/profiler/
--rw-r--r--   0 runner    (1001) docker     (127)       52 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/profiler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      341 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/profiler/extention.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.409873 colossalai-nightly-2024.3.9/colossalai/legacy/utils/profiler/legacy/
--rw-r--r--   0 runner    (1001) docker     (127)      266 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/profiler/legacy/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10461 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/profiler/legacy/comm_profiler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4861 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/profiler/legacy/pcie_profiler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3776 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/profiler/legacy/prof_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     8459 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/profiler/profiler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4036 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/utils/profiler/stateful_tensor_mem_extention.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.409873 colossalai-nightly-2024.3.9/colossalai/legacy/zero/
--rw-r--r--   0 runner    (1001) docker     (127)     1570 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.409873 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/
--rw-r--r--   0 runner    (1001) docker     (127)      619 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7315 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/colo_init_context.py
--rw-r--r--   0 runner    (1001) docker     (127)     1518 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/gemini_context.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.409873 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/ophooks/
--rw-r--r--   0 runner    (1001) docker     (127)      118 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/ophooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      774 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/ophooks/_shard_grad_ophook.py
--rw-r--r--   0 runner    (1001) docker     (127)     1363 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/ophooks/_shard_param_ophook.py
--rw-r--r--   0 runner    (1001) docker     (127)     5081 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/ophooks/runtime_mem_tracer_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)     4715 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/ophooks/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.409873 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/paramhooks/
--rw-r--r--   0 runner    (1001) docker     (127)       77 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/paramhooks/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1251 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/paramhooks/_param_hookmgr.py
--rw-r--r--   0 runner    (1001) docker     (127)     6905 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/stateful_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     3965 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/stateful_tensor_mgr.py
--rw-r--r--   0 runner    (1001) docker     (127)     6369 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/tensor_placement_policy.py
--rw-r--r--   0 runner    (1001) docker     (127)     3807 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/tensor_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.409873 colossalai-nightly-2024.3.9/colossalai/legacy/zero/init_ctx/
--rw-r--r--   0 runner    (1001) docker     (127)      171 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/init_ctx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11099 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/init_ctx/init_context.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.409873 colossalai-nightly-2024.3.9/colossalai/legacy/zero/shard_utils/
--rw-r--r--   0 runner    (1001) docker     (127)      259 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/shard_utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      635 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/shard_utils/base_shard_strategy.py
--rw-r--r--   0 runner    (1001) docker     (127)     2296 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/shard_utils/bucket_tensor_shard_strategy.py
--rw-r--r--   0 runner    (1001) docker     (127)      706 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/shard_utils/commons.py
--rw-r--r--   0 runner    (1001) docker     (127)     2751 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/shard_utils/tensor_shard_strategy.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.413873 colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_model/
--rw-r--r--   0 runner    (1001) docker     (127)       75 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3003 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_model/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     8289 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_model/reduce_scatter.py
--rw-r--r--   0 runner    (1001) docker     (127)    28757 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_model/sharded_model_v2.py
--rw-r--r--   0 runner    (1001) docker     (127)      808 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_model/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4865 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_model/zero_hook.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.413873 colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_optim/
--rw-r--r--   0 runner    (1001) docker     (127)       83 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_optim/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    18982 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.413873 colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_param/
--rw-r--r--   0 runner    (1001) docker     (127)      131 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_param/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3859 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_param/sharded_param.py
--rw-r--r--   0 runner    (1001) docker     (127)     1144 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_param/sharded_tensor.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.413873 colossalai-nightly-2024.3.9/colossalai/logging/
--rw-r--r--   0 runner    (1001) docker     (127)     1593 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/logging/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     6110 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/logging/logger.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.413873 colossalai-nightly-2024.3.9/colossalai/moe/
--rw-r--r--   0 runner    (1001) docker     (127)      531 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/moe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12432 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/moe/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)    36076 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/moe/checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (127)     6249 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/moe/experts.py
--rw-r--r--   0 runner    (1001) docker     (127)    16261 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/moe/layers.py
--rw-r--r--   0 runner    (1001) docker     (127)    18267 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/moe/load_balance.py
--rw-r--r--   0 runner    (1001) docker     (127)     3075 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/moe/loss.py
--rw-r--r--   0 runner    (1001) docker     (127)     6138 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/moe/manager.py
--rw-r--r--   0 runner    (1001) docker     (127)    20401 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/moe/routers.py
--rw-r--r--   0 runner    (1001) docker     (127)     7403 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/moe/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.413873 colossalai-nightly-2024.3.9/colossalai/nn/
--rw-r--r--   0 runner    (1001) docker     (127)      114 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9563 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/init.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.417873 colossalai-nightly-2024.3.9/colossalai/nn/layer/
--rw-r--r--   0 runner    (1001) docker     (127)       21 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/layer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7635 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/layer/colo_attention.py
--rw-r--r--   0 runner    (1001) docker     (127)     2497 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/layer/layernorm.py
--rw-r--r--   0 runner    (1001) docker     (127)     6394 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/layer/scaled_softmax.py
--rw-r--r--   0 runner    (1001) docker     (127)      449 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/layer/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.417873 colossalai-nightly-2024.3.9/colossalai/nn/loss/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/loss/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.417873 colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/
--rw-r--r--   0 runner    (1001) docker     (127)      673 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5867 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/cosine.py
--rw-r--r--   0 runner    (1001) docker     (127)     7717 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/delayed.py
--rw-r--r--   0 runner    (1001) docker     (127)     1178 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     2672 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/multistep.py
--rw-r--r--   0 runner    (1001) docker     (127)     5050 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/onecycle.py
--rw-r--r--   0 runner    (1001) docker     (127)     2458 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/poly.py
--rw-r--r--   0 runner    (1001) docker     (127)     3516 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/torch.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.417873 colossalai-nightly-2024.3.9/colossalai/nn/optimizer/
--rw-r--r--   0 runner    (1001) docker     (127)      303 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/optimizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8611 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/optimizer/cpu_adam.py
--rw-r--r--   0 runner    (1001) docker     (127)     6420 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/optimizer/fused_adam.py
--rw-r--r--   0 runner    (1001) docker     (127)     9056 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/optimizer/fused_lamb.py
--rw-r--r--   0 runner    (1001) docker     (127)     6012 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/optimizer/fused_sgd.py
--rw-r--r--   0 runner    (1001) docker     (127)     7946 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/optimizer/hybrid_adam.py
--rw-r--r--   0 runner    (1001) docker     (127)     4423 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/optimizer/lamb.py
--rw-r--r--   0 runner    (1001) docker     (127)     3567 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/optimizer/lars.py
--rw-r--r--   0 runner    (1001) docker     (127)     6768 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/nn/optimizer/nvme_optimizer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.417873 colossalai-nightly-2024.3.9/colossalai/pipeline/
--rw-r--r--   0 runner    (1001) docker     (127)      344 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/pipeline/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    26187 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/pipeline/p2p.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.421873 colossalai-nightly-2024.3.9/colossalai/pipeline/schedule/
--rw-r--r--   0 runner    (1001) docker     (127)      241 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/pipeline/schedule/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5194 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/pipeline/schedule/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1478 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/pipeline/schedule/base.py
--rw-r--r--   0 runner    (1001) docker     (127)    20007 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/pipeline/schedule/generate.py
--rw-r--r--   0 runner    (1001) docker     (127)    26918 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/pipeline/schedule/interleaved_pp.py
--rw-r--r--   0 runner    (1001) docker     (127)    19503 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/pipeline/schedule/one_f_one_b.py
--rw-r--r--   0 runner    (1001) docker     (127)     6790 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/pipeline/stage_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.421873 colossalai-nightly-2024.3.9/colossalai/shardformer/
--rw-r--r--   0 runner    (1001) docker     (127)       44 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3327 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.421873 colossalai-nightly-2024.3.9/colossalai/shardformer/layer/
--rw-r--r--   0 runner    (1001) docker     (127)      839 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/layer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    25087 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/layer/_operation.py
--rw-r--r--   0 runner    (1001) docker     (127)     3520 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/layer/dropout.py
--rw-r--r--   0 runner    (1001) docker     (127)    12912 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/layer/embedding.py
--rw-r--r--   0 runner    (1001) docker     (127)    17470 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/layer/linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     4589 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/layer/loss.py
--rw-r--r--   0 runner    (1001) docker     (127)    11370 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/layer/normalization.py
--rw-r--r--   0 runner    (1001) docker     (127)     8403 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/layer/parallel_module.py
--rw-r--r--   0 runner    (1001) docker     (127)    31070 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/layer/qkv_fused_linear.py
--rw-r--r--   0 runner    (1001) docker     (127)    10310 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/layer/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.425873 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    62017 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/bert.py
--rw-r--r--   0 runner    (1001) docker     (127)     4254 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/blip2.py
--rw-r--r--   0 runner    (1001) docker     (127)    49686 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)    17492 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/chatglm2.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.425873 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/chatglm2_6b/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/chatglm2_6b/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2249 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/chatglm2_6b/configuration_chatglm.py
--rw-r--r--   0 runner    (1001) docker     (127)    54608 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/chatglm2_6b/modeling_chatglm.py
--rw-r--r--   0 runner    (1001) docker     (127)    34661 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/falcon.py
--rw-r--r--   0 runner    (1001) docker     (127)    51397 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/gpt2.py
--rw-r--r--   0 runner    (1001) docker     (127)    37537 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/gptj.py
--rw-r--r--   0 runner    (1001) docker     (127)     1053 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/jit.py
--rw-r--r--   0 runner    (1001) docker     (127)    27316 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)     3235 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/mistral.py
--rw-r--r--   0 runner    (1001) docker     (127)    30439 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/opt.py
--rw-r--r--   0 runner    (1001) docker     (127)     8328 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/sam.py
--rw-r--r--   0 runner    (1001) docker     (127)    37936 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/t5.py
--rw-r--r--   0 runner    (1001) docker     (127)    15888 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/vit.py
--rw-r--r--   0 runner    (1001) docker     (127)    46618 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/whisper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.429873 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10382 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/auto_policy.py
--rw-r--r--   0 runner    (1001) docker     (127)    10322 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/base_policy.py
--rw-r--r--   0 runner    (1001) docker     (127)    25384 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/bert.py
--rw-r--r--   0 runner    (1001) docker     (127)    14089 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/blip2.py
--rw-r--r--   0 runner    (1001) docker     (127)    16402 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)    10609 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)    15893 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/falcon.py
--rw-r--r--   0 runner    (1001) docker     (127)    17970 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/gpt2.py
--rw-r--r--   0 runner    (1001) docker     (127)    12492 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/gptj.py
--rw-r--r--   0 runner    (1001) docker     (127)    13337 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)     7142 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/mistral.py
--rw-r--r--   0 runner    (1001) docker     (127)    12115 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/opt.py
--rw-r--r--   0 runner    (1001) docker     (127)     9404 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/sam.py
--rw-r--r--   0 runner    (1001) docker     (127)    19859 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/t5.py
--rw-r--r--   0 runner    (1001) docker     (127)    10056 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/vit.py
--rw-r--r--   0 runner    (1001) docker     (127)    21297 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/policies/whisper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.429873 colossalai-nightly-2024.3.9/colossalai/shardformer/shard/
--rw-r--r--   0 runner    (1001) docker     (127)      167 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/shard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4034 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/shard/shard_config.py
--rw-r--r--   0 runner    (1001) docker     (127)     9749 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/shard/sharder.py
--rw-r--r--   0 runner    (1001) docker     (127)     1787 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/shard/shardformer.py
--rw-r--r--   0 runner    (1001) docker     (127)      550 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/shardformer/shard/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.429873 colossalai-nightly-2024.3.9/colossalai/tensor/
--rw-r--r--   0 runner    (1001) docker     (127)      600 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3447 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/colo_parameter.py
--rw-r--r--   0 runner    (1001) docker     (127)     3349 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/colo_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)    21633 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/comm_spec.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.433873 colossalai-nightly-2024.3.9/colossalai/tensor/d_tensor/
--rw-r--r--   0 runner    (1001) docker     (127)     1203 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/d_tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    18027 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/d_tensor/api.py
--rw-r--r--   0 runner    (1001) docker     (127)    11144 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/d_tensor/comm_spec.py
--rw-r--r--   0 runner    (1001) docker     (127)     2776 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/d_tensor/layout.py
--rw-r--r--   0 runner    (1001) docker     (127)    26974 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/d_tensor/layout_converter.py
--rw-r--r--   0 runner    (1001) docker     (127)      231 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/d_tensor/misc.py
--rw-r--r--   0 runner    (1001) docker     (127)     9293 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/d_tensor/sharding_spec.py
--rw-r--r--   0 runner    (1001) docker     (127)     3263 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/d_tensor/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.433873 colossalai-nightly-2024.3.9/colossalai/tensor/moe_tensor/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/moe_tensor/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3778 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/moe_tensor/api.py
--rw-r--r--   0 runner    (1001) docker     (127)     1413 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/moe_tensor/moe_info.py
--rw-r--r--   0 runner    (1001) docker     (127)     5169 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/param_op_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)    35882 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/shape_consistency.py
--rw-r--r--   0 runner    (1001) docker     (127)    11626 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/sharding_spec.py
--rw-r--r--   0 runner    (1001) docker     (127)     8452 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/tensor/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.433873 colossalai-nightly-2024.3.9/colossalai/testing/
--rw-r--r--   0 runner    (1001) docker     (127)      865 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/testing/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5348 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/testing/comparison.py
--rw-r--r--   0 runner    (1001) docker     (127)     1406 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/testing/pytest_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (127)      542 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/testing/random.py
--rw-r--r--   0 runner    (1001) docker     (127)     9201 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/testing/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.433873 colossalai-nightly-2024.3.9/colossalai/utils/
--rw-r--r--   0 runner    (1001) docker     (127)      586 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1938 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/utils/common.py
--rw-r--r--   0 runner    (1001) docker     (127)     2470 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/utils/memory.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.433873 colossalai-nightly-2024.3.9/colossalai/utils/model/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/utils/model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3905 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/utils/model/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.433873 colossalai-nightly-2024.3.9/colossalai/utils/multi_tensor_apply/
--rw-r--r--   0 runner    (1001) docker     (127)      101 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/utils/multi_tensor_apply/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1142 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/utils/multi_tensor_apply/multi_tensor_apply.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.433873 colossalai-nightly-2024.3.9/colossalai/utils/rank_recorder/
--rw-r--r--   0 runner    (1001) docker     (127)       90 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/utils/rank_recorder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5453 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/utils/rank_recorder/rank_recorder.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.433873 colossalai-nightly-2024.3.9/colossalai/utils/tensor_detector/
--rw-r--r--   0 runner    (1001) docker     (127)       44 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/utils/tensor_detector/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8392 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/utils/tensor_detector/tensor_detector.py
--rw-r--r--   0 runner    (1001) docker     (127)     4321 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/utils/timer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.433873 colossalai-nightly-2024.3.9/colossalai/zero/
--rw-r--r--   0 runner    (1001) docker     (127)      390 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.437873 colossalai-nightly-2024.3.9/colossalai/zero/gemini/
--rw-r--r--   0 runner    (1001) docker     (127)      490 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.437873 colossalai-nightly-2024.3.9/colossalai/zero/gemini/chunk/
--rw-r--r--   0 runner    (1001) docker     (127)      342 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/chunk/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    25030 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/chunk/chunk.py
--rw-r--r--   0 runner    (1001) docker     (127)    11791 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/chunk/manager.py
--rw-r--r--   0 runner    (1001) docker     (127)     6291 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/chunk/search_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1428 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/chunk/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    41864 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/gemini_ddp.py
--rw-r--r--   0 runner    (1001) docker     (127)     2458 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/gemini_hook.py
--rw-r--r--   0 runner    (1001) docker     (127)     6243 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/gemini_mgr.py
--rw-r--r--   0 runner    (1001) docker     (127)    36398 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/gemini_optimizer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.437873 colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/
--rw-r--r--   0 runner    (1001) docker     (127)      511 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1281 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/chunk_memstats_collector.py
--rw-r--r--   0 runner    (1001) docker     (127)     4063 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/memory_monitor.py
--rw-r--r--   0 runner    (1001) docker     (127)     4101 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/memory_stats.py
--rw-r--r--   0 runner    (1001) docker     (127)     3591 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/memstats_collector.py
--rw-r--r--   0 runner    (1001) docker     (127)      858 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/param_runtime_order.py
--rw-r--r--   0 runner    (1001) docker     (127)     3716 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/runtime_mem_tracer.py
--rw-r--r--   0 runner    (1001) docker     (127)     4126 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/static_memstats_collector.py
--rw-r--r--   0 runner    (1001) docker     (127)     1795 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     9569 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/placement_policy.py
--rw-r--r--   0 runner    (1001) docker     (127)     4213 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/gemini/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.437873 colossalai-nightly-2024.3.9/colossalai/zero/low_level/
--rw-r--r--   0 runner    (1001) docker     (127)       88 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/low_level/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     7282 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/low_level/_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.441873 colossalai-nightly-2024.3.9/colossalai/zero/low_level/bookkeeping/
--rw-r--r--   0 runner    (1001) docker     (127)      242 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/low_level/bookkeeping/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      409 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/low_level/bookkeeping/base_store.py
--rw-r--r--   0 runner    (1001) docker     (127)     4487 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/low_level/bookkeeping/bucket_store.py
--rw-r--r--   0 runner    (1001) docker     (127)     4136 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/low_level/bookkeeping/gradient_store.py
--rw-r--r--   0 runner    (1001) docker     (127)     1502 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/low_level/bookkeeping/parameter_store.py
--rw-r--r--   0 runner    (1001) docker     (127)     1494 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/low_level/bookkeeping/tensor_bucket.py
--rw-r--r--   0 runner    (1001) docker     (127)    42465 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/low_level/low_level_optim.py
--rw-r--r--   0 runner    (1001) docker     (127)     4898 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/colossalai/zero/wrapper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.441873 colossalai-nightly-2024.3.9/colossalai_nightly.egg-info/
--rw-r--r--   0 runner    (1001) docker     (127)    34234 2024-03-09 00:13:38.000000 colossalai-nightly-2024.3.9/colossalai_nightly.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    48107 2024-03-09 00:13:38.000000 colossalai-nightly-2024.3.9/colossalai_nightly.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-03-09 00:13:38.000000 colossalai-nightly-2024.3.9/colossalai_nightly.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (127)       69 2024-03-09 00:13:38.000000 colossalai-nightly-2024.3.9/colossalai_nightly.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (127)      150 2024-03-09 00:13:38.000000 colossalai-nightly-2024.3.9/colossalai_nightly.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (127)       37 2024-03-09 00:13:38.000000 colossalai-nightly-2024.3.9/colossalai_nightly.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.305872 colossalai-nightly-2024.3.9/examples/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.441873 colossalai-nightly-2024.3.9/examples/language/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/examples/language/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4226 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/examples/language/data_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      713 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/examples/language/model_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4385 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/examples/language/performance_evaluator.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.441873 colossalai-nightly-2024.3.9/extensions/
--rw-r--r--   0 runner    (1001) docker     (127)     1156 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2517 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/base_extension.py
--rw-r--r--   0 runner    (1001) docker     (127)     4730 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/cpp_extension.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.441873 colossalai-nightly-2024.3.9/extensions/cpu_adam/
--rw-r--r--   0 runner    (1001) docker     (127)      151 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/cpu_adam/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1043 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/cpu_adam/cpu_adam_arm.py
--rw-r--r--   0 runner    (1001) docker     (127)     1660 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/cpu_adam/cpu_adam_x86.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.441873 colossalai-nightly-2024.3.9/extensions/csrc/
--rw-r--r--   0 runner    (1001) docker     (127)      350 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.441873 colossalai-nightly-2024.3.9/extensions/csrc/arm/
--rw-r--r--   0 runner    (1001) docker     (127)    12939 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/arm/cpu_adam_arm.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     5960 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/arm/cpu_adam_arm.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.445873 colossalai-nightly-2024.3.9/extensions/csrc/cuda/
--rw-r--r--   0 runner    (1001) docker     (127)     2606 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/colossal_C_frontend.cpp
--rw-r--r--   0 runner    (1001) docker     (127)      214 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/compat.h
--rw-r--r--   0 runner    (1001) docker     (127)    18561 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/cpu_adam.cpp
--rw-r--r--   0 runner    (1001) docker     (127)     5965 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/cpu_adam.h
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.445873 colossalai-nightly-2024.3.9/extensions/csrc/cuda/include/
--rw-r--r--   0 runner    (1001) docker     (127)     8585 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/include/block_reduce.h
--rw-r--r--   0 runner    (1001) docker     (127)     4878 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/layer_norm_cuda.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    25829 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/layer_norm_cuda_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     3906 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/moe_cuda.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    25627 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/moe_cuda_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     5046 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/multi_tensor_adam.cu
--rw-r--r--   0 runner    (1001) docker     (127)     5200 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/multi_tensor_apply.cuh
--rw-r--r--   0 runner    (1001) docker     (127)    13279 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)    13115 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/multi_tensor_lamb.cu
--rw-r--r--   0 runner    (1001) docker     (127)     4440 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/multi_tensor_scale_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     6479 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu
--rw-r--r--   0 runner    (1001) docker     (127)     2684 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/scaled_masked_softmax.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    21112 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/scaled_masked_softmax.h
--rw-r--r--   0 runner    (1001) docker     (127)     3467 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/scaled_masked_softmax_cuda.cu
--rw-r--r--   0 runner    (1001) docker     (127)     2066 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp
--rw-r--r--   0 runner    (1001) docker     (127)    23530 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h
--rw-r--r--   0 runner    (1001) docker     (127)     2818 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu
--rw-r--r--   0 runner    (1001) docker     (127)    14851 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/cuda/type_shim.h
--rw-r--r--   0 runner    (1001) docker     (127)     6588 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/csrc/scaled_softmax.py
--rw-r--r--   0 runner    (1001) docker     (127)     3896 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/cuda_extension.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.449873 colossalai-nightly-2024.3.9/extensions/flash_attention/
--rw-r--r--   0 runner    (1001) docker     (127)      527 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/flash_attention/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3502 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/flash_attention/flash_attention_dao_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)     2360 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/flash_attention/flash_attention_npu.py
--rw-r--r--   0 runner    (1001) docker     (127)     3622 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/flash_attention/flash_attention_xformers_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.449873 colossalai-nightly-2024.3.9/extensions/layernorm/
--rw-r--r--   0 runner    (1001) docker     (127)       88 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/layernorm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      822 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/layernorm/layernorm_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.449873 colossalai-nightly-2024.3.9/extensions/moe/
--rw-r--r--   0 runner    (1001) docker     (127)       70 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/moe/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      959 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/moe/moe_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.449873 colossalai-nightly-2024.3.9/extensions/optimizer/
--rw-r--r--   0 runner    (1001) docker     (127)      104 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/optimizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1104 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/optimizer/fused_optimizer_cuda.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.449873 colossalai-nightly-2024.3.9/extensions/softmax/
--rw-r--r--   0 runner    (1001) docker     (127)      271 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/softmax/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1027 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/softmax/scaled_masked_softmax_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py
--rw-r--r--   0 runner    (1001) docker     (127)      589 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/triton_extension.py
--rw-r--r--   0 runner    (1001) docker     (127)     8284 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/extensions/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.449873 colossalai-nightly-2024.3.9/requirements/
--rw-r--r--   0 runner    (1001) docker     (127)      217 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/requirements/requirements-infer.txt
--rw-r--r--   0 runner    (1001) docker     (127)      558 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/requirements/requirements-test.txt
--rw-r--r--   0 runner    (1001) docker     (127)      150 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/requirements/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (127)       38 2024-03-09 00:13:38.465873 colossalai-nightly-2024.3.9/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (127)     4397 2024-03-09 00:13:37.000000 colossalai-nightly-2024.3.9/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.309872 colossalai-nightly-2024.3.9/tests/
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.449873 colossalai-nightly-2024.3.9/tests/kit/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.449873 colossalai-nightly-2024.3.9/tests/kit/model_zoo/
--rw-r--r--   0 runner    (1001) docker     (127)     1069 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.449873 colossalai-nightly-2024.3.9/tests/kit/model_zoo/custom/
--rw-r--r--   0 runner    (1001) docker     (127)      129 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/custom/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      832 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/custom/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     1140 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/custom/hanging_param_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     1206 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/custom/nested_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     1206 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/custom/repeated_computed_layers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1259 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/custom/simple_net.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.449873 colossalai-nightly-2024.3.9/tests/kit/model_zoo/diffusers/
--rw-r--r--   0 runner    (1001) docker     (127)       25 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/diffusers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2534 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/diffusers/diffusers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1499 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/executor.py
--rw-r--r--   0 runner    (1001) docker     (127)     3412 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/registry.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.449873 colossalai-nightly-2024.3.9/tests/kit/model_zoo/timm/
--rw-r--r--   0 runner    (1001) docker     (127)       20 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/timm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5975 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/timm/timm.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.453873 colossalai-nightly-2024.3.9/tests/kit/model_zoo/torchaudio/
--rw-r--r--   0 runner    (1001) docker     (127)       26 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/torchaudio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4614 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/torchaudio/torchaudio.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.453873 colossalai-nightly-2024.3.9/tests/kit/model_zoo/torchrec/
--rw-r--r--   0 runner    (1001) docker     (127)       24 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/torchrec/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3899 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/torchrec/torchrec.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.453873 colossalai-nightly-2024.3.9/tests/kit/model_zoo/torchvision/
--rw-r--r--   0 runner    (1001) docker     (127)       27 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/torchvision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4970 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/torchvision/torchvision.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.453873 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/
--rw-r--r--   0 runner    (1001) docker     (127)      408 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3768 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/albert.py
--rw-r--r--   0 runner    (1001) docker     (127)    12561 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/bert.py
--rw-r--r--   0 runner    (1001) docker     (127)     2289 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/blip2.py
--rw-r--r--   0 runner    (1001) docker     (127)     4487 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)     2313 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)     4251 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/falcon.py
--rw-r--r--   0 runner    (1001) docker     (127)     5466 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/gpt.py
--rw-r--r--   0 runner    (1001) docker     (127)     3597 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/gptj.py
--rw-r--r--   0 runner    (1001) docker     (127)     3097 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)     2754 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/mistral.py
--rw-r--r--   0 runner    (1001) docker     (127)     3063 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/opt.py
--rw-r--r--   0 runner    (1001) docker     (127)     1833 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/sam.py
--rw-r--r--   0 runner    (1001) docker     (127)     3061 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/t5.py
--rw-r--r--   0 runner    (1001) docker     (127)     2214 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/vit.py
--rw-r--r--   0 runner    (1001) docker     (127)     3717 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/whisper.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.453873 colossalai-nightly-2024.3.9/tests/test_analyzer/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_analyzer/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.457873 colossalai-nightly-2024.3.9/tests/test_analyzer/test_fx/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_analyzer/test_fx/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3684 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_analyzer/test_fx/test_bias_addition.py
--rw-r--r--   0 runner    (1001) docker     (127)     2139 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_analyzer/test_fx/test_mod_dir.py
--rw-r--r--   0 runner    (1001) docker     (127)     1750 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_analyzer/test_fx/test_nested_ckpt.py
--rw-r--r--   0 runner    (1001) docker     (127)     2127 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_analyzer/test_fx/test_shape_prop.py
--rw-r--r--   0 runner    (1001) docker     (127)     1718 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_analyzer/test_fx/test_symbolic_profile.py
--rw-r--r--   0 runner    (1001) docker     (127)     1271 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_analyzer/test_fx/zoo.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.457873 colossalai-nightly-2024.3.9/tests/test_analyzer/test_subclasses/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_analyzer/test_subclasses/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3714 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_analyzer/test_subclasses/test_aten.py
--rw-r--r--   0 runner    (1001) docker     (127)     1893 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_analyzer/test_subclasses/test_flop_tensor.py
--rw-r--r--   0 runner    (1001) docker     (127)     1518 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_analyzer/test_subclasses/test_meta_mode.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.457873 colossalai-nightly-2024.3.9/tests/test_auto_parallel/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.457873 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_pass/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_pass/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1773 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_pass/test_node_converting_pass.py
--rw-r--r--   0 runner    (1001) docker     (127)     2250 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.457873 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2724 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py
--rw-r--r--   0 runner    (1001) docker     (127)     2003 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_broadcast.py
--rw-r--r--   0 runner    (1001) docker     (127)     2457 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py
--rw-r--r--   0 runner    (1001) docker     (127)     3394 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py
--rw-r--r--   0 runner    (1001) docker     (127)     3744 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py
--rw-r--r--   0 runner    (1001) docker     (127)     3674 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.457873 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_gpt/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_gpt/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    10992 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py
--rw-r--r--   0 runner    (1001) docker     (127)     7740 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py
--rw-r--r--   0 runner    (1001) docker     (127)     3818 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py
--rw-r--r--   0 runner    (1001) docker     (127)     2052 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.461873 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11142 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     7532 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4712 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_batch_norm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     6559 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_function_node.py
--rw-r--r--   0 runner    (1001) docker     (127)     6045 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_module_node.py
--rw-r--r--   0 runner    (1001) docker     (127)    10730 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_binary_elementwise_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     8739 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bmm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    12596 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_conv_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3595 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_default_reshape_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    11448 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_embedding_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2942 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getattr_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     7719 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getitem_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4242 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_layer_norm_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    12847 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_linear_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     8338 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_matmul_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2552 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_norm_pooling_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2556 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_output_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    18275 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_permute_and_transpose_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3056 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_placeholder_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     4168 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_shard_option.py
--rw-r--r--   0 runner    (1001) docker     (127)     8709 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_softmax_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    12227 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    12148 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     2781 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py
--rw-r--r--   0 runner    (1001) docker     (127)     3625 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)    13858 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     3556 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py
--rw-r--r--   0 runner    (1001) docker     (127)     8505 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     5283 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.461873 colossalai-nightly-2024.3.9/tests/test_shardformer/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:38.465873 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    12151 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     7653 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_bert.py
--rw-r--r--   0 runner    (1001) docker     (127)     2888 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_blip2.py
--rw-r--r--   0 runner    (1001) docker     (127)     7035 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_bloom.py
--rw-r--r--   0 runner    (1001) docker     (127)     7154 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_chatglm2.py
--rw-r--r--   0 runner    (1001) docker     (127)     6670 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_falcon.py
--rw-r--r--   0 runner    (1001) docker     (127)     7430 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_gpt2.py
--rw-r--r--   0 runner    (1001) docker     (127)     7209 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_gptj.py
--rw-r--r--   0 runner    (1001) docker     (127)     7417 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_llama.py
--rw-r--r--   0 runner    (1001) docker     (127)     5225 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_mistral.py
--rw-r--r--   0 runner    (1001) docker     (127)     6765 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_opt.py
--rw-r--r--   0 runner    (1001) docker     (127)     2568 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_sam.py
--rw-r--r--   0 runner    (1001) docker     (127)     6953 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_t5.py
--rw-r--r--   0 runner    (1001) docker     (127)     6550 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_vit.py
--rw-r--r--   0 runner    (1001) docker     (127)     7314 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_whisper.py
--rw-r--r--   0 runner    (1001) docker     (127)      833 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_shard_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2634 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/tests/test_shardformer/test_with_torch_ddp.py
--rw-r--r--   0 runner    (1001) docker     (127)        6 2024-03-09 00:13:29.000000 colossalai-nightly-2024.3.9/version.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.429213 colossalai-nightly-2024.5.4/
+-rw-r--r--   0 runner    (1001) docker     (127)    30134 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (127)      198 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (127)    36765 2024-05-04 00:14:38.429213 colossalai-nightly-2024.5.4/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    31195 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_C/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_C/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      597 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_analyzer/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/
+-rw-r--r--   0 runner    (1001) docker     (127)      165 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20868 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/_meta_registration.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2088 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/_monkey_patch.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18677 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/flop_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7589 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/meta_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)      114 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/envs.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/
+-rw-r--r--   0 runner    (1001) docker     (127)      129 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18998 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/codegen.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9947 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/graph_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8482 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/node_util.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/
+-rw-r--r--   0 runner    (1001) docker     (127)      106 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12708 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/graph_profile.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9939 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/shape_prop.py
+-rw-r--r--   0 runner    (1001) docker     (127)      952 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/symbolic_profile.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.281213 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/
+-rw-r--r--   0 runner    (1001) docker     (127)       63 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5415 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/bias_addition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1108 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/custom_leaf_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3568 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/proxy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5862 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/symbolic_trace.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15712 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/tracer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/accelerator/
+-rw-r--r--   0 runner    (1001) docker     (127)      431 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9402 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/base_accelerator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10154 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/cpu_accelerator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9442 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/cuda_accelerator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9453 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/accelerator/npu_accelerator.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/amp/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/
+-rw-r--r--   0 runner    (1001) docker     (127)      222 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2119 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/base_grad_scaler.py
+-rw-r--r--   0 runner    (1001) docker     (127)      735 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/constant_grad_scaler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4977 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/dynamic_grad_scaler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/
+-rw-r--r--   0 runner    (1001) docker     (127)      226 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2523 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)      504 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/bf16.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2695 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/fp16.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7959 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.285213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/
+-rw-r--r--   0 runner    (1001) docker     (127)      155 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      377 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/build_c_ext.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7711 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3468 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18538 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4921 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/operation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.289213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/
+-rw-r--r--   0 runner    (1001) docker     (127)       95 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      296 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.289213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/
+-rw-r--r--   0 runner    (1001) docker     (127)      241 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3940 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/activation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2840 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7571 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/conv.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2512 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24482 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1028 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9318 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/norm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7392 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3258 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2827 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/where.py
+-rw-r--r--   0 runner    (1001) docker     (127)      761 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4748 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/shard_metainfo.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.289213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6826 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/amp_optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3584 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/base_offload_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2072 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/mem_optimize.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5167 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/region.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20031 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/region_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9909 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/runtime.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18503 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/solver.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17919 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/training_simulator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2793 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/util.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.293213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5111 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/comm_metainfo_pass.py
+-rw-r--r--   0 runner    (1001) docker     (127)      417 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6050 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/meta_info_prop.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11328 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/runtime_apply_pass.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22439 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/runtime_preparation_pass.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.293213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/pipeline_shard/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/pipeline_shard/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.293213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2805 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16998 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.297213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/
+-rw-r--r--   0 runner    (1001) docker     (127)     2062 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3731 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/addmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3070 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/batch_norm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4988 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/binary_elementwise_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4805 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/bmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5563 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/conv_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2798 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/default_reshape_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11414 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/embedding_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1316 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/getattr_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1734 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/getitem_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1901 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/layer_norm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13535 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/linear_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20347 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/matmul_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16308 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/node_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1762 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/normal_pooling_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2115 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/output_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2997 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/permute_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1466 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/placeholder_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)      744 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1980 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/softmax_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2238 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/split_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.301213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/
+-rw-r--r--   0 runner    (1001) docker     (127)     2100 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14274 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/batch_norm_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5162 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/binary_elementwise_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24078 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/conv_strategy_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12070 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/embedding_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3615 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getattr_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7361 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getitem_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9225 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/layer_norm_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    41664 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/matmul_strategy_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5538 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/normal_pooling_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4597 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/output_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3636 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/placeholder_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18860 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/reshape_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4702 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/softmax_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13053 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/strategy_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4965 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/sum_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2278 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/tensor_constructor_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3981 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/unary_elementwise_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4158 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/where_generator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3053 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/sum_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1156 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/tensor_constructor_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2330 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/transpose_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1774 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/unary_elementwise_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1964 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/view_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3553 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/where_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1522 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/options.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10812 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/sharding_strategy.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.301213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/
+-rw-r--r--   0 runner    (1001) docker     (127)      238 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9987 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/cost_graph.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5767 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/graph_analysis.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20206 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/solver.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8474 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/strategies_constructor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.301213 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)     1210 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5899 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/broadcast.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8346 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/factory.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3841 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/misc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9066 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/reshape.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4408 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/sharding.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.301213 colossalai-nightly-2024.5.4/colossalai/booster/
+-rw-r--r--   0 runner    (1001) docker     (127)       93 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1481 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/accelerator.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20278 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/booster.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.305213 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/
+-rw-r--r--   0 runner    (1001) docker     (127)     1298 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      102 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/bf16.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3218 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_apex.py
+-rw-r--r--   0 runner    (1001) docker     (127)      870 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_naive.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4824 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_torch.py
+-rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp8.py
+-rw-r--r--   0 runner    (1001) docker     (127)      565 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/mixed_precision_base.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.305213 colossalai-nightly-2024.5.4/colossalai/booster/plugin/
+-rw-r--r--   0 runner    (1001) docker     (127)      529 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3098 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/dp_plugin_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28131 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/gemini_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    63839 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/hybrid_parallel_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19546 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/low_level_zero_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20853 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2475 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/plugin_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)      559 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/pp_plugin_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9770 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/torch_ddp_plugin.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15017 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/booster/plugin/torch_fsdp_plugin.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.305213 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/
+-rw-r--r--   0 runner    (1001) docker     (127)      318 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15804 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/checkpoint_io_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8761 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/general_checkpoint_io.py
+-rw-r--r--   0 runner    (1001) docker     (127)    43972 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/hybrid_parallel_checkpoint_io.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5758 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/index_file.py
+-rw-r--r--   0 runner    (1001) docker     (127)    29632 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/checkpoint_io/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.305213 colossalai-nightly-2024.5.4/colossalai/cli/
+-rw-r--r--   0 runner    (1001) docker     (127)       40 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.305213 colossalai-nightly-2024.5.4/colossalai/cli/check/
+-rw-r--r--   0 runner    (1001) docker     (127)      396 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/check/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8454 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/check/check_installation.py
+-rw-r--r--   0 runner    (1001) docker     (127)      310 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/cli.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/cli/launcher/
+-rw-r--r--   0 runner    (1001) docker     (127)     3654 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/launcher/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3214 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/launcher/hostinfo.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4286 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/launcher/multinode_runner.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10530 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cli/launcher/run.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/cluster/
+-rw-r--r--   0 runner    (1001) docker     (127)      296 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cluster/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4241 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cluster/device_mesh_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7233 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cluster/dist_coordinator.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2356 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cluster/process_group_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10848 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/cluster/process_group_mesh.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/context/
+-rw-r--r--   0 runner    (1001) docker     (127)       96 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/context/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3153 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/context/config.py
+-rw-r--r--   0 runner    (1001) docker     (127)      921 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/context/singleton_meta.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/device/
+-rw-r--r--   0 runner    (1001) docker     (127)      139 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/device/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17443 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/device/alpha_beta_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5634 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/device/calc_pipeline_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23616 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/device/device_mesh.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/fx/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1548 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/_compatibility.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19328 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/_meta_regist_12.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1906 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/_meta_regist_13.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.309213 colossalai-nightly-2024.5.4/colossalai/fx/codegen/
+-rw-r--r--   0 runner    (1001) docker     (127)       45 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/codegen/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    45231 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/codegen/activation_checkpoint_codegen.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7438 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/graph_module.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.313213 colossalai-nightly-2024.5.4/colossalai/fx/passes/
+-rw-r--r--   0 runner    (1001) docker     (127)      266 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13496 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/adding_split_node_pass.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12261 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/concrete_info_prop.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14066 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/meta_info_prop.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15886 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/passes_for_gpt2_test.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6888 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/shard_1d_pass.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13714 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/split_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6169 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/passes/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.313213 colossalai-nightly-2024.5.4/colossalai/fx/profiler/
+-rw-r--r--   0 runner    (1001) docker     (127)      768 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      871 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6285 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/dataflow.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.313213 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/
+-rw-r--r--   0 runner    (1001) docker     (127)      282 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      782 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7062 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.317213 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/
+-rw-r--r--   0 runner    (1001) docker     (127)      211 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1304 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/activation_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3387 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/arithmetic.py
+-rw-r--r--   0 runner    (1001) docker     (127)      632 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)      438 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2027 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1188 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (127)      402 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/python_ops.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2188 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/torch_ops.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.317213 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/
+-rw-r--r--   0 runner    (1001) docker     (127)      252 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1055 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/activation_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2037 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6708 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (127)      424 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/dropout.py
+-rw-r--r--   0 runner    (1001) docker     (127)      431 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)      495 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1582 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1013 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3020 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/rnn.py
+-rw-r--r--   0 runner    (1001) docker     (127)      271 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/torch_op.py
+-rw-r--r--   0 runner    (1001) docker     (127)      603 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1050 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/shard_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2232 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/memory_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13214 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/opcount.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15377 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/profiler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4235 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/shard_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4990 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/profiler/tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4010 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/proxy.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.317213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/
+-rw-r--r--   0 runner    (1001) docker     (127)      201 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4850 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/_meta_trace.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2197 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/_symbolic_trace.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1479 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/_tracer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.317213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/
+-rw-r--r--   0 runner    (1001) docker     (127)       90 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/
+-rw-r--r--   0 runner    (1001) docker     (127)      193 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2798 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addbmm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2171 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addmm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4445 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/bias_addition_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)      745 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/linear.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/
+-rw-r--r--   0 runner    (1001) docker     (127)       78 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4395 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/bias_addition_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2370 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/conv.py
+-rw-r--r--   0 runner    (1001) docker     (127)      503 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26431 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/experimental.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/
+-rw-r--r--   0 runner    (1001) docker     (127)       62 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/
+-rw-r--r--   0 runner    (1001) docker     (127)      167 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/activation_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3200 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/arithmetic.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5707 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (127)      339 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)      517 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2047 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/python_ops.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5622 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/torch_ops.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/
+-rw-r--r--   0 runner    (1001) docker     (127)      180 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      428 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/activation_function.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4752 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/convolution.py
+-rw-r--r--   0 runner    (1001) docker     (127)      254 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)      400 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1129 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6769 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/pooling.py
+-rw-r--r--   0 runner    (1001) docker     (127)      644 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/rnn.py
+-rw-r--r--   0 runner    (1001) docker     (127)      834 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/registry.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24642 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/fx/tracer/tracer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.321213 colossalai-nightly-2024.5.4/colossalai/inference/
+-rw-r--r--   0 runner    (1001) docker     (127)      235 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/engine/
+-rw-r--r--   0 runner    (1001) docker     (127)       67 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8315 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/engine.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8949 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/microbatch_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/
+-rw-r--r--   0 runner    (1001) docker     (127)      225 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19778 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20627 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21828 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/llama.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/
+-rw-r--r--   0 runner    (1001) docker     (127)      360 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5408 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3438 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8485 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/llama.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/
+-rw-r--r--   0 runner    (1001) docker     (127)       90 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4483 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/batch_infer_state.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4582 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/kvcache_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/quant/
+-rw-r--r--   0 runner    (1001) docker     (127)       61 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/
+-rw-r--r--   0 runner    (1001) docker     (127)      155 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/cai_gptq/
+-rw-r--r--   0 runner    (1001) docker     (127)      372 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/cai_gptq/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13893 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/cai_gptq/cai_quant_linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1629 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/cai_gptq/gptq_op.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2423 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/gptq_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.325213 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/
+-rw-r--r--   0 runner    (1001) docker     (127)      297 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20063 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/base_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6500 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35519 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10276 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/parallel_linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6088 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/interface/
+-rw-r--r--   0 runner    (1001) docker     (127)      152 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/interface/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      851 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/interface/model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4120 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/interface/optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)      333 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/interface/pretrained.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/kernel/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/
+-rw-r--r--   0 runner    (1001) docker     (127)     1127 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2499 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/base_extension.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4730 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpp_extension.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpu_adam/
+-rw-r--r--   0 runner    (1001) docker     (127)      150 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpu_adam/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1025 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpu_adam/cpu_adam_arm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1624 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpu_adam/cpu_adam_x86.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/
+-rw-r--r--   0 runner    (1001) docker     (127)      350 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.329213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/arm/
+-rw-r--r--   0 runner    (1001) docker     (127)    12939 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     5960 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.333213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/
+-rw-r--r--   0 runner    (1001) docker     (127)     2606 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/colossal_C_frontend.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)      214 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/compat.h
+-rw-r--r--   0 runner    (1001) docker     (127)    18561 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/cpu_adam.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     5965 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/cpu_adam.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.333213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/include/
+-rw-r--r--   0 runner    (1001) docker     (127)     8585 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/include/block_reduce.h
+-rw-r--r--   0 runner    (1001) docker     (127)     4878 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    25829 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     3906 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/moe_cuda.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    25627 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/moe_cuda_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     5046 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_adam.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     5200 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_apply.cuh
+-rw-r--r--   0 runner    (1001) docker     (127)    13279 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    13115 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_lamb.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     4440 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_scale_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     6479 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     2684 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    21112 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.h
+-rw-r--r--   0 runner    (1001) docker     (127)     3467 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax_cuda.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     2066 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    23530 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h
+-rw-r--r--   0 runner    (1001) docker     (127)     2818 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    14851 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/type_shim.h
+-rw-r--r--   0 runner    (1001) docker     (127)     6588 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/scaled_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3878 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cuda_extension.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.333213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/
+-rw-r--r--   0 runner    (1001) docker     (127)      470 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3760 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/flash_attention_dao_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1922 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/flash_attention_npu.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1799 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/flash_attention_sdpa_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.333213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/layernorm/
+-rw-r--r--   0 runner    (1001) docker     (127)       89 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/layernorm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      822 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/layernorm/layernorm_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.337213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/moe/
+-rw-r--r--   0 runner    (1001) docker     (127)       71 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/moe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      959 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/moe/moe_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.337213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      105 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1104 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/optimizer/fused_optimizer_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.337213 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/
+-rw-r--r--   0 runner    (1001) docker     (127)      271 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1027 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/scaled_masked_softmax_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)      589 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/triton_extension.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8284 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/extensions/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.337213 colossalai-nightly-2024.5.4/colossalai/kernel/jit/
+-rw-r--r--   0 runner    (1001) docker     (127)      317 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/jit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      670 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/jit/bias_dropout_add.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1358 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/jit/bias_gelu.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3591 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/jit/option.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3649 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/kernel_loader.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.337213 colossalai-nightly-2024.5.4/colossalai/kernel/triton/
+-rw-r--r--   0 runner    (1001) docker     (127)     1096 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14981 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/context_attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2349 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/copy_kv_cache_dest.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7022 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/custom_autotune.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1804 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/flash_decoding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2960 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/fused_layernorm.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18024 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/gptq_triton.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3280 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/int8_rotary_embedding_kernel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6847 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/llama_act_combine_kernel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4967 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/qkv_matmul_kernel.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5947 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/self_attention_nofusion.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21773 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/smooth_attention.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3744 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/softmax.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7896 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/kernel/triton/token_attention_kernel.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/lazy/
+-rw-r--r--   0 runner    (1001) docker     (127)      107 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/lazy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2187 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/lazy/construction.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25011 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/lazy/lazy_init.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13895 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/lazy/pretrained.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/
+-rw-r--r--   0 runner    (1001) docker     (127)      301 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/amp/
+-rw-r--r--   0 runner    (1001) docker     (127)     2310 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      153 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/amp_type.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/amp/apex_amp/
+-rw-r--r--   0 runner    (1001) docker     (127)     1637 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/apex_amp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1073 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/apex_amp/apex_amp.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/
+-rw-r--r--   0 runner    (1001) docker     (127)     2453 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13449 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/_fp16_optimizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1741 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6081 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/naive_amp.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/
+-rw-r--r--   0 runner    (1001) docker     (127)     1538 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26771 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/_grad_scaler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3368 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/torch_amp.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.341213 colossalai-nightly-2024.5.4/colossalai/legacy/builder/
+-rw-r--r--   0 runner    (1001) docker     (127)      166 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/builder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3025 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/builder/builder.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/communication/
+-rw-r--r--   0 runner    (1001) docker     (127)      868 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11387 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/collective.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17484 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/p2p.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9047 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/p2p_v2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1945 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/ring.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5151 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/communication/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      978 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/context/
+-rw-r--r--   0 runner    (1001) docker     (127)      149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    24229 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/parallel_context.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1142 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/parallel_mode.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/
+-rw-r--r--   0 runner    (1001) docker     (127)      763 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2144 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_1d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6269 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_2d.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12944 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_2p5d.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13290 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_3d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2041 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_data.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2174 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2652 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_pipeline.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4170 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_sequence.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2051 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1180 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/process_group_initializer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/context/random/
+-rw-r--r--   0 runner    (1001) docker     (127)      420 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/random/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5204 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/random/_helper.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3407 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/context/random/seed_manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)      149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/core.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/engine/
+-rw-r--r--   0 runner    (1001) docker     (127)       87 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7784 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/_base_engine.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.345213 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_accumulation/
+-rw-r--r--   0 runner    (1001) docker     (127)     2558 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_accumulation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10265 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_accumulation/_gradient_accumulation.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.349213 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/
+-rw-r--r--   0 runner    (1001) docker     (127)      537 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      750 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_base_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_data_parallel_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2138 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_moe_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2488 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_pipeline_parallel_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1148 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_sequence_parallel_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)      749 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_zero_gradient_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1020 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.349213 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/
+-rw-r--r--   0 runner    (1001) docker     (127)      315 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5816 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_base_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3808 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_non_pipeline_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (127)    40085 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_pipeline_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7133 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_pipeline_schedule_v2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1993 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/global_variables.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.349213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/
+-rw-r--r--   0 runner    (1001) docker     (127)      152 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4606 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/async_engine.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5935 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/async_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.349213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1345 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/get_tokenizer.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13595 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/infer_batch.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5332 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/io_struct.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6304 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/ray_dist_init.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1612 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/ray_init_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2810 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/req_queue.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3282 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/sampling_params.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1514 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/stats.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.349213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/
+-rw-r--r--   0 runner    (1001) docker     (127)       65 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6972 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/engine.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/
+-rw-r--r--   0 runner    (1001) docker     (127)       80 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21591 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/llama.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/polices/
+-rw-r--r--   0 runner    (1001) docker     (127)       78 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/polices/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5477 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/polices/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11605 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/pipeline/
+-rw-r--r--   0 runner    (1001) docker     (127)       83 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9019 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/pipeline/microbatch_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/
+-rw-r--r--   0 runner    (1001) docker     (127)      123 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4483 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/batch_infer_state.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21286 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/engine.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4580 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/kvcache_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/
+-rw-r--r--   0 runner    (1001) docker     (127)      225 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2706 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23642 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22757 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17826 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/llama.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/
+-rw-r--r--   0 runner    (1001) docker     (127)      209 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4440 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2987 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4898 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19842 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/initialize.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/
+-rw-r--r--   0 runner    (1001) docker     (127)       63 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/_ops/
+-rw-r--r--   0 runner    (1001) docker     (127)       22 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/_ops/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8669 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/_ops/_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.353213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/
+-rw-r--r--   0 runner    (1001) docker     (127)      242 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2817 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/base_layer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.357213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/
+-rw-r--r--   0 runner    (1001) docker     (127)      300 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1404 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      999 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/dropout.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6156 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5224 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1718 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/normalization.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.357213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/
+-rw-r--r--   0 runner    (1001) docker     (127)      459 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3724 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5063 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    44898 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.357213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/
+-rw-r--r--   0 runner    (1001) docker     (127)      458 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35857 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)      853 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    49324 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.357213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/
+-rw-r--r--   0 runner    (1001) docker     (127)      494 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    39868 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1234 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    49527 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.357213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/
+-rw-r--r--   0 runner    (1001) docker     (127)      498 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    22832 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3037 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    49624 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/
+-rw-r--r--   0 runner    (1001) docker     (127)      152 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6392 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)      483 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10693 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)      445 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2411 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/utils/common.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/vanilla/
+-rw-r--r--   0 runner    (1001) docker     (127)      345 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/vanilla/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14693 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/vanilla/layers.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/wrapper/
+-rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/wrapper/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2170 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/wrapper/pipeline_wrapper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/
+-rw-r--r--   0 runner    (1001) docker     (127)     1592 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4606 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_1d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5732 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_2d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5540 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_2p5d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6436 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_3d.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/
+-rw-r--r--   0 runner    (1001) docker     (127)      686 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      148 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      787 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_2d.py
+-rw-r--r--   0 runner    (1001) docker     (127)      801 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_2p5d.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1263 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_3d.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.361213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/
+-rw-r--r--   0 runner    (1001) docker     (127)       65 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6469 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/data_parallel.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/
+-rw-r--r--   0 runner    (1001) docker     (127)      962 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/
+-rw-r--r--   0 runner    (1001) docker     (127)      742 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1165 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/base_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27958 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/cache_mgr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8710 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/cached_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2041 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/copyer.py
+-rw-r--r--   0 runner    (1001) docker     (127)      807 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/embedding_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5690 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9962 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7138 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise_split_cache.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1954 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/colo_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1150 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1136 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4780 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/module_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3874 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/reducer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/
+-rw-r--r--   0 runner    (1001) docker     (127)      163 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1558 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/layer_spec.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/
+-rw-r--r--   0 runner    (1001) docker     (127)      149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/adaptor/
+-rw-r--r--   0 runner    (1001) docker     (127)       79 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/adaptor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6151 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/adaptor/fx.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6818 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/topo.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11447 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/pipelinable.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5759 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/pipeline_process_group.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.365213 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/
+-rw-r--r--   0 runner    (1001) docker     (127)      237 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    59091 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/_pipeline_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14979 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/_pipeline_schedule.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5382 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9017 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/registry/
+-rw-r--r--   0 runner    (1001) docker     (127)      690 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/registry/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3052 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/registry/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/
+-rw-r--r--   0 runner    (1001) docker     (127)      418 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      779 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/compute_spec.py
+-rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/const.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8692 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/dist_spec_mgr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2714 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/distspec.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1678 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/op_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10505 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/process_group.py
+-rw-r--r--   0 runner    (1001) docker     (127)      735 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/tensor/tensor_spec.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/
+-rw-r--r--   0 runner    (1001) docker     (127)       53 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14773 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/_trainer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/
+-rw-r--r--   0 runner    (1001) docker     (127)      648 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2712 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_base_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3226 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_checkpoint_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)      231 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_commons_.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13085 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_log_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2091 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_lr_scheduler_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16242 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_metric_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)     1438 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9872 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/activation_checkpoint.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.369213 colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/
+-rw-r--r--   0 runner    (1001) docker     (127)      114 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5297 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/module_checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2149 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11338 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpointing.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16595 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/common.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/utils/data_sampler/
+-rw-r--r--   0 runner    (1001) docker     (127)      177 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/data_sampler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      339 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/data_sampler/base_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6704 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/data_sampler/data_parallel_sampler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6416 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/memory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/
+-rw-r--r--   0 runner    (1001) docker     (127)       52 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      341 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/extention.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/
+-rw-r--r--   0 runner    (1001) docker     (127)      266 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10461 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/comm_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4861 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/pcie_profiler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3776 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/prof_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8459 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/profiler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4036 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/stateful_tensor_mem_extention.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/
+-rw-r--r--   0 runner    (1001) docker     (127)     1570 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/
+-rw-r--r--   0 runner    (1001) docker     (127)      619 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7315 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/colo_init_context.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1518 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/gemini_context.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/
+-rw-r--r--   0 runner    (1001) docker     (127)      118 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      774 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/_shard_grad_ophook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1363 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/_shard_param_ophook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5081 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/runtime_mem_tracer_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4715 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/paramhooks/
+-rw-r--r--   0 runner    (1001) docker     (127)       77 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/paramhooks/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1251 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/paramhooks/_param_hookmgr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6905 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/stateful_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3965 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/stateful_tensor_mgr.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6369 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/tensor_placement_policy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3807 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/tensor_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.373213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/init_ctx/
+-rw-r--r--   0 runner    (1001) docker     (127)      171 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/init_ctx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11099 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/init_ctx/init_context.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.377213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/
+-rw-r--r--   0 runner    (1001) docker     (127)      259 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      635 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/base_shard_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2296 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/bucket_tensor_shard_strategy.py
+-rw-r--r--   0 runner    (1001) docker     (127)      706 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/commons.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2751 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/tensor_shard_strategy.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.377213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/
+-rw-r--r--   0 runner    (1001) docker     (127)       75 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3003 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8289 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/reduce_scatter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28757 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/sharded_model_v2.py
+-rw-r--r--   0 runner    (1001) docker     (127)      808 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4865 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/zero_hook.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.377213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_optim/
+-rw-r--r--   0 runner    (1001) docker     (127)       83 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_optim/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18982 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.377213 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/
+-rw-r--r--   0 runner    (1001) docker     (127)      131 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3859 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/sharded_param.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1144 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/sharded_tensor.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.377213 colossalai-nightly-2024.5.4/colossalai/logging/
+-rw-r--r--   0 runner    (1001) docker     (127)     1593 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/logging/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6110 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/logging/logger.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/moe/
+-rw-r--r--   0 runner    (1001) docker     (127)      531 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12432 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    36076 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6249 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/experts.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16261 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18267 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/load_balance.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3075 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/loss.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6138 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20401 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/routers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7403 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/moe/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/nn/
+-rw-r--r--   0 runner    (1001) docker     (127)      114 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9563 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/init.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/nn/layer/
+-rw-r--r--   0 runner    (1001) docker     (127)       21 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/layer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2497 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/layer/layernorm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6739 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/layer/scaled_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (127)      449 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/layer/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/nn/loss/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/loss/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/
+-rw-r--r--   0 runner    (1001) docker     (127)      673 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5867 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/cosine.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7717 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/delayed.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1178 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2672 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/multistep.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5050 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/onecycle.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2458 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/poly.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3516 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/torch.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.381213 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      303 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8611 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/cpu_adam.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6478 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_adam.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9056 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_lamb.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6012 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_sgd.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8004 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/hybrid_adam.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4423 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/lamb.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3567 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/lars.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6768 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/nn/optimizer/nvme_optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.385213 colossalai-nightly-2024.5.4/colossalai/pipeline/
+-rw-r--r--   0 runner    (1001) docker     (127)      344 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26607 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/p2p.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.385213 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/
+-rw-r--r--   0 runner    (1001) docker     (127)      241 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5194 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1478 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20009 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/generate.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26918 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/interleaved_pp.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19591 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/one_f_one_b.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9552 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/pipeline/stage_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.385213 colossalai-nightly-2024.5.4/colossalai/quantization/
+-rw-r--r--   0 runner    (1001) docker     (127)      144 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13038 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/quantization/bnb.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4399 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/quantization/bnb_config.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.385213 colossalai-nightly-2024.5.4/colossalai/shardformer/
+-rw-r--r--   0 runner    (1001) docker     (127)      118 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3327 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.385213 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/
+-rw-r--r--   0 runner    (1001) docker     (127)     1119 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    38285 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/_operation.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13198 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/attn.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3520 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/dropout.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15865 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/embedding.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26710 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5018 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/loss.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11530 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/normalization.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17676 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/parallel_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)    31848 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/qkv_fused_linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10572 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/layer/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.389213 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    62664 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/bert.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5064 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/blip2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    49669 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17629 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.389213 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2249 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/configuration_chatglm.py
+-rw-r--r--   0 runner    (1001) docker     (127)    54608 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/modeling_chatglm.py
+-rw-r--r--   0 runner    (1001) docker     (127)    34314 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/falcon.py
+-rw-r--r--   0 runner    (1001) docker     (127)    59838 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/gpt2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    44054 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/gptj.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1053 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/jit.py
+-rw-r--r--   0 runner    (1001) docker     (127)    46388 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27319 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/mistral.py
+-rw-r--r--   0 runner    (1001) docker     (127)    36942 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/opt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8328 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/sam.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37484 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/t5.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16136 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/vit.py
+-rw-r--r--   0 runner    (1001) docker     (127)    54363 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/whisper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.393213 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10821 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/auto_policy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8822 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/base_policy.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27227 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/bert.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15862 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/blip2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17356 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12232 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16474 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/falcon.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22241 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/gpt2.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14001 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/gptj.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18711 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14549 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/mistral.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13829 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/opt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9682 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/sam.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22174 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/t5.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10928 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/vit.py
+-rw-r--r--   0 runner    (1001) docker     (127)    23410 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/policies/whisper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.393213 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/
+-rw-r--r--   0 runner    (1001) docker     (127)      320 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3763 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/grad_ckpt_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6642 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/shard_config.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9749 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/sharder.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1778 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/shardformer.py
+-rw-r--r--   0 runner    (1001) docker     (127)      550 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/shardformer/shard/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.393213 colossalai-nightly-2024.5.4/colossalai/tensor/
+-rw-r--r--   0 runner    (1001) docker     (127)      600 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3447 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/colo_parameter.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3349 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/colo_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21633 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/comm_spec.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/
+-rw-r--r--   0 runner    (1001) docker     (127)     1203 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18034 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11144 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/comm_spec.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2776 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/layout.py
+-rw-r--r--   0 runner    (1001) docker     (127)    27564 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/layout_converter.py
+-rw-r--r--   0 runner    (1001) docker     (127)      231 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/misc.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9293 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/sharding_spec.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3263 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3778 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1413 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/moe_info.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/tensor/padded_tensor/
+-rw-r--r--   0 runner    (1001) docker     (127)      194 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/padded_tensor/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3527 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/padded_tensor/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5169 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/param_op_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)    35882 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/shape_consistency.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11626 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/sharding_spec.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8452 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/tensor/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/testing/
+-rw-r--r--   0 runner    (1001) docker     (127)      865 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/testing/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5538 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/testing/comparison.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1406 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/testing/pytest_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (127)      542 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/testing/random.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9201 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/testing/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)      586 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1938 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/common.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2470 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/memory.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/utils/model/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3905 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/model/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/utils/multi_tensor_apply/
+-rw-r--r--   0 runner    (1001) docker     (127)      101 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/multi_tensor_apply/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1142 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/multi_tensor_apply/multi_tensor_apply.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.397213 colossalai-nightly-2024.5.4/colossalai/utils/rank_recorder/
+-rw-r--r--   0 runner    (1001) docker     (127)       90 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/rank_recorder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5453 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/rank_recorder/rank_recorder.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/utils/tensor_detector/
+-rw-r--r--   0 runner    (1001) docker     (127)       44 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/tensor_detector/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8392 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/tensor_detector/tensor_detector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4321 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/utils/timer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/zero/
+-rw-r--r--   0 runner    (1001) docker     (127)      390 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/zero/gemini/
+-rw-r--r--   0 runner    (1001) docker     (127)      490 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/
+-rw-r--r--   0 runner    (1001) docker     (127)      342 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    25030 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/chunk.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11791 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/manager.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6291 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/search_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1428 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    42911 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_ddp.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2458 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_hook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6243 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_mgr.py
+-rw-r--r--   0 runner    (1001) docker     (127)    37706 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_optimizer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/
+-rw-r--r--   0 runner    (1001) docker     (127)      511 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1281 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/chunk_memstats_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4063 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memory_monitor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4101 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memory_stats.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3591 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memstats_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)      858 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/param_runtime_order.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3716 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/runtime_mem_tracer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4126 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/static_memstats_collector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1795 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9569 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/placement_policy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4213 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/gemini/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.401213 colossalai-nightly-2024.5.4/colossalai/zero/low_level/
+-rw-r--r--   0 runner    (1001) docker     (127)       88 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7283 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/
+-rw-r--r--   0 runner    (1001) docker     (127)      242 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      409 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/base_store.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4545 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/bucket_store.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4362 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/gradient_store.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1502 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/parameter_store.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1494 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/tensor_bucket.py
+-rw-r--r--   0 runner    (1001) docker     (127)    43271 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/low_level/low_level_optim.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4898 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/colossalai/zero/wrapper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)    36765 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    48382 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       69 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      205 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       37 2024-05-04 00:14:38.000000 colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.277213 colossalai-nightly-2024.5.4/examples/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/examples/language/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/examples/language/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4227 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/examples/language/data_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      713 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/examples/language/model_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4385 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/examples/language/performance_evaluator.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/extensions/
+-rw-r--r--   0 runner    (1001) docker     (127)     1127 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2499 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/base_extension.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4730 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/cpp_extension.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/extensions/cpu_adam/
+-rw-r--r--   0 runner    (1001) docker     (127)      150 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/cpu_adam/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1025 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/cpu_adam/cpu_adam_arm.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1624 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/cpu_adam/cpu_adam_x86.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.405213 colossalai-nightly-2024.5.4/extensions/csrc/
+-rw-r--r--   0 runner    (1001) docker     (127)      350 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.409213 colossalai-nightly-2024.5.4/extensions/csrc/arm/
+-rw-r--r--   0 runner    (1001) docker     (127)    12939 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/arm/cpu_adam_arm.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     5960 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/arm/cpu_adam_arm.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.409213 colossalai-nightly-2024.5.4/extensions/csrc/cuda/
+-rw-r--r--   0 runner    (1001) docker     (127)     2606 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/colossal_C_frontend.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)      214 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/compat.h
+-rw-r--r--   0 runner    (1001) docker     (127)    18561 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/cpu_adam.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)     5965 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/cpu_adam.h
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.409213 colossalai-nightly-2024.5.4/extensions/csrc/cuda/include/
+-rw-r--r--   0 runner    (1001) docker     (127)     8585 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/include/block_reduce.h
+-rw-r--r--   0 runner    (1001) docker     (127)     4878 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/layer_norm_cuda.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    25829 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/layer_norm_cuda_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     3906 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/moe_cuda.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    25627 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/moe_cuda_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     5046 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_adam.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     5200 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_apply.cuh
+-rw-r--r--   0 runner    (1001) docker     (127)    13279 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    13115 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_lamb.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     4440 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_scale_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     6479 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     2684 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_masked_softmax.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    21112 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_masked_softmax.h
+-rw-r--r--   0 runner    (1001) docker     (127)     3467 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_masked_softmax_cuda.cu
+-rw-r--r--   0 runner    (1001) docker     (127)     2066 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp
+-rw-r--r--   0 runner    (1001) docker     (127)    23530 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h
+-rw-r--r--   0 runner    (1001) docker     (127)     2818 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu
+-rw-r--r--   0 runner    (1001) docker     (127)    14851 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/cuda/type_shim.h
+-rw-r--r--   0 runner    (1001) docker     (127)     6588 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/csrc/scaled_softmax.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3878 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/cuda_extension.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/extensions/flash_attention/
+-rw-r--r--   0 runner    (1001) docker     (127)      470 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/flash_attention/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3760 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/flash_attention/flash_attention_dao_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1922 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/flash_attention/flash_attention_npu.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1799 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/flash_attention/flash_attention_sdpa_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/extensions/layernorm/
+-rw-r--r--   0 runner    (1001) docker     (127)       89 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/layernorm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      822 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/layernorm/layernorm_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/extensions/moe/
+-rw-r--r--   0 runner    (1001) docker     (127)       71 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/moe/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      959 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/moe/moe_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/extensions/optimizer/
+-rw-r--r--   0 runner    (1001) docker     (127)      105 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/optimizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1104 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/optimizer/fused_optimizer_cuda.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/extensions/softmax/
+-rw-r--r--   0 runner    (1001) docker     (127)      271 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/softmax/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1027 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/softmax/scaled_masked_softmax_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1098 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py
+-rw-r--r--   0 runner    (1001) docker     (127)      589 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/triton_extension.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8284 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/extensions/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/requirements/
+-rw-r--r--   0 runner    (1001) docker     (127)      217 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/requirements/requirements-infer.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      557 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/requirements/requirements-test.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      205 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/requirements/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       38 2024-05-04 00:14:38.429213 colossalai-nightly-2024.5.4/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     4379 2024-05-04 00:14:37.000000 colossalai-nightly-2024.5.4/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.277213 colossalai-nightly-2024.5.4/tests/
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/tests/kit/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/
+-rw-r--r--   0 runner    (1001) docker     (127)     1070 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/
+-rw-r--r--   0 runner    (1001) docker     (127)      129 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      832 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1140 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/hanging_param_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1206 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/nested_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1206 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/repeated_computed_layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1259 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/simple_net.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/diffusers/
+-rw-r--r--   0 runner    (1001) docker     (127)       25 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/diffusers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2534 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/diffusers/diffusers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1499 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/executor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3413 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/registry.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.413213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/timm/
+-rw-r--r--   0 runner    (1001) docker     (127)       20 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/timm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5975 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/timm/timm.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.417213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchaudio/
+-rw-r--r--   0 runner    (1001) docker     (127)       26 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchaudio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4614 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchaudio/torchaudio.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.417213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchrec/
+-rw-r--r--   0 runner    (1001) docker     (127)       24 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchrec/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3899 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchrec/torchrec.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.417213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchvision/
+-rw-r--r--   0 runner    (1001) docker     (127)       27 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchvision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4970 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchvision/torchvision.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.417213 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/
+-rw-r--r--   0 runner    (1001) docker     (127)      408 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3768 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/albert.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12561 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/bert.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2289 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/blip2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4487 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2282 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4251 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/falcon.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6361 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/gpt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3597 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/gptj.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3328 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2837 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/mistral.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3063 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/opt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1833 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/sam.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3061 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/t5.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2214 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/vit.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3717 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/whisper.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.417213 colossalai-nightly-2024.5.4/tests/test_analyzer/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3684 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_bias_addition.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2139 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_mod_dir.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1750 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_nested_ckpt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2127 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_shape_prop.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1718 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_symbolic_profile.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1271 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/zoo.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3714 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_aten.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1893 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_flop_tensor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1518 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_meta_mode.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_auto_parallel/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1773 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/test_node_converting_pass.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2250 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2702 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2003 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_broadcast.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2446 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3383 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3733 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3674 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.421213 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10992 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7729 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3818 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2052 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.425213 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11120 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7521 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4701 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_batch_norm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6548 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_function_node.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6034 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_module_node.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10708 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_binary_elementwise_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8717 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bmm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12574 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_conv_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3595 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_default_reshape_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11426 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_embedding_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2942 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getattr_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7708 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getitem_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4231 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_layer_norm_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12825 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_linear_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8338 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_matmul_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2552 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_norm_pooling_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2556 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_output_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    18264 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_permute_and_transpose_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3056 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_placeholder_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4168 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_shard_option.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8698 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_softmax_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12216 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12137 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2781 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3625 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13847 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3556 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8505 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5283 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.429213 colossalai-nightly-2024.5.4/tests/test_shardformer/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5683 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_flash_attention.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:38.429213 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12502 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8340 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_bert.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3196 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_blip2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7723 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_bloom.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8331 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_chatglm2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6763 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_falcon.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8635 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_gpt2.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7743 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_gptj.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12103 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5625 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_mistral.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7402 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_opt.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2557 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_sam.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7227 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_t5.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6528 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_vit.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7293 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_whisper.py
+-rw-r--r--   0 runner    (1001) docker     (127)      833 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_shard_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2623 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/tests/test_shardformer/test_with_torch_ddp.py
+-rw-r--r--   0 runner    (1001) docker     (127)        6 2024-05-04 00:14:29.000000 colossalai-nightly-2024.5.4/version.txt
```

### Comparing `colossalai-nightly-2024.3.9/LICENSE` & `colossalai-nightly-2024.5.4/LICENSE`

 * *Files 1% similar despite different names*

```diff
@@ -547,8 +547,23 @@
 
    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
-   THE SOFTWARE.  
+   THE SOFTWARE.
+   ---------------- LICENSE FOR Hugging Face accelerate ----------------
+
+   Copyright 2021 The HuggingFace Team
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
```

### Comparing `colossalai-nightly-2024.3.9/PKG-INFO` & `colossalai-nightly-2024.5.4/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: colossalai-nightly
-Version: 2024.3.9
+Version: 2024.5.4
 Summary: An integrated large-scale model training system with efficient parallelization techniques
 Home-page: https://www.colossalai.org
 License: Apache Software License 2.0
 Project-URL: Forum, https://github.com/hpcaitech/ColossalAI/discussions
 Project-URL: Bug Tracker, https://github.com/hpcaitech/ColossalAI/issues
 Project-URL: Examples, https://github.com/hpcaitech/ColossalAI-Examples
 Project-URL: Documentation, http://colossalai.readthedocs.io
@@ -32,14 +32,18 @@
         
         
            | [English](README.md) | [](docs/README-zh-Hans.md) |
         
         </div>
         
         ## Latest News
+        * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-resolution-in-open-source)
+        * [2024/04] [Most cost-effective solutions for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-pretraining-tailored-to-llama3-series)
+        * [2024/03] [314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
+        * [2024/03] [Open-Sora: Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models](https://hpc-ai.com/blog/open-sora-v1.0)
         * [2024/03] [Open-SoraSora Replication Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million](https://hpc-ai.com/blog/open-sora)
         * [2024/01] [Inference Performance Improved by 46%, Open Source Solution Breaks the Length Limit of LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-SwiftInfer)
         * [2024/01] [Construct Refined 13B Private Model With Just $5000 USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/colossal-llama-2-13b)
         * [2023/11] [Enhanced MoE Parallelism, Open-source MoE Model Training Can Be 9 Times More Efficient](https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-efficient)
         * [2023/09] [One Half-Day of Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-Free Domain-Specific LLM Solution](https://www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-large-models-open-source-and-commercial-free-domain-specific-llm-solution)
         * [2023/09] [70 Billion Parameter LLaMA2 Model Training Accelerated by 195%](https://www.hpc-ai.tech/blog/70b-llama2-training)
         * [2023/07] [HPC-AI Tech Raises 22 Million USD in Series A Funding](https://www.hpc-ai.tech/blog/hpc-ai-tech-raises-22-million-usd-in-series-a-funding-to-fuel-team-expansion-and-business-growth)
@@ -47,25 +51,25 @@
         ## Table of Contents
         <ul>
          <li><a href="#Why-Colossal-AI">Why Colossal-AI</a> </li>
          <li><a href="#Features">Features</a> </li>
          <li>
            <a href="#Colossal-AI-in-the-Real-World">Colossal-AI for Real World Applications</a>
            <ul>
-             <li><a href="#Open-Sora">Open-Sora: Open-SoraSora Replication Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million</a></li>
+             <li><a href="#Open-Sora">Open-Sora: Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models</a></li>
              <li><a href="#Colossal-LLaMA-2">Colossal-LLaMA-2: One Half-Day of Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-Free Domain-Specific Llm Solution</a></li>
              <li><a href="#ColossalChat">ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline</a></li>
              <li><a href="#AIGC">AIGC: Acceleration of Stable Diffusion</a></li>
              <li><a href="#Biomedicine">Biomedicine: Acceleration of AlphaFold Protein Structure</a></li>
            </ul>
          </li>
          <li>
            <a href="#Parallel-Training-Demo">Parallel Training Demo</a>
            <ul>
-             <li><a href="#LLaMA2">LLaMA 1/2</a></li>
+             <li><a href="#LLaMA3">LLaMA 1/2/3 </a></li>
              <li><a href="#MoE">MoE</a></li>
              <li><a href="#GPT-3">GPT-3</a></li>
              <li><a href="#GPT-2">GPT-2</a></li>
              <li><a href="#BERT">BERT</a></li>
              <li><a href="#PaLM">PaLM</a></li>
              <li><a href="#OPT">OPT</a></li>
              <li><a href="#ViT">ViT</a></li>
@@ -78,14 +82,15 @@
              <li><a href="#GPT-2-Single">GPT-2</a></li>
              <li><a href="#PaLM-Single">PaLM</a></li>
            </ul>
          </li>
          <li>
            <a href="#Inference">Inference</a>
            <ul>
+             <li><a href="#Grok-1">Grok-1: 314B model of PyTorch + HuggingFace Inference</a></li>
              <li><a href="#SwiftInfer">SwiftInfer:Breaks the Length Limit of LLM for Multi-Round Conversations with 46% Acceleration</a></li>
              <li><a href="#GPT-3-Inference">GPT-3</a></li>
              <li><a href="#OPT-Serving">OPT-175B Online Serving for Text Generation</a></li>
              <li><a href="#BLOOM-Inference">176B BLOOM</a></li>
            </ul>
          </li>
          <li>
@@ -133,26 +138,27 @@
           - Parallelism based on the configuration file
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         ## Colossal-AI in the Real World
         ### Open-Sora
         
-        [Open-Sora](https://github.com/hpcaitech/Open-Sora)Sora Replication Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million
+        [Open-Sora](https://github.com/hpcaitech/Open-Sora)Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models
         [[code]](https://github.com/hpcaitech/Open-Sora)
-        [[blog]](https://hpc-ai.com/blog/open-sora)
+        [[blog]](https://hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-resolution-in-open-source)
+        [[HuggingFace model weights]](https://huggingface.co/hpcai-tech/Open-Sora)
+        [[Demo]](https://github.com/hpcaitech/Open-Sora?tab=readme-ov-file#-latest-demo)
         
-        <p id="diffusion_demo" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/open-sora-1.png" width=600/>
-        </p>
-        
-        <p id="diffusion_demo" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/open-sora-2.png" width=600/>
-        </p>
+        <div align="center">
+           <a href="https://www.youtube.com/watch?v=iDTxepqixuc">
+           <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/sora-demo.png" width="700" />
+           </a>
+        </div>
         
+        <p align="right">(<a href="#top">back to top</a>)</p>
         
         ### Colossal-LLaMA-2
         
         - 7B: One half-day of training using a few hundred dollars yields similar results to mainstream large models, open-source and commercial-free domain-specific LLM solution.
         [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Colossal-LLaMA-2)
         [[blog]](https://www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-large-models-open-source-and-commercial-free-domain-specific-llm-solution)
         [[HuggingFace model weights]](https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-7b-base)
@@ -273,30 +279,38 @@
         
         - [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer): accelerating structure prediction of protein monomers and multimer by 11x.
         
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         ## Parallel Training Demo
+        ### LLaMA3
+        <p align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/LLaMA3-70B-H100.png" width=600/>
+        </p>
+        
+        - 70 billion parameter LLaMA3 model training accelerated by 18%
+        [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)
+        
         ### LLaMA2
         <p align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/llama2_pretraining.png" width=600/>
         </p>
         
         - 70 billion parameter LLaMA2 model training accelerated by 195%
-        [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama2)
+        [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)
         [[blog]](https://www.hpc-ai.tech/blog/70b-llama2-training)
         
         ### LLaMA1
         <p align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/LLaMA_pretraining.png" width=600/>
         </p>
         
         - 65-billion-parameter large model pretraining accelerated by 38%
-        [[code]](https://github.com/hpcaitech/ColossalAI/tree/example/llama/examples/language/llama)
+        [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)
         [[blog]](https://www.hpc-ai.tech/blog/large-model-pretraining)
         
         ### MoE
         <p align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/MOE_training.png" width=800/>
         </p>
         
@@ -370,14 +384,26 @@
         
         - 34x larger model size on the same hardware
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         
         ## Inference
+        ### Grok-1
+        <p id="Grok-1" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/grok-1-inference.jpg" width=600/>
+        </p>
+        
+         - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use Python + PyTorch + HuggingFace version for Inference.
+        
+        [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1)
+        [[blog]](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
+        [[HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/grok-1)
+        [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/models/colossalai/grok-1-pytorch/summary)
+        
         <p id="SwiftInfer" align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/SwiftInfer.jpg" width=800/>
         </p>
         
         - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance improved by 46%, open source solution breaks the length limit of LLM for multi-round conversations
         
         <p id="GPT-3-Inference" align="center">
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: colossalai-nightly Version: 2024.3.9 Summary: An
+Metadata-Version: 2.1 Name: colossalai-nightly Version: 2024.5.4 Summary: An
 integrated large-scale model training system with efficient parallelization
 techniques Home-page: https://www.colossalai.org License: Apache Software
 License 2.0 Project-URL: Forum, https://github.com/hpcaitech/ColossalAI/
 discussions Project-URL: Bug Tracker, https://github.com/hpcaitech/ColossalAI/
 issues Project-URL: Examples, https://github.com/hpcaitech/ColossalAI-Examples
 Project-URL: Documentation, http://colossalai.readthedocs.io Project-URL:
 Github, https://github.com/hpcaitech/ColossalAI Description: # Colossal-AI
@@ -22,59 +22,73 @@
  (https://img.shields.io/badge/%F0%9F%A4%97HuggingFace-Join-yellow)](https://
 huggingface.co/hpcai-tech) [![slack badge](https://img.shields.io/badge/Slack-
 join-blueviolet?logo=slack&)](https://github.com/hpcaitech/public_assets/tree/
  main/colossalai/contact/slack) [![WeChat badge](https://img.shields.io/badge/
 --green?logo=wechat&)](https://raw.githubusercontent.com/hpcaitech/
 public_assets/main/colossalai/img/WeChat.png) | [English](README.md) | []
                           (docs/README-zh-Hans.md) |
-## Latest News * [2024/03] [Open-SoraSora Replication Solution with 46% Cost
-Reduction, Sequence Expansion to Nearly a Million](https://hpc-ai.com/blog/
-open-sora) * [2024/01] [Inference Performance Improved by 46%, Open Source
-Solution Breaks the Length Limit of LLM for Multi-Round Conversations](https://
-hpc-ai.com/blog/Colossal-AI-SwiftInfer) * [2024/01] [Construct Refined 13B
-Private Model With Just $5000 USD, Upgraded Colossal-AI Llama-2 Open Source]
-(https://hpc-ai.com/blog/colossal-llama-2-13b) * [2023/11] [Enhanced MoE
-Parallelism, Open-source MoE Model Training Can Be 9 Times More Efficient]
-(https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-moe-model-
-training-can-be-9-times-more-efficient) * [2023/09] [One Half-Day of Training
-Using a Few Hundred Dollars Yields Similar Results to Mainstream Large Models,
-Open-Source and Commercial-Free Domain-Specific LLM Solution](https://www.hpc-
-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-
-similar-results-to-mainstream-large-models-open-source-and-commercial-free-
-domain-specific-llm-solution) * [2023/09] [70 Billion Parameter LLaMA2 Model
-Training Accelerated by 195%](https://www.hpc-ai.tech/blog/70b-llama2-training)
-* [2023/07] [HPC-AI Tech Raises 22 Million USD in Series A Funding](https://
-www.hpc-ai.tech/blog/hpc-ai-tech-raises-22-million-usd-in-series-a-funding-to-
-fuel-team-expansion-and-business-growth) ## Table of Contents
+## Latest News * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open
+Source with Single-Shot 16-Second Video Generation and 720p Resolution](https:/
+/hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-
+video-generation-and-720p-resolution-in-open-source) * [2024/04] [Most cost-
+effective solutions for inference, fine-tuning and pretraining, tailored to
+LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-
+inference-fine-tuning-and-pretraining-tailored-to-llama3-series) * [2024/03]
+[314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and
+Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-
+billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-
+use-pytorchhuggingface-version-is-here) * [2024/03] [Open-Sora: Revealing
+Complete Model Parameters, Training Details, and Everything for Sora-like Video
+Generation Models](https://hpc-ai.com/blog/open-sora-v1.0) * [2024/03] [Open-
+SoraSora Replication Solution with 46% Cost Reduction, Sequence Expansion to
+Nearly a Million](https://hpc-ai.com/blog/open-sora) * [2024/01] [Inference
+Performance Improved by 46%, Open Source Solution Breaks the Length Limit of
+LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-
+SwiftInfer) * [2024/01] [Construct Refined 13B Private Model With Just $5000
+USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/
+colossal-llama-2-13b) * [2023/11] [Enhanced MoE Parallelism, Open-source MoE
+Model Training Can Be 9 Times More Efficient](https://www.hpc-ai.tech/blog/
+enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-
+efficient) * [2023/09] [One Half-Day of Training Using a Few Hundred Dollars
+Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-
+Free Domain-Specific LLM Solution](https://www.hpc-ai.tech/blog/one-half-day-
+of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-
+large-models-open-source-and-commercial-free-domain-specific-llm-solution) *
+[2023/09] [70 Billion Parameter LLaMA2 Model Training Accelerated by 195%]
+(https://www.hpc-ai.tech/blog/70b-llama2-training) * [2023/07] [HPC-AI Tech
+Raises 22 Million USD in Series A Funding](https://www.hpc-ai.tech/blog/hpc-ai-
+tech-raises-22-million-usd-in-series-a-funding-to-fuel-team-expansion-and-
+business-growth) ## Table of Contents
     * _W_h_y_ _C_o_l_o_s_s_a_l_-_A_I
     * _F_e_a_t_u_r_e_s
     * _C_o_l_o_s_s_a_l_-_A_I_ _f_o_r_ _R_e_a_l_ _W_o_r_l_d_ _A_p_p_l_i_c_a_t_i_o_n_s
-          o _O_p_e_n_-_S_o_r_a_:_ _O_p_e_n_-_S_o_r_a____S_o_r_a_ _R_e_p_l_i_c_a_t_i_o_n_ _S_o_l_u_t_i_o_n_ _w_i_t_h_ _4_6_%_ _C_o_s_t
-            _R_e_d_u_c_t_i_o_n_,_ _S_e_q_u_e_n_c_e_ _E_x_p_a_n_s_i_o_n_ _t_o_ _N_e_a_r_l_y_ _a_ _M_i_l_l_i_o_n
+          o _O_p_e_n_-_S_o_r_a_:_ _R_e_v_e_a_l_i_n_g_ _C_o_m_p_l_e_t_e_ _M_o_d_e_l_ _P_a_r_a_m_e_t_e_r_s_,_ _T_r_a_i_n_i_n_g_ _D_e_t_a_i_l_s_,
+            _a_n_d_ _E_v_e_r_y_t_h_i_n_g_ _f_o_r_ _S_o_r_a_-_l_i_k_e_ _V_i_d_e_o_ _G_e_n_e_r_a_t_i_o_n_ _M_o_d_e_l_s
           o _C_o_l_o_s_s_a_l_-_L_L_a_M_A_-_2_:_ _O_n_e_ _H_a_l_f_-_D_a_y_ _o_f_ _T_r_a_i_n_i_n_g_ _U_s_i_n_g_ _a_ _F_e_w_ _H_u_n_d_r_e_d
             _D_o_l_l_a_r_s_ _Y_i_e_l_d_s_ _S_i_m_i_l_a_r_ _R_e_s_u_l_t_s_ _t_o_ _M_a_i_n_s_t_r_e_a_m_ _L_a_r_g_e_ _M_o_d_e_l_s_,_ _O_p_e_n_-
             _S_o_u_r_c_e_ _a_n_d_ _C_o_m_m_e_r_c_i_a_l_-_F_r_e_e_ _D_o_m_a_i_n_-_S_p_e_c_i_f_i_c_ _L_l_m_ _S_o_l_u_t_i_o_n
           o _C_o_l_o_s_s_a_l_C_h_a_t_:_ _A_n_ _O_p_e_n_-_S_o_u_r_c_e_ _S_o_l_u_t_i_o_n_ _f_o_r_ _C_l_o_n_i_n_g_ _C_h_a_t_G_P_T_ _W_i_t_h_ _a
             _C_o_m_p_l_e_t_e_ _R_L_H_F_ _P_i_p_e_l_i_n_e
           o _A_I_G_C_:_ _A_c_c_e_l_e_r_a_t_i_o_n_ _o_f_ _S_t_a_b_l_e_ _D_i_f_f_u_s_i_o_n
           o _B_i_o_m_e_d_i_c_i_n_e_:_ _A_c_c_e_l_e_r_a_t_i_o_n_ _o_f_ _A_l_p_h_a_F_o_l_d_ _P_r_o_t_e_i_n_ _S_t_r_u_c_t_u_r_e
     * _P_a_r_a_l_l_e_l_ _T_r_a_i_n_i_n_g_ _D_e_m_o
-          o _L_L_a_M_A_ _1_/_2
+          o _L_L_a_M_A_ _1_/_2_/_3
           o _M_o_E
           o _G_P_T_-_3
           o _G_P_T_-_2
           o _B_E_R_T
           o _P_a_L_M
           o _O_P_T
           o _V_i_T
           o _R_e_c_o_m_m_e_n_d_a_t_i_o_n_ _S_y_s_t_e_m_ _M_o_d_e_l_s
     * _S_i_n_g_l_e_ _G_P_U_ _T_r_a_i_n_i_n_g_ _D_e_m_o
           o _G_P_T_-_2
           o _P_a_L_M
     * _I_n_f_e_r_e_n_c_e
+          o _G_r_o_k_-_1_:_ _3_1_4_B_ _m_o_d_e_l_ _o_f_ _P_y_T_o_r_c_h_ _+_ _H_u_g_g_i_n_g_F_a_c_e_ _I_n_f_e_r_e_n_c_e
           o _S_w_i_f_t_I_n_f_e_r_:_B_r_e_a_k_s_ _t_h_e_ _L_e_n_g_t_h_ _L_i_m_i_t_ _o_f_ _L_L_M_ _f_o_r_ _M_u_l_t_i_-_R_o_u_n_d
             _C_o_n_v_e_r_s_a_t_i_o_n_s_ _w_i_t_h_ _4_6_%_ _A_c_c_e_l_e_r_a_t_i_o_n
           o _G_P_T_-_3
           o _O_P_T_-_1_7_5_B_ _O_n_l_i_n_e_ _S_e_r_v_i_n_g_ _f_o_r_ _T_e_x_t_ _G_e_n_e_r_a_t_i_o_n
           o _1_7_6_B_ _B_L_O_O_M
     * _I_n_s_t_a_l_l_a_t_i_o_n
           o _P_y_P_I
@@ -98,21 +112,24 @@
 (https://arxiv.org/abs/2105.13120) - [Zero Redundancy Optimizer (ZeRO)](https:/
 /arxiv.org/abs/1910.02054) - [Auto-Parallelism](https://arxiv.org/abs/
 2302.02599) - Heterogeneous Memory Management - [PatrickStar](https://
 arxiv.org/abs/2108.05818) - Friendly Usage - Parallelism based on the
 configuration file
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
 ## Colossal-AI in the Real World ### Open-Sora [Open-Sora](https://github.com/
-hpcaitech/Open-Sora)Sora Replication Solution with 46% Cost Reduction,
-Sequence Expansion to Nearly a Million [[code]](https://github.com/hpcaitech/
-Open-Sora) [[blog]](https://hpc-ai.com/blog/open-sora)
- [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
-                             sora/open-sora-1.png]
- [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
-                             sora/open-sora-2.png]
+hpcaitech/Open-Sora)Revealing Complete Model Parameters, Training Details,
+and Everything for Sora-like Video Generation Models [[code]](https://
+github.com/hpcaitech/Open-Sora) [[blog]](https://hpc-ai.com/blog/open-soras-
+comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-
+resolution-in-open-source) [[HuggingFace model weights]](https://
+huggingface.co/hpcai-tech/Open-Sora) [[Demo]](https://github.com/hpcaitech/
+Open-Sora?tab=readme-ov-file#-latest-demo)
+ _[_h_t_t_p_s_:_/_/_r_a_w_._g_i_t_h_u_b_u_s_e_r_c_o_n_t_e_n_t_._c_o_m_/_h_p_c_a_i_t_e_c_h_/_p_u_b_l_i_c___a_s_s_e_t_s_/_m_a_i_n_/_a_p_p_l_i_c_a_t_i_o_n_s_/
+                              _s_o_r_a_/_s_o_r_a_-_d_e_m_o_._p_n_g_]
+                                                                  (_b_a_c_k_ _t_o_ _t_o_p)
 ### Colossal-LLaMA-2 - 7B: One half-day of training using a few hundred dollars
 yields similar results to mainstream large models, open-source and commercial-
 free domain-specific LLM solution. [[code]](https://github.com/hpcaitech/
 ColossalAI/tree/main/applications/Colossal-LLaMA-2) [[blog]](https://www.hpc-
 ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-
 similar-results-to-mainstream-large-models-open-source-and-commercial-free-
 domain-specific-llm-solution) [[HuggingFace model weights]](https://
@@ -203,25 +220,29 @@
 - [FastFold with Intel](https://github.com/hpcaitech/FastFold): 3x inference
 acceleration and 39% cost reduce.
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                            xTrimoMultimer_Table.jpg]
 - [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer):
 accelerating structure prediction of protein monomers and multimer by 11x.
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Parallel Training Demo ### LLaMA2
+## Parallel Training Demo ### LLaMA3
+   [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
+                          images/LLaMA3-70B-H100.png]
+- 70 billion parameter LLaMA3 model training accelerated by 18% [[code]](https:
+//github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama) ### LLaMA2
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                             llama2_pretraining.png]
 - 70 billion parameter LLaMA2 model training accelerated by 195% [[code]]
-(https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama2) [
+(https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama) [
 [blog]](https://www.hpc-ai.tech/blog/70b-llama2-training) ### LLaMA1
    [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
                          images/LLaMA_pretraining.png]
 - 65-billion-parameter large model pretraining accelerated by 38% [[code]]
-(https://github.com/hpcaitech/ColossalAI/tree/example/llama/examples/language/
-llama) [[blog]](https://www.hpc-ai.tech/blog/large-model-pretraining) ### MoE
+(https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama) [
+[blog]](https://www.hpc-ai.tech/blog/large-model-pretraining) ### MoE
    [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
                            images/MOE_training.png]
 - Enhanced MoE parallelism, Open-source MoE model training can be 9 times more
 efficient [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/
 language/openmoe) [[blog]](https://www.hpc-ai.tech/blog/enhanced-moe-
 parallelism-open-source-moe-model-training-can-be-9-times-more-efficient) ###
 GPT-3
@@ -262,15 +283,25 @@
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 GPT2-NVME.png]
 - 120x larger model size on the same hardware (RTX 3080) ### PaLM
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 PaLM-GPU1.png]
 - 34x larger model size on the same hardware
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Inference
+## Inference ### Grok-1
+   [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
+                         images/grok-1-inference.jpg]
+- 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use
+Python + PyTorch + HuggingFace version for Inference. [[code]](https://
+github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1) [[blog]]
+(https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-
+3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here) [
+[HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/
+grok-1) [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/
+models/colossalai/grok-1-pytorch/summary)
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 SwiftInfer.jpg]
 - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance
 improved by 46%, open source solution breaks the length limit of LLM for multi-
 round conversations
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                              inference_GPT-3.jpg]
```

### Comparing `colossalai-nightly-2024.3.9/README.md` & `colossalai-nightly-2024.5.4/README.md`

 * *Files 7% similar despite different names*

```diff
@@ -21,14 +21,18 @@
 
 
    | [English](README.md) | [](docs/README-zh-Hans.md) |
 
 </div>
 
 ## Latest News
+* [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-resolution-in-open-source)
+* [2024/04] [Most cost-effective solutions for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-pretraining-tailored-to-llama3-series)
+* [2024/03] [314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
+* [2024/03] [Open-Sora: Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models](https://hpc-ai.com/blog/open-sora-v1.0)
 * [2024/03] [Open-SoraSora Replication Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million](https://hpc-ai.com/blog/open-sora)
 * [2024/01] [Inference Performance Improved by 46%, Open Source Solution Breaks the Length Limit of LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-SwiftInfer)
 * [2024/01] [Construct Refined 13B Private Model With Just $5000 USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/colossal-llama-2-13b)
 * [2023/11] [Enhanced MoE Parallelism, Open-source MoE Model Training Can Be 9 Times More Efficient](https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-efficient)
 * [2023/09] [One Half-Day of Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-Free Domain-Specific LLM Solution](https://www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-large-models-open-source-and-commercial-free-domain-specific-llm-solution)
 * [2023/09] [70 Billion Parameter LLaMA2 Model Training Accelerated by 195%](https://www.hpc-ai.tech/blog/70b-llama2-training)
 * [2023/07] [HPC-AI Tech Raises 22 Million USD in Series A Funding](https://www.hpc-ai.tech/blog/hpc-ai-tech-raises-22-million-usd-in-series-a-funding-to-fuel-team-expansion-and-business-growth)
@@ -36,25 +40,25 @@
 ## Table of Contents
 <ul>
  <li><a href="#Why-Colossal-AI">Why Colossal-AI</a> </li>
  <li><a href="#Features">Features</a> </li>
  <li>
    <a href="#Colossal-AI-in-the-Real-World">Colossal-AI for Real World Applications</a>
    <ul>
-     <li><a href="#Open-Sora">Open-Sora: Open-SoraSora Replication Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million</a></li>
+     <li><a href="#Open-Sora">Open-Sora: Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models</a></li>
      <li><a href="#Colossal-LLaMA-2">Colossal-LLaMA-2: One Half-Day of Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-Free Domain-Specific Llm Solution</a></li>
      <li><a href="#ColossalChat">ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline</a></li>
      <li><a href="#AIGC">AIGC: Acceleration of Stable Diffusion</a></li>
      <li><a href="#Biomedicine">Biomedicine: Acceleration of AlphaFold Protein Structure</a></li>
    </ul>
  </li>
  <li>
    <a href="#Parallel-Training-Demo">Parallel Training Demo</a>
    <ul>
-     <li><a href="#LLaMA2">LLaMA 1/2</a></li>
+     <li><a href="#LLaMA3">LLaMA 1/2/3 </a></li>
      <li><a href="#MoE">MoE</a></li>
      <li><a href="#GPT-3">GPT-3</a></li>
      <li><a href="#GPT-2">GPT-2</a></li>
      <li><a href="#BERT">BERT</a></li>
      <li><a href="#PaLM">PaLM</a></li>
      <li><a href="#OPT">OPT</a></li>
      <li><a href="#ViT">ViT</a></li>
@@ -67,14 +71,15 @@
      <li><a href="#GPT-2-Single">GPT-2</a></li>
      <li><a href="#PaLM-Single">PaLM</a></li>
    </ul>
  </li>
  <li>
    <a href="#Inference">Inference</a>
    <ul>
+     <li><a href="#Grok-1">Grok-1: 314B model of PyTorch + HuggingFace Inference</a></li>
      <li><a href="#SwiftInfer">SwiftInfer:Breaks the Length Limit of LLM for Multi-Round Conversations with 46% Acceleration</a></li>
      <li><a href="#GPT-3-Inference">GPT-3</a></li>
      <li><a href="#OPT-Serving">OPT-175B Online Serving for Text Generation</a></li>
      <li><a href="#BLOOM-Inference">176B BLOOM</a></li>
    </ul>
  </li>
  <li>
@@ -122,26 +127,27 @@
   - Parallelism based on the configuration file
 
 <p align="right">(<a href="#top">back to top</a>)</p>
 
 ## Colossal-AI in the Real World
 ### Open-Sora
 
-[Open-Sora](https://github.com/hpcaitech/Open-Sora)Sora Replication Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million
+[Open-Sora](https://github.com/hpcaitech/Open-Sora)Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models
 [[code]](https://github.com/hpcaitech/Open-Sora)
-[[blog]](https://hpc-ai.com/blog/open-sora)
+[[blog]](https://hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-resolution-in-open-source)
+[[HuggingFace model weights]](https://huggingface.co/hpcai-tech/Open-Sora)
+[[Demo]](https://github.com/hpcaitech/Open-Sora?tab=readme-ov-file#-latest-demo)
 
-<p id="diffusion_demo" align="center">
-<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/open-sora-1.png" width=600/>
-</p>
-
-<p id="diffusion_demo" align="center">
-<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/open-sora-2.png" width=600/>
-</p>
+<div align="center">
+   <a href="https://www.youtube.com/watch?v=iDTxepqixuc">
+   <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/sora-demo.png" width="700" />
+   </a>
+</div>
 
+<p align="right">(<a href="#top">back to top</a>)</p>
 
 ### Colossal-LLaMA-2
 
 - 7B: One half-day of training using a few hundred dollars yields similar results to mainstream large models, open-source and commercial-free domain-specific LLM solution.
 [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Colossal-LLaMA-2)
 [[blog]](https://www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-large-models-open-source-and-commercial-free-domain-specific-llm-solution)
 [[HuggingFace model weights]](https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-7b-base)
@@ -262,30 +268,38 @@
 
 - [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer): accelerating structure prediction of protein monomers and multimer by 11x.
 
 
 <p align="right">(<a href="#top">back to top</a>)</p>
 
 ## Parallel Training Demo
+### LLaMA3
+<p align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/LLaMA3-70B-H100.png" width=600/>
+</p>
+
+- 70 billion parameter LLaMA3 model training accelerated by 18%
+[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)
+
 ### LLaMA2
 <p align="center">
 <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/llama2_pretraining.png" width=600/>
 </p>
 
 - 70 billion parameter LLaMA2 model training accelerated by 195%
-[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama2)
+[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)
 [[blog]](https://www.hpc-ai.tech/blog/70b-llama2-training)
 
 ### LLaMA1
 <p align="center">
 <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/LLaMA_pretraining.png" width=600/>
 </p>
 
 - 65-billion-parameter large model pretraining accelerated by 38%
-[[code]](https://github.com/hpcaitech/ColossalAI/tree/example/llama/examples/language/llama)
+[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)
 [[blog]](https://www.hpc-ai.tech/blog/large-model-pretraining)
 
 ### MoE
 <p align="center">
 <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/MOE_training.png" width=800/>
 </p>
 
@@ -359,14 +373,26 @@
 
 - 34x larger model size on the same hardware
 
 <p align="right">(<a href="#top">back to top</a>)</p>
 
 
 ## Inference
+### Grok-1
+<p id="Grok-1" align="center">
+<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/grok-1-inference.jpg" width=600/>
+</p>
+
+ - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use Python + PyTorch + HuggingFace version for Inference.
+
+[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1)
+[[blog]](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
+[[HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/grok-1)
+[[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/models/colossalai/grok-1-pytorch/summary)
+
 <p id="SwiftInfer" align="center">
 <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/SwiftInfer.jpg" width=800/>
 </p>
 
 - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance improved by 46%, open source solution breaks the length limit of LLM for multi-round conversations
 
 <p id="GPT-3-Inference" align="center">
```

#### html2text {}

```diff
@@ -15,59 +15,73 @@
  (https://img.shields.io/badge/%F0%9F%A4%97HuggingFace-Join-yellow)](https://
 huggingface.co/hpcai-tech) [![slack badge](https://img.shields.io/badge/Slack-
 join-blueviolet?logo=slack&)](https://github.com/hpcaitech/public_assets/tree/
  main/colossalai/contact/slack) [![WeChat badge](https://img.shields.io/badge/
 --green?logo=wechat&)](https://raw.githubusercontent.com/hpcaitech/
 public_assets/main/colossalai/img/WeChat.png) | [English](README.md) | []
                           (docs/README-zh-Hans.md) |
-## Latest News * [2024/03] [Open-SoraSora Replication Solution with 46% Cost
-Reduction, Sequence Expansion to Nearly a Million](https://hpc-ai.com/blog/
-open-sora) * [2024/01] [Inference Performance Improved by 46%, Open Source
-Solution Breaks the Length Limit of LLM for Multi-Round Conversations](https://
-hpc-ai.com/blog/Colossal-AI-SwiftInfer) * [2024/01] [Construct Refined 13B
-Private Model With Just $5000 USD, Upgraded Colossal-AI Llama-2 Open Source]
-(https://hpc-ai.com/blog/colossal-llama-2-13b) * [2023/11] [Enhanced MoE
-Parallelism, Open-source MoE Model Training Can Be 9 Times More Efficient]
-(https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-moe-model-
-training-can-be-9-times-more-efficient) * [2023/09] [One Half-Day of Training
-Using a Few Hundred Dollars Yields Similar Results to Mainstream Large Models,
-Open-Source and Commercial-Free Domain-Specific LLM Solution](https://www.hpc-
-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-
-similar-results-to-mainstream-large-models-open-source-and-commercial-free-
-domain-specific-llm-solution) * [2023/09] [70 Billion Parameter LLaMA2 Model
-Training Accelerated by 195%](https://www.hpc-ai.tech/blog/70b-llama2-training)
-* [2023/07] [HPC-AI Tech Raises 22 Million USD in Series A Funding](https://
-www.hpc-ai.tech/blog/hpc-ai-tech-raises-22-million-usd-in-series-a-funding-to-
-fuel-team-expansion-and-business-growth) ## Table of Contents
+## Latest News * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open
+Source with Single-Shot 16-Second Video Generation and 720p Resolution](https:/
+/hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-
+video-generation-and-720p-resolution-in-open-source) * [2024/04] [Most cost-
+effective solutions for inference, fine-tuning and pretraining, tailored to
+LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-
+inference-fine-tuning-and-pretraining-tailored-to-llama3-series) * [2024/03]
+[314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and
+Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-
+billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-
+use-pytorchhuggingface-version-is-here) * [2024/03] [Open-Sora: Revealing
+Complete Model Parameters, Training Details, and Everything for Sora-like Video
+Generation Models](https://hpc-ai.com/blog/open-sora-v1.0) * [2024/03] [Open-
+SoraSora Replication Solution with 46% Cost Reduction, Sequence Expansion to
+Nearly a Million](https://hpc-ai.com/blog/open-sora) * [2024/01] [Inference
+Performance Improved by 46%, Open Source Solution Breaks the Length Limit of
+LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-
+SwiftInfer) * [2024/01] [Construct Refined 13B Private Model With Just $5000
+USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/
+colossal-llama-2-13b) * [2023/11] [Enhanced MoE Parallelism, Open-source MoE
+Model Training Can Be 9 Times More Efficient](https://www.hpc-ai.tech/blog/
+enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-
+efficient) * [2023/09] [One Half-Day of Training Using a Few Hundred Dollars
+Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-
+Free Domain-Specific LLM Solution](https://www.hpc-ai.tech/blog/one-half-day-
+of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-
+large-models-open-source-and-commercial-free-domain-specific-llm-solution) *
+[2023/09] [70 Billion Parameter LLaMA2 Model Training Accelerated by 195%]
+(https://www.hpc-ai.tech/blog/70b-llama2-training) * [2023/07] [HPC-AI Tech
+Raises 22 Million USD in Series A Funding](https://www.hpc-ai.tech/blog/hpc-ai-
+tech-raises-22-million-usd-in-series-a-funding-to-fuel-team-expansion-and-
+business-growth) ## Table of Contents
     * _W_h_y_ _C_o_l_o_s_s_a_l_-_A_I
     * _F_e_a_t_u_r_e_s
     * _C_o_l_o_s_s_a_l_-_A_I_ _f_o_r_ _R_e_a_l_ _W_o_r_l_d_ _A_p_p_l_i_c_a_t_i_o_n_s
-          o _O_p_e_n_-_S_o_r_a_:_ _O_p_e_n_-_S_o_r_a____S_o_r_a_ _R_e_p_l_i_c_a_t_i_o_n_ _S_o_l_u_t_i_o_n_ _w_i_t_h_ _4_6_%_ _C_o_s_t
-            _R_e_d_u_c_t_i_o_n_,_ _S_e_q_u_e_n_c_e_ _E_x_p_a_n_s_i_o_n_ _t_o_ _N_e_a_r_l_y_ _a_ _M_i_l_l_i_o_n
+          o _O_p_e_n_-_S_o_r_a_:_ _R_e_v_e_a_l_i_n_g_ _C_o_m_p_l_e_t_e_ _M_o_d_e_l_ _P_a_r_a_m_e_t_e_r_s_,_ _T_r_a_i_n_i_n_g_ _D_e_t_a_i_l_s_,
+            _a_n_d_ _E_v_e_r_y_t_h_i_n_g_ _f_o_r_ _S_o_r_a_-_l_i_k_e_ _V_i_d_e_o_ _G_e_n_e_r_a_t_i_o_n_ _M_o_d_e_l_s
           o _C_o_l_o_s_s_a_l_-_L_L_a_M_A_-_2_:_ _O_n_e_ _H_a_l_f_-_D_a_y_ _o_f_ _T_r_a_i_n_i_n_g_ _U_s_i_n_g_ _a_ _F_e_w_ _H_u_n_d_r_e_d
             _D_o_l_l_a_r_s_ _Y_i_e_l_d_s_ _S_i_m_i_l_a_r_ _R_e_s_u_l_t_s_ _t_o_ _M_a_i_n_s_t_r_e_a_m_ _L_a_r_g_e_ _M_o_d_e_l_s_,_ _O_p_e_n_-
             _S_o_u_r_c_e_ _a_n_d_ _C_o_m_m_e_r_c_i_a_l_-_F_r_e_e_ _D_o_m_a_i_n_-_S_p_e_c_i_f_i_c_ _L_l_m_ _S_o_l_u_t_i_o_n
           o _C_o_l_o_s_s_a_l_C_h_a_t_:_ _A_n_ _O_p_e_n_-_S_o_u_r_c_e_ _S_o_l_u_t_i_o_n_ _f_o_r_ _C_l_o_n_i_n_g_ _C_h_a_t_G_P_T_ _W_i_t_h_ _a
             _C_o_m_p_l_e_t_e_ _R_L_H_F_ _P_i_p_e_l_i_n_e
           o _A_I_G_C_:_ _A_c_c_e_l_e_r_a_t_i_o_n_ _o_f_ _S_t_a_b_l_e_ _D_i_f_f_u_s_i_o_n
           o _B_i_o_m_e_d_i_c_i_n_e_:_ _A_c_c_e_l_e_r_a_t_i_o_n_ _o_f_ _A_l_p_h_a_F_o_l_d_ _P_r_o_t_e_i_n_ _S_t_r_u_c_t_u_r_e
     * _P_a_r_a_l_l_e_l_ _T_r_a_i_n_i_n_g_ _D_e_m_o
-          o _L_L_a_M_A_ _1_/_2
+          o _L_L_a_M_A_ _1_/_2_/_3
           o _M_o_E
           o _G_P_T_-_3
           o _G_P_T_-_2
           o _B_E_R_T
           o _P_a_L_M
           o _O_P_T
           o _V_i_T
           o _R_e_c_o_m_m_e_n_d_a_t_i_o_n_ _S_y_s_t_e_m_ _M_o_d_e_l_s
     * _S_i_n_g_l_e_ _G_P_U_ _T_r_a_i_n_i_n_g_ _D_e_m_o
           o _G_P_T_-_2
           o _P_a_L_M
     * _I_n_f_e_r_e_n_c_e
+          o _G_r_o_k_-_1_:_ _3_1_4_B_ _m_o_d_e_l_ _o_f_ _P_y_T_o_r_c_h_ _+_ _H_u_g_g_i_n_g_F_a_c_e_ _I_n_f_e_r_e_n_c_e
           o _S_w_i_f_t_I_n_f_e_r_:_B_r_e_a_k_s_ _t_h_e_ _L_e_n_g_t_h_ _L_i_m_i_t_ _o_f_ _L_L_M_ _f_o_r_ _M_u_l_t_i_-_R_o_u_n_d
             _C_o_n_v_e_r_s_a_t_i_o_n_s_ _w_i_t_h_ _4_6_%_ _A_c_c_e_l_e_r_a_t_i_o_n
           o _G_P_T_-_3
           o _O_P_T_-_1_7_5_B_ _O_n_l_i_n_e_ _S_e_r_v_i_n_g_ _f_o_r_ _T_e_x_t_ _G_e_n_e_r_a_t_i_o_n
           o _1_7_6_B_ _B_L_O_O_M
     * _I_n_s_t_a_l_l_a_t_i_o_n
           o _P_y_P_I
@@ -91,21 +105,24 @@
 (https://arxiv.org/abs/2105.13120) - [Zero Redundancy Optimizer (ZeRO)](https:/
 /arxiv.org/abs/1910.02054) - [Auto-Parallelism](https://arxiv.org/abs/
 2302.02599) - Heterogeneous Memory Management - [PatrickStar](https://
 arxiv.org/abs/2108.05818) - Friendly Usage - Parallelism based on the
 configuration file
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
 ## Colossal-AI in the Real World ### Open-Sora [Open-Sora](https://github.com/
-hpcaitech/Open-Sora)Sora Replication Solution with 46% Cost Reduction,
-Sequence Expansion to Nearly a Million [[code]](https://github.com/hpcaitech/
-Open-Sora) [[blog]](https://hpc-ai.com/blog/open-sora)
- [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
-                             sora/open-sora-1.png]
- [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
-                             sora/open-sora-2.png]
+hpcaitech/Open-Sora)Revealing Complete Model Parameters, Training Details,
+and Everything for Sora-like Video Generation Models [[code]](https://
+github.com/hpcaitech/Open-Sora) [[blog]](https://hpc-ai.com/blog/open-soras-
+comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-
+resolution-in-open-source) [[HuggingFace model weights]](https://
+huggingface.co/hpcai-tech/Open-Sora) [[Demo]](https://github.com/hpcaitech/
+Open-Sora?tab=readme-ov-file#-latest-demo)
+ _[_h_t_t_p_s_:_/_/_r_a_w_._g_i_t_h_u_b_u_s_e_r_c_o_n_t_e_n_t_._c_o_m_/_h_p_c_a_i_t_e_c_h_/_p_u_b_l_i_c___a_s_s_e_t_s_/_m_a_i_n_/_a_p_p_l_i_c_a_t_i_o_n_s_/
+                              _s_o_r_a_/_s_o_r_a_-_d_e_m_o_._p_n_g_]
+                                                                  (_b_a_c_k_ _t_o_ _t_o_p)
 ### Colossal-LLaMA-2 - 7B: One half-day of training using a few hundred dollars
 yields similar results to mainstream large models, open-source and commercial-
 free domain-specific LLM solution. [[code]](https://github.com/hpcaitech/
 ColossalAI/tree/main/applications/Colossal-LLaMA-2) [[blog]](https://www.hpc-
 ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-
 similar-results-to-mainstream-large-models-open-source-and-commercial-free-
 domain-specific-llm-solution) [[HuggingFace model weights]](https://
@@ -196,25 +213,29 @@
 - [FastFold with Intel](https://github.com/hpcaitech/FastFold): 3x inference
 acceleration and 39% cost reduce.
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                            xTrimoMultimer_Table.jpg]
 - [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer):
 accelerating structure prediction of protein monomers and multimer by 11x.
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Parallel Training Demo ### LLaMA2
+## Parallel Training Demo ### LLaMA3
+   [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
+                          images/LLaMA3-70B-H100.png]
+- 70 billion parameter LLaMA3 model training accelerated by 18% [[code]](https:
+//github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama) ### LLaMA2
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                             llama2_pretraining.png]
 - 70 billion parameter LLaMA2 model training accelerated by 195% [[code]]
-(https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama2) [
+(https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama) [
 [blog]](https://www.hpc-ai.tech/blog/70b-llama2-training) ### LLaMA1
    [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
                          images/LLaMA_pretraining.png]
 - 65-billion-parameter large model pretraining accelerated by 38% [[code]]
-(https://github.com/hpcaitech/ColossalAI/tree/example/llama/examples/language/
-llama) [[blog]](https://www.hpc-ai.tech/blog/large-model-pretraining) ### MoE
+(https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama) [
+[blog]](https://www.hpc-ai.tech/blog/large-model-pretraining) ### MoE
    [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
                            images/MOE_training.png]
 - Enhanced MoE parallelism, Open-source MoE model training can be 9 times more
 efficient [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/
 language/openmoe) [[blog]](https://www.hpc-ai.tech/blog/enhanced-moe-
 parallelism-open-source-moe-model-training-can-be-9-times-more-efficient) ###
 GPT-3
@@ -255,15 +276,25 @@
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 GPT2-NVME.png]
 - 120x larger model size on the same hardware (RTX 3080) ### PaLM
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 PaLM-GPU1.png]
 - 34x larger model size on the same hardware
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Inference
+## Inference ### Grok-1
+   [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
+                         images/grok-1-inference.jpg]
+- 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use
+Python + PyTorch + HuggingFace version for Inference. [[code]](https://
+github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1) [[blog]]
+(https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-
+3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here) [
+[HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/
+grok-1) [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/
+models/colossalai/grok-1-pytorch/summary)
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 SwiftInfer.jpg]
 - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance
 improved by 46%, open source solution breaks the length limit of LLM for multi-
 round conversations
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                              inference_GPT-3.jpg]
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/__init__.py`

 * *Ordering differences only*

 * *Files 20% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-from .initialize import launch, launch_from_openmpi, launch_from_slurm, launch_from_torch
 from . import accelerator
+from .initialize import launch, launch_from_openmpi, launch_from_slurm, launch_from_torch
 
 try:
     # .version will be created by setup.py
     from .version import __version__
 except ModuleNotFoundError:
     # this will only happen if the user did not run `pip install`
     # and directly set PYTHONPATH to use Colossal-AI which is a bad practice
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/_analyzer/_subclasses/_meta_registration.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/_meta_registration.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/_analyzer/_subclasses/_monkey_patch.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/_monkey_patch.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/_analyzer/_subclasses/flop_tensor.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/flop_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/_analyzer/_subclasses/meta_tensor.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/_subclasses/meta_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/codegen.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/codegen.py`

 * *Files 0% similar despite different names*

```diff
@@ -242,15 +242,15 @@
             emit_node_func(node, body)
             delete_unused_value_func(node, body)
             node_idx += 1
 
 
 @compatibility(is_backward_compatible=True)
 class ActivationCheckpointCodeGen(CodeGen):
-    def _gen_python_code(self, nodes, root_module: str, namespace: _Namespace) -> PythonCode:
+    def _gen_python_code(self, nodes, root_module: str, namespace: _Namespace, verbose=None) -> PythonCode:
         free_vars: List[str] = []
         body: List[str] = []
         globals_: Dict[str, Any] = {}
         wrapped_fns: Dict[str, None] = {}
 
         # Wrap string in list to pass by reference
         maybe_return_annotation: List[str] = [""]
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/graph_module.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/graph_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/node_util.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/node_util.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/passes/graph_profile.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/graph_profile.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/passes/shape_prop.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/passes/shape_prop.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/symbolic_profile.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/symbolic_profile.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/tracer/bias_addition.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/bias_addition.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/tracer/custom_leaf_module.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/custom_leaf_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/tracer/proxy.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/proxy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/tracer/symbolic_trace.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/symbolic_trace.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/_analyzer/fx/tracer/tracer.py` & `colossalai-nightly-2024.5.4/colossalai/_analyzer/fx/tracer/tracer.py`

 * *Files 0% similar despite different names*

```diff
@@ -233,15 +233,15 @@
         return self.graph
 
     @contextmanager
     def _tracer_override(self):
         # override the tracer to support custom modules and checkpointing
         if self.trace_act_ckpt:
             orig_ckpt_func_apply = torch.utils.checkpoint.CheckpointFunction.apply
-            orig_ckpt_func_without_reentrant = torch.utils.checkpoint._checkpoint_without_reentrant
+            orig_ckpt_func_without_reentrant = torch.utils.checkpoint._checkpoint_without_reentrant_generator
 
             def checkpoint(run_function, preserve_rng_state=False, *args):
                 self.ckpt_regions.append(self.ckpt_idx)
                 out = run_function(*args)
                 self.ckpt_idx = self.ckpt_regions.pop(-1) + 1
                 return out
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/accelerator/api.py` & `colossalai-nightly-2024.5.4/colossalai/accelerator/api.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/accelerator/base_accelerator.py` & `colossalai-nightly-2024.5.4/colossalai/accelerator/base_accelerator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/accelerator/cpu_accelerator.py` & `colossalai-nightly-2024.5.4/colossalai/accelerator/cpu_accelerator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/accelerator/cuda_accelerator.py` & `colossalai-nightly-2024.5.4/colossalai/accelerator/cuda_accelerator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/accelerator/npu_accelerator.py` & `colossalai-nightly-2024.5.4/colossalai/accelerator/npu_accelerator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/grad_scaler/base_grad_scaler.py` & `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/base_grad_scaler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/grad_scaler/constant_grad_scaler.py` & `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/constant_grad_scaler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/grad_scaler/dynamic_grad_scaler.py` & `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/grad_scaler/dynamic_grad_scaler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/mixed_precision_mixin/base.py` & `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/mixed_precision_mixin/fp16.py` & `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_mixin/fp16.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/amp/naive_amp/mixed_precision_optimizer.py` & `colossalai-nightly-2024.5.4/colossalai/amp/naive_amp/mixed_precision_optimizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/checkpoint/ckpt_solver_base.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_chen.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/ckpt_solver_rotor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/checkpoint/operation.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/checkpoint/operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/activation.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/activation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/binary_elementwise_ops.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/conv.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/conv.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/linear.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/non_spmd.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/norm.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/norm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/pooling.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/pooling.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/tensor.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/meta_registry/where.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/meta_registry/where.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/registry.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/meta_profiler/shard_metainfo.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/meta_profiler/shard_metainfo.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/amp_optimizer.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/amp_optimizer.py`

 * *Files 2% similar despite different names*

```diff
@@ -122,15 +122,15 @@
             return combined_scale
 
     @property
     def loss_scale(self):
         return self.grad_scaler.scale.item()
 
     def zero_grad(self, *args, **kwargs):
-        self.module.overflow_counter = torch.cuda.IntTensor([0])
+        self.module.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_accelerator().get_current_device())
         return self.optim.zero_grad(set_to_none=True)
 
     def step(self, *args, **kwargs):
         # Copy gradients from model params to main params.
         self._set_grad_ptr()
 
         found_inf = self._check_overflow()
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/base_offload_module.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/base_offload_module.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from functools import partial
 from typing import Optional, Set
 
 import torch
 import torch.nn as nn
 
-from colossalai.utils import _cast_float
-from colossalai.zero.legacy.gemini.tensor_utils import free_storage
+from colossalai.utils import _cast_float, get_current_device
+from colossalai.utils.common import free_storage
 
 from .region_manager import RegionManager
 from .util import GlobalRuntimeInfo
 
 
 class BaseOffloadModule:
     """
@@ -21,15 +21,15 @@
         is_sync (bool): synchronous mode or not.
     """
 
     def __init__(self, model: nn.Module, region_manager: RegionManager, is_sync=True):
         self.model = model
         self.region_manager = region_manager
         self.grad_hook_list = []
-        self.overflow_counter = torch.cuda.IntTensor([0])
+        self.overflow_counter = torch.tensor([0], dtype=torch.int, device=get_current_device())
 
         self.grad_offload_stream = torch.cuda.current_stream() if is_sync else GlobalRuntimeInfo.d2h_stream
 
         self._cast_buffers()
 
     def register_grad_hook(self):
         for p in self.model.parameters():
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/mem_optimize.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/mem_optimize.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/region.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/region.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,14 @@
 from typing import Dict, List, Tuple
 
 import torch
 from torch.fx import Node
 
-from colossalai.zero.legacy.gemini.tensor_utils import alloc_storage, free_storage
+from colossalai.utils.common import free_storage
+from colossalai.zero.gemini.chunk.chunk import alloc_storage
 
 
 class Region:
     """
     Region: A container owning a piece of contiguous nodes in the DNN computing graph.
 
     Args:
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/region_manager.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/region_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/runtime.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/runtime.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/solver.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/solver.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/training_simulator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/training_simulator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/offload/util.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/offload/util.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/passes/comm_metainfo_pass.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/comm_metainfo_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/passes/meta_info_prop.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/meta_info_prop.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/passes/runtime_apply_pass.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/runtime_apply_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/passes/runtime_preparation_pass.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/passes/runtime_preparation_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/constants.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/constants.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/initialize.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/initialize.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/addmm_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/addmm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/batch_norm_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/batch_norm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/binary_elementwise_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/binary_elementwise_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/bmm_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/bmm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/conv_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/conv_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/default_reshape_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/default_reshape_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/embedding_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/embedding_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/getattr_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/getattr_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/getitem_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/getitem_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/layer_norm_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/layer_norm_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/linear_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/linear_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/matmul_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/matmul_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/node_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/node_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/normal_pooling_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/normal_pooling_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/output_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/output_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/permute_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/permute_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/placeholder_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/placeholder_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/registry.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/softmax_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/softmax_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/split_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/split_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/batch_norm_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/batch_norm_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/binary_elementwise_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/binary_elementwise_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/conv_strategy_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/conv_strategy_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/embedding_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/embedding_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getattr_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getattr_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getitem_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/getitem_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/layer_norm_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/layer_norm_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/matmul_strategy_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/matmul_strategy_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/normal_pooling_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/normal_pooling_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/output_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/output_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/placeholder_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/placeholder_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/reshape_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/reshape_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/softmax_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/softmax_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/strategy_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/strategy_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/sum_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/sum_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/tensor_constructor_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/tensor_constructor_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/unary_elementwise_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/unary_elementwise_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/strategy/where_generator.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/strategy/where_generator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/sum_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/sum_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/tensor_constructor_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/tensor_constructor_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/transpose_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/transpose_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/unary_elementwise_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/unary_elementwise_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/view_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/view_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/node_handler/where_handler.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/node_handler/where_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/options.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/options.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/sharding_strategy.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/sharding_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/solver/cost_graph.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/cost_graph.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/solver/graph_analysis.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/graph_analysis.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/solver/solver.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/solver.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/solver/strategies_constructor.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/solver/strategies_constructor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/utils/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/utils/broadcast.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/broadcast.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/utils/factory.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/factory.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/utils/misc.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/misc.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/utils/reshape.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/reshape.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/auto_parallel/tensor_shard/utils/sharding.py` & `colossalai-nightly-2024.5.4/colossalai/auto_parallel/tensor_shard/utils/sharding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/accelerator.py` & `colossalai-nightly-2024.5.4/colossalai/booster/accelerator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/booster.py` & `colossalai-nightly-2024.5.4/colossalai/booster/booster.py`

 * *Files 23% similar despite different names*

```diff
@@ -4,17 +4,26 @@
 
 import torch
 import torch.nn as nn
 from torch.optim import Optimizer
 from torch.optim.lr_scheduler import _LRScheduler as LRScheduler
 from torch.utils.data import DataLoader
 
+SUPPORT_PEFT = False
+try:
+    import peft
+
+    SUPPORT_PEFT = True
+except ImportError:
+    pass
+
 import colossalai.interface.pretrained as pretrained_utils
 from colossalai.checkpoint_io import GeneralCheckpointIO
 from colossalai.interface import ModelWrapper, OptimizerWrapper
+from colossalai.quantization import BnbQuantizationConfig
 
 from .accelerator import Accelerator
 from .mixed_precision import MixedPrecision, mixed_precision_factory
 from .plugin import Plugin
 from .plugin.pp_plugin_base import PipelinePluginBase
 
 __all__ = ["Booster"]
@@ -217,14 +226,64 @@
         """
         assert (
             self.plugin is not None
         ), f"no_sync is only enabled when a plugin is provided and the plugin supports no_sync."
         assert self.plugin.support_no_sync(), f"The plugin {self.plugin.__class__.__name__} does not support no_sync."
         return self.plugin.no_sync(model, optimizer)
 
+    def enable_lora(
+        self,
+        model: nn.Module,
+        pretrained_dir: Optional[str] = None,
+        lora_config: "peft.LoraConfig" = None,
+        bnb_quantization_config: Optional[BnbQuantizationConfig] = None,
+        quantize=False,
+    ) -> nn.Module:
+        """
+        Wrap the passed in model with LoRA modules for training. If pretrained directory is provided, lora configs and weights are loaded from that directory.
+        Lora in ColossalAI is implemented using Huggingface peft library, so the arguments for Lora configuration are same as those of peft.
+
+        Args:
+            model (nn.Module): The model to be appended with LoRA modules.
+            pretrained_dir(str, optional): The path to the pretrained directory, can be a local directory
+                or model_id of a PEFT configuration hosted inside a model repo on the Hugging Face Hub.
+                When set to None, create new lora configs and weights for the model using the passed in lora_config. Defaults to None.
+            lora_config: (peft.LoraConfig, optional): Passed in LoraConfig for peft. Defaults to None.
+        """
+        if not SUPPORT_PEFT:
+            raise ImportError("Please install Huggingface Peft library to enable lora features in ColossalAI!")
+
+        assert self.plugin is not None, f"Lora can only be enabled when a plugin is provided."
+        assert self.plugin.support_lora(), f"The plugin {self.plugin.__class__.__name__} does not support lora."
+        if pretrained_dir is None:
+            assert (
+                lora_config is not None
+            ), "Please provide configuration for Lora when pretrained directory path isn't passed in."
+            assert isinstance(
+                lora_config, peft.LoraConfig
+            ), "The passed in configuration should be an instance of peft.LoraConfig."
+        if lora_config is None:
+            assert (
+                pretrained_dir is not None
+            ), "Please provide pretrained directory path if not passing in lora configuration."
+        if quantize is True:
+            if bnb_quantization_config is not None:
+                warnings.warn(
+                    "User defined BnbQuantizationConfig is not fully tested in ColossalAI. Use it at your own risk."
+                )
+            else:
+                bnb_quantization_config = BnbQuantizationConfig(
+                    load_in_4bit=True,
+                    bnb_4bit_compute_dtype=torch.bfloat16,
+                    bnb_4bit_use_double_quant=True,
+                    bnb_4bit_quant_type="nf4",
+                )
+
+        return self.plugin.enable_lora(model, pretrained_dir, lora_config, bnb_quantization_config)
+
     def load_model(self, model: Union[nn.Module, ModelWrapper], checkpoint: str, strict: bool = True) -> None:
         """Load model from checkpoint.
 
         Args:
             model (nn.Module or ModelWrapper): A model boosted by Booster.
             checkpoint (str): Path to the checkpoint. It must be a local path.
                 It should be a directory path if the checkpoint is sharded. Otherwise, it should be a file path.
@@ -319,7 +378,24 @@
         """Load lr scheduler from checkpoint.
 
         Args:
             lr_scheduler (LRScheduler): A lr scheduler boosted by Booster.
             checkpoint (str): Path to the checkpoint. It must be a local file path.
         """
         self.checkpoint_io.load_lr_scheduler(lr_scheduler, checkpoint)
+
+    def save_lora_as_pretrained(
+        self, model: Union[nn.Module, ModelWrapper], checkpoint: str, use_safetensors: bool = False
+    ) -> None:
+        """
+        Save the lora adapters and adapter configuration file to a pretrained checkpoint directory.
+
+        Args:
+            model (Union[nn.Module, ModelWrapper]): A model boosted by Booster.
+            checkpoint (str): Path to the checkpoint directory. It must be a local path.
+            use_safetensors (bool, optional): Whether to use safe tensors when saving. Defaults to False.
+        """
+        if not SUPPORT_PEFT:
+            raise ImportError("Please install Huggingface Peft library to enable lora features in ColossalAI!")
+        assert self.plugin is not None, f"Lora can only be enabled when a plugin is provided."
+        assert self.plugin.support_lora(), f"The plugin {self.plugin.__class__.__name__} does not support lora."
+        self.checkpoint_io.save_lora_as_pretrained(model, checkpoint, use_safetensors)
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/mixed_precision/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/mixed_precision/fp16_apex.py` & `colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_apex.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/mixed_precision/fp16_naive.py` & `colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_naive.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/mixed_precision/fp16_torch.py` & `colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/fp16_torch.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/mixed_precision/mixed_precision_base.py` & `colossalai-nightly-2024.5.4/colossalai/booster/mixed_precision/mixed_precision_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/plugin/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/plugin/dp_plugin_base.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/dp_plugin_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/plugin/gemini_plugin.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/gemini_plugin.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import gc
 import logging
 import os
 import random
 from pathlib import Path
-from typing import Callable, Iterator, List, Optional, Tuple
+from typing import Callable, Dict, Iterator, List, Optional, Tuple
 
 import numpy as np
 import torch
 import torch.distributed as dist
 import torch.nn as nn
 from torch.distributed.distributed_c10d import _get_default_group
 from torch.optim import Optimizer
@@ -40,18 +40,18 @@
 
 ZERO_AXIS, DP_AXIS, TP_AXIS = 0, 1, 2
 
 
 def get_param_info(optim: Optimizer):
     # Get a backup of necessary information of parameters for future use, which includes:
     # 1. A mapping from integer param_id to param32 shape.
-
     if optim is None:
         return {}
     param_info = {"id2shape": {}}
+
     start_index = 0
     for group in optim.param_groups:
         for param_id, param in enumerate(group["params"], start_index):
             original_shape = param.shape if isinstance(param, torch.Tensor) else None
             param_info["id2shape"][param_id] = original_shape
 
         start_index += len(group["params"])
@@ -420,14 +420,15 @@
 
         self.pg_mesh = ProcessGroupMesh(self.zero_size, self.extra_dp_size, self.tp_size)
         self.zero_group = (
             self.pg_mesh.get_group_along_axis(ZERO_AXIS) if self.zero_size < world_size else _get_default_group()
         )
         self.extra_dp_group = self.pg_mesh.get_group_along_axis(DP_AXIS) if self.extra_dp_size > 1 else None
         self.tp_group = self.pg_mesh.get_group_along_axis(TP_AXIS) if self.tp_size > 1 else None
+        self.dp_size = self.zero_size * self.extra_dp_size
 
         self.shard_config = ShardConfig(
             tensor_parallel_process_group=self.tp_group,
             enable_tensor_parallelism=self.enable_tensor_parallelism,
             enable_all_optimization=self.enable_all_optimization,
             enable_fused_normalization=self.enable_fused_normalization,
             enable_flash_attention=self.enable_flash_attention,
@@ -439,14 +440,17 @@
     def __del__(self):
         """Destroy the process groups in ProcessGroupMesh"""
         self.pg_mesh.destroy_mesh_process_groups()
 
     def support_no_sync(self) -> bool:
         return False
 
+    def support_lora(self) -> bool:
+        return False
+
     def control_precision(self) -> bool:
         return True
 
     def supported_precisions(self) -> List[str]:
         return SUPPORTED_PRECISION
 
     def control_device(self) -> bool:
@@ -523,15 +527,15 @@
         self,
         model: nn.Module,
         optimizer: Optional[Optimizer] = None,
         criterion: Optional[Callable] = None,
         dataloader: Optional[DataLoader] = None,
         lr_scheduler: Optional[LRScheduler] = None,
     ) -> Tuple[nn.Module, OptimizerWrapper, Callable, DataLoader, LRScheduler]:
-        optimizer_params_info = get_param_info(optimizer)
+        params_info = get_param_info(optimizer)
         if not isinstance(model, ModelWrapper):
             # convert model to sync bn
             # FIXME(ver217): gemini does not support sync bn
             # In torch/nn/modules/_functions.py, line 22, ``mean, invstd = torch.batch_norm_stats(input, eps)`` will get fp32 mean and invstd even though the input is fp16.
             # This inconsistency of dtype will cause the error.
             # We have two possible solutions:
             # 1. keep batch norm always in fp32. This is hard for gemini, as it use chunks.
@@ -554,21 +558,26 @@
         if optimizer is not None and not isinstance(optimizer, OptimizerWrapper):
             optimizer = GeminiOptimizer(
                 optimizer,
                 model,
                 **self.zero_optim_config,
                 **self.optim_kwargs,
                 tp_group=self.tp_group,
-                optimizer_params_info=optimizer_params_info,
+                params_info=params_info,
                 verbose=self.verbose,
             )
 
         return model, optimizer, criterion, dataloader, lr_scheduler
 
     def control_checkpoint_io(self) -> bool:
         return True
 
     def get_checkpoint_io(self) -> CheckpointIO:
         return GeminiCheckpointIO()
 
     def no_sync(self, model: nn.Module, optimizer: OptimizerWrapper) -> Iterator[None]:
         raise NotImplementedError
+
+    def enable_lora(
+        self, model: nn.Module, pretrained_dir: Optional[str] = None, lora_config: Optional[Dict] = None
+    ) -> nn.Module:
+        raise NotImplementedError
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/plugin/hybrid_parallel_plugin.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/hybrid_parallel_plugin.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 import ctypes
 import random
 import warnings
 from contextlib import contextmanager
 from functools import partial
 from types import MethodType
-from typing import Any, Callable, Iterator, List, Optional, OrderedDict, Tuple, Union
+from typing import Any, Callable, Dict, Iterator, List, Optional, OrderedDict, Tuple, Union
 
 import numpy as np
 import torch
 import torch.distributed as dist
 from torch import Tensor, inf
 from torch.distributed import ProcessGroup, get_world_size
 from torch.nn import Module, SyncBatchNorm
@@ -22,23 +22,23 @@
 from colossalai.accelerator import get_accelerator
 from colossalai.amp.naive_amp.mixed_precision_optimizer import MixedPrecisionOptimizer
 from colossalai.checkpoint_io import CheckpointIO, HybridParallelCheckpointIO
 from colossalai.cluster import ProcessGroupMesh
 from colossalai.interface import AMPModelMixin, ModelWrapper, OptimizerWrapper
 from colossalai.pipeline.schedule import InterleavedSchedule, OneForwardOneBackwardSchedule
 from colossalai.pipeline.stage_manager import PipelineStageManager
-from colossalai.shardformer import ShardConfig, ShardFormer
+from colossalai.shardformer import GradientCheckpointConfig, ShardConfig, ShardFormer
 from colossalai.shardformer.layer.utils import SeqParallelUtils
 from colossalai.shardformer.policies.base_policy import Policy
 from colossalai.tensor.d_tensor.api import is_distributed_tensor
 from colossalai.zero.low_level import LowLevelZeroOptimizer
 
 from .pp_plugin_base import PipelinePluginBase
 
-DP_AXIS, PP_AXIS, TP_AXIS = 0, 1, 2
+SUPPORT_SP_MODE = ["split_gather", "ring", "all_to_all"]
 
 PRECISION_TORCH_TYPE = {"fp16": torch.float16, "fp32": torch.float32, "bf16": torch.bfloat16}
 
 
 def _convert_floating_point(x, dtype: torch.dtype = torch.float16):
     if isinstance(x, torch.Tensor) and torch.is_floating_point(x):
         return x.to(dtype)
@@ -49,22 +49,24 @@
     def __init__(
         self,
         module: Module,
         precision: str,
         shard_config: ShardConfig,
         dp_group: ProcessGroup,
         tp_group: ProcessGroup,
+        sp_group: ProcessGroup,
         use_ddp: bool,
         ddp_config: dict,
         custom_policy: Policy,
     ) -> None:
         self.stage_manager = shard_config.pipeline_stage_manager
         self.shard_config = shard_config
         self.dp_group = dp_group
         self.tp_group = tp_group
+        self.sp_group = sp_group
         self.use_dpp = use_ddp
         self.require_grad_sync = True
 
         shardformer = ShardFormer(shard_config)
         if custom_policy is not None:
             assert isinstance(custom_policy, object)
         module, self.shared_params = shardformer.optimize(module, policy=custom_policy)
@@ -164,21 +166,32 @@
         Args:
             grads (Optional[List[torch.Tensor]]): A list of gradient tensors to synchronize. If not
                 provided, gradients will be extracted from the model.
 
         Returns:
             None
         """
-        if self.tp_group.size() > 1 and self.shard_config.enable_sequence_parallelism:
+
+        if self.shard_config.enable_sequence_parallelism:
+            if self.shard_config.sequence_parallelism_mode == "all_to_all":
+                return
+
+            if self.shard_config.sequence_parallelism_mode in ["split_gather", "ring"]:
+                # If sequence parallelism is enabled and mode is split_gather or ring, gradients are synchronized
+                # across the tensor parallelism group.
+                group = self.tp_group
+            else:
+                raise ValueError(f"Unknown sequence parallelism mode: {self.shard_config.sequence_parallelism_mode}")
+
             if grads is not None:
                 # Synchronize provided gradient tensors across the tensor parallelism group.
-                SeqParallelUtils.allreduce_partial_data_grad(tp_group=self.tp_group, grads=grads)
+                SeqParallelUtils.allreduce_partial_data_grad(process_group=group, grads=grads)
             else:
                 # Synchronize gradients from the model across the tensor parallelism group.
-                SeqParallelUtils.allreduce_partial_data_grad(tp_group=self.tp_group, model=self.module)
+                SeqParallelUtils.allreduce_partial_data_grad(process_group=group, model=self.module)
 
     def forward(self, *args, **kwargs):
         if self.convert_fn is not None:
             args = tree_map(self.convert_fn, args)
             kwargs = tree_map(self.convert_fn, kwargs)
         return super().forward(*args, **kwargs)
 
@@ -718,18 +731,17 @@
                 return grads_to_sync
             else:
                 return None
 
         # Get all working gradients and gradients to be synchronized.
         all_working_grads = _get_all_working_grads()
         grads_to_sync = _get_grads_to_sync(all_working_grads)
-
         if self.require_grad_sync and grads_to_sync is not None:
             # Synchronize sequence parallelism gradients if required.
-            SeqParallelUtils.allreduce_partial_data_grad(tp_group=self.tp_pg, grads=grads_to_sync)
+            SeqParallelUtils.allreduce_partial_data_grad(process_group=self.tp_pg, grads=grads_to_sync)
         else:
             return
 
     def backward(self, loss, retain_graph=False):
         """
         Backpropagate gradients through the model and optionally synchronize sequence parallelism gradients.
 
@@ -882,27 +894,30 @@
     booster = Booster(plugin=plugin)
     model, optimizer, criterion, train_dataloader, _ = booster.boost(model, optimizer, criterion, train_dataloader)
     ```
 
     Args:
         tp_size (int): The size of tensor parallelism. Tensor parallelism will not be used when tp_size is set to 1.
         pp_size (int): The number of pipeline stages in pipeline parallelism. Pipeline parallelism will not be used when pp_size is set to 1.
+        sp_size (int): The size of sequence parallelism.
         precision (str, optional): Specifies the precision of parameters during training.
                                     Auto-mixied precision will be used when this argument is set to 'fp16' or 'bf16', otherwise model is trained with 'fp32'.
                                     Defaults to 'fp16'.
         zero_stage (int, optional): The stage of ZeRO for data parallelism. Can only be choosed from [0, 1, 2].
                                         When set to 0, ZeRO will not be used. Defaults to 0.
         enable_all_optimization (bool, optional): Whether to switch on all the optimizations supported by Shardformer.
                                                     Currently all the optimization methods include fused normalization, flash attention and JIT.
                                                     Defaults to False.
         enable_fused_normalization (bool, optional): Whether to switch on fused normalization in Shardformer. Defaults to False.
         enable_flash_attention (bool, optional): Whether to switch on flash attention in Shardformer. Defaults to False.
         enable_jit_fused (bool, optional): Whether to switch on JIT in Shardformer. Default to False.
         enable_sequence_parallelism (bool): Whether to turn on sequence parallelism in Shardformer. Defaults to False.
+        sequence_parallelism_mode (str): The Sequence parallelism mode. Can only be choosed from ["split_gather", "ring", "all_to_all"]. Defaults to "split_gather".
         enable_sequence_overlap (bool): Whether to turn on sequence overlap in Shardformer. Defaults to False.
+        parallel_output (bool): Whether to keep the output parallel when enabling tensor parallelism. Default to True.
         num_microbatches (int, optional): Number of microbatches when using pipeline parallelism. Defaults to None.
         microbatch_size (int, optional): Microbatch size when using pipeline parallelism.
             Either ``num_microbatches`` or ``microbatch_size`` should be provided if using pipeline.
             If ``num_microbatches`` is provided, this will be ignored. Defaults to None.
         initial_scale (float, optional): The initial loss scale of AMP. Defaults to 2**16.
         min_scale (float, optional): The minimum loss scale of AMP. Defaults to 1.
         growth_factor (float, optional): The multiplication factor for increasing loss scale when using AMP. Defaults to 2.
@@ -920,29 +935,35 @@
         zero_bucket_size_in_m (int, optional): Gradient reduce bucket size in million elements when using ZeRO. Defaults to 12.
         cpu_offload (bool, optional): Whether to open cpu_offload when using ZeRO. Defaults to False.
         communication_dtype (torch.dtype, optional): Communication dtype when using ZeRO. If not specified, the dtype of param will be used. Defaults to None.
         overlap_communication (bool, optional): Whether to overlap communication and computation when using ZeRO. Defaults to True.
         custom_policy (Policy, optional): Custom policy for Shardformer. Defaults to None.
         pp_style (str, optional): The style for pipeline parallelism. Defaults to '1f1b'.
         num_model_chunks (int, optional): The number of model chunks for interleaved pipeline parallelism. Defaults to 1.
+        gradient_checkpoint_config (GradientCheckpointConfig, optional): Configuration for gradient checkpointing. Defaults to None.
         enable_metadata_cache (bool, optional): Whether to enable metadata cache for pipeline parallelism. Defaults to True.
+        make_vocab_size_divisible_by (int, optional): it's used when padding the vocabulary size, to make it choose an faster kenel. Default to 64.
+
     """
 
     def __init__(
         self,
         tp_size: int,
         pp_size: int,
+        sp_size: int = None,
         precision: str = "fp16",
         zero_stage: int = 0,
         enable_all_optimization: bool = False,
         enable_fused_normalization: bool = False,
         enable_flash_attention: bool = False,
         enable_jit_fused: bool = False,
         enable_sequence_parallelism: bool = False,
+        sequence_parallelism_mode: str = None,
         enable_sequence_overlap: bool = False,
+        parallel_output: bool = True,
         num_microbatches: Optional[int] = None,
         microbatch_size: Optional[int] = None,
         initial_scale: float = 2**16,
         min_scale: float = 1,
         growth_factor: float = 2,
         backoff_factor: float = 0.5,
         growth_interval: int = 1000,
@@ -958,52 +979,89 @@
         zero_bucket_size_in_m: int = 12,
         cpu_offload: bool = False,
         communication_dtype: Optional[torch.dtype] = None,
         overlap_communication: bool = True,
         custom_policy: Policy = None,
         pp_style: str = "1f1b",
         num_model_chunks: int = 1,
+        num_layers_per_stage: Optional[List[int]] = None,
+        gradient_checkpoint_config: Optional[GradientCheckpointConfig] = None,
         enable_metadata_cache: bool = True,
+        make_vocab_size_divisible_by: int = 64,
+        dp_outside: bool = True,
     ) -> None:
         super().__init__()
         assert (
             dist.get_world_size() % (tp_size * pp_size) == 0
-        ), f"world size {dist.get_world_size()} is not divisible by tp_size {tp_size} * pp_size {pp_size}"
+        ), f"World size {dist.get_world_size()} is not divisible by tp_size {tp_size} * pp_size {pp_size}"
 
         if enable_sequence_parallelism:
-            assert tp_size > 1, "Sequence parallelism must be enabled when using tensor parallelism"
+            self.sequence_parallelism_mode = sequence_parallelism_mode if sequence_parallelism_mode is not None else "1"
+            assert (
+                self.sequence_parallelism_mode in SUPPORT_SP_MODE
+            ), f"Sequence parallelism mode {self.sequence_parallelism_mode} is not in the supported list {SUPPORT_SP_MODE}"
+            if self.sequence_parallelism_mode in ["split_gather", "ring"]:
+                assert (
+                    tp_size > 1
+                ), f"Sequence parallelism mode {self.sequence_parallelism_mode} must be enabled when using tensor parallelism"
+                if sp_size != 1:
+                    warnings.warn(
+                        f"The sp_size will be the same as tp_size in sequence parallelism mode {self.sequence_parallelism_mode}, will ignore the given sequence parallelism size."
+                    )
+                self.sp_size = 1
+                self.dp_size = dist.get_world_size() // (tp_size * pp_size)
+            elif self.sequence_parallelism_mode in ["all_to_all"]:
+                assert (
+                    tp_size == 1
+                ), f"Sequence parallelism mode {self.sequence_parallelism_mode} cannot be used with tensor parallelism"
+                assert (
+                    pp_size == 1
+                ), f"Sequence parallelism mode {self.sequence_parallelism_mode} cannot be used with pipeline parallelism"
+                self.sp_size = dist.get_world_size() if sp_size is None else sp_size
+                self.dp_size = dist.get_world_size() // (self.sp_size * pp_size)
+        else:
+            self.dp_size = dist.get_world_size() // (tp_size * pp_size)
+            assert (
+                sp_size == 1 or sp_size is None
+            ), f"sp_size can only be set to a >1 number when enable_sequence_parallelism is True"
+            self.sp_size = 1
 
         self.tp_size = tp_size
         self.pp_size = pp_size
-        self.dp_size = dist.get_world_size() // (tp_size * pp_size)
         self.precision = precision
         self.zero_stage = zero_stage
         self.cpu_offload = cpu_offload
         self.enable_all_optimization = enable_all_optimization
         self.enable_fused_normalization = enable_fused_normalization
         self.enable_flash_attention = enable_flash_attention
         self.enable_jit_fused = enable_jit_fused
         self.enable_sequence_parallelism = enable_sequence_parallelism
-        self.pg_mesh = ProcessGroupMesh(self.dp_size, self.pp_size, self.tp_size)
+        if dp_outside:
+            self.dp_axis, self.pp_axis, self.tp_axis, self.sp_axis = 0, 1, 2, 3
+            self.pg_mesh = ProcessGroupMesh(self.dp_size, self.pp_size, self.tp_size, self.sp_size)
+        else:
+            self.pp_axis, self.dp_axis, self.tp_axis, self.sp_axis = 0, 1, 2, 3
+            self.pg_mesh = ProcessGroupMesh(self.pp_size, self.dp_size, self.tp_size, self.sp_size)
         self.stage_manager = None
         self.schedule = None
         self.custom_policy = custom_policy
         assert zero_stage in (0, 1, 2)
         if self.pp_size > 1:
             assert pp_style in ["1f1b", "interleaved"], "Unsupported pipeline parallelism style"
             assert pp_style == "interleaved" or num_model_chunks == 1, "num_model_chunks must be 1 when using 1f1b"
             assert (
                 num_microbatches is not None or microbatch_size is not None
             ), "num_microbatches or microbatch_size must be specified when using pipeline parallelism"
             assert self.zero_stage <= 1, "zero stage must be 0 or 1 when using pipeline parallelism"
             self.stage_manager = PipelineStageManager(
                 self.pg_mesh,
-                pipeline_axis=PP_AXIS,
+                pipeline_axis=self.pp_axis,
                 enable_interleave=pp_style == "interleaved",
                 num_model_chunks=num_model_chunks,
+                num_layers_per_stage=num_layers_per_stage,
             )
 
             if pp_style == "interleaved":
                 assert num_model_chunks > 1, "number of model chunks must be > 1 when using interleaved"
                 self.schedule = InterleavedSchedule(
                     stage_manager=self.stage_manager,
                     num_model_chunks=num_model_chunks,
@@ -1017,28 +1075,37 @@
                     num_microbatches=num_microbatches,
                     microbatch_size=microbatch_size,
                     enable_metadata_cache=enable_metadata_cache,
                 )
             else:
                 raise NotImplementedError()
 
-        self.tp_group = self.pg_mesh.get_group_along_axis(TP_AXIS)
-        self.dp_group = self.pg_mesh.get_group_along_axis(DP_AXIS)
-        self.pp_group = self.pg_mesh.get_group_along_axis(PP_AXIS)
+        self.tp_group = self.pg_mesh.get_group_along_axis(self.tp_axis)
+        self.dp_group = self.pg_mesh.get_group_along_axis(self.dp_axis)
+        self.pp_group = self.pg_mesh.get_group_along_axis(self.pp_axis)
+        if self.enable_sequence_parallelism and self.sequence_parallelism_mode in ["split_gather", "ring"]:
+            self.sp_group = self.pg_mesh.get_group_along_axis(self.tp_axis)
+        else:
+            self.sp_group = self.pg_mesh.get_group_along_axis(self.sp_axis)
 
         self.shard_config = ShardConfig(
             tensor_parallel_process_group=self.tp_group,
+            sequence_parallel_process_group=self.sp_group,
             pipeline_stage_manager=self.stage_manager,
             enable_tensor_parallelism=self.tp_size > 1,
             enable_all_optimization=self.enable_all_optimization,
             enable_fused_normalization=self.enable_fused_normalization,
             enable_flash_attention=self.enable_flash_attention,
             enable_jit_fused=self.enable_jit_fused,
             enable_sequence_parallelism=enable_sequence_parallelism,
+            sequence_parallelism_mode=sequence_parallelism_mode,
             enable_sequence_overlap=enable_sequence_overlap,
+            parallel_output=parallel_output,
+            make_vocab_size_divisible_by=make_vocab_size_divisible_by,
+            gradient_checkpoint_config=gradient_checkpoint_config,
         )
         self.amp_config = dict(
             initial_scale=initial_scale,
             growth_factor=growth_factor,
             backoff_factor=backoff_factor,
             growth_interval=growth_interval,
             hysteresis=hysteresis,
@@ -1085,34 +1152,47 @@
 
     def control_precision(self) -> bool:
         return True
 
     def support_no_sync(self) -> bool:
         return True
 
+    def support_lora(self) -> bool:
+        return False
+
     def control_checkpoint_io(self) -> bool:
         return True
 
     def configure(
         self,
         model: Module,
         optimizer: Optional[Optimizer] = None,
         criterion: Optional[Callable] = None,
         dataloader: Optional[DataLoader] = None,
         lr_scheduler: Optional[LRScheduler] = None,
     ) -> Tuple[Module, OptimizerWrapper, Callable, DataLoader, LRScheduler]:
         param_info = get_param_info(optimizer)
         if not isinstance(model, ModelWrapper):
-            use_ddp = self.dp_size > 1 and self.pp_size == 1 and self.zero_stage == 0
+            use_ddp = (self.dp_size > 1 and self.pp_size == 1 and self.zero_stage == 0) or (
+                self.dp_size == 1
+                and self.pp_size == 1
+                and self.enable_sequence_parallelism
+                and self.sequence_parallelism_mode == "all_to_all"
+            )
+            if self.enable_sequence_parallelism and self.sequence_parallelism_mode == "all_to_all":
+                dp_group = self.pg_mesh.create_group_along_axis([self.dp_axis, self.sp_axis])
+            else:
+                dp_group = self.dp_group
             model = HybridParallelModule(
                 model,
                 precision=self.precision,
                 shard_config=self.shard_config,
-                dp_group=self.dp_group,
+                dp_group=dp_group,
                 tp_group=self.tp_group,
+                sp_group=self.sp_group,
                 use_ddp=use_ddp,
                 ddp_config=self.ddp_config,
                 custom_policy=self.custom_policy,
             )
         if optimizer is not None and not isinstance(optimizer, OptimizerWrapper):
             if self.zero_stage == 0:
                 if self.precision in ["fp16", "bf16"]:
@@ -1134,27 +1214,28 @@
                         use_pipeline=self.enable_pipeline_parallelism,
                         param_info=param_info,
                         max_norm=self.max_norm,
                         pp_process_group=self.pp_group,
                         tp_process_group=self.tp_group,
                     )
             else:
-                if self.dp_size == 1:
+                zero_dp_size = dist.get_world_size(dp_group)
+                if zero_dp_size == 1:
                     warnings.warn(
                         "Use Zero Optimizer when data parallel size is 1 may introduce unnecessary overhead. "
                         "If you are not intended to use cpu_offload, please consider set zero_stage=0."
                     )
 
                 assert self.precision != "fp32", "Please set precision to 'fp16' or 'bf16' when using ZeRO."
                 optimizer = HybridParallelZeroOptimizer(
                     optimizer,
                     model,
                     use_pipeline=self.enable_pipeline_parallelism,
                     param_info=param_info,
-                    dp_process_group=self.dp_group,
+                    dp_process_group=dp_group,
                     tp_process_group=self.tp_group,
                     pp_process_group=self.pp_group,
                     verbose=True,
                     clip_grad_norm=self.max_norm,
                     **self.zero_config,
                     **self.amp_config,
                 )
@@ -1171,14 +1252,17 @@
             Union[HybridParallelNaiveOptimizer, HybridParallelAMPOptimizer, HybridParallelZeroOptimizer]
         ] = None,
         return_loss: bool = True,
         return_outputs: bool = False,
     ) -> dict:
         assert self.enable_pipeline_parallelism, "pipeline parallelism is not enabled"
 
+        if return_outputs:
+            warnings.warn("return_outputs may lead to significant extra memory consumption.")
+
         # Create a context for gradient synchronization based on the optimizer type.
         # If it's a HybridParallelZeroOptimizer, use optimizer.no_sync(); otherwise, use model.no_sync().
         # This is to avoid redundant gradient reduction in pipeline parallelism (multiple microbatch values should be reduced once),
         # so we disable it, performing manual reduction instead.
         ctx = optimizer.no_sync() if isinstance(optimizer, HybridParallelZeroOptimizer) else model.no_sync()
 
         with ctx:
@@ -1239,15 +1323,18 @@
 
         Returns:
             :class:`torch.utils.data.DataLoader`: A DataLoader used for training or testing.
         """
         _kwargs = kwargs.copy()
         distributed_sampler_cls = distributed_sampler_cls or DistributedSampler
         sampler = distributed_sampler_cls(
-            dataset, num_replicas=self.pg_mesh.size(DP_AXIS), rank=self.pg_mesh.coordinate(DP_AXIS), shuffle=shuffle
+            dataset,
+            num_replicas=self.pg_mesh.size(self.dp_axis),
+            rank=self.pg_mesh.coordinate(self.dp_axis),
+            shuffle=shuffle,
         )
 
         # Deterministic dataloader
         def seed_worker(worker_id):
             worker_seed = seed
             np.random.seed(worker_seed)
             torch.manual_seed(worker_seed)
@@ -1268,7 +1355,12 @@
         return HybridParallelCheckpointIO(self.dp_group, self.pp_group, self.tp_group, self.zero_stage)
 
     def no_sync(self, model: Module, optimizer: OptimizerWrapper) -> Iterator[None]:
         assert (
             self.zero_stage != 2
         ), "ZERO2 is not compatible with no_sync function, please run gradient accumulation with gradient synchronization allowed."
         return optimizer.no_sync() if isinstance(optimizer, HybridParallelZeroOptimizer) else model.no_sync()
+
+    def enable_lora(
+        self, model: Module, pretrained_dir: Optional[str] = None, lora_config: Optional[Dict] = None
+    ) -> Module:
+        raise NotImplementedError
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/plugin/low_level_zero_plugin.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/low_level_zero_plugin.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,16 +1,19 @@
+import enum
 import logging
 import os
+import warnings
 from functools import partial
 from pathlib import Path
 from types import MethodType
-from typing import Callable, Iterator, List, Optional, Tuple
+from typing import Callable, Dict, Iterator, List, Optional, Tuple
 
 import torch
 import torch.nn as nn
+from torch.nn import Parameter
 from torch.optim import Optimizer
 from torch.optim.lr_scheduler import _LRScheduler as LRScheduler
 from torch.utils._pytree import tree_map
 from torch.utils.data import DataLoader
 
 from colossalai.accelerator import get_accelerator
 from colossalai.checkpoint_io import CheckpointIndexFile, CheckpointIO
@@ -21,14 +24,15 @@
     load_shard_state_dict,
     load_states_into_optimizer,
     save_param_groups,
     save_state_dict,
     sharded_optimizer_loading_epilogue,
 )
 from colossalai.interface import AMPModelMixin, ModelWrapper, OptimizerWrapper
+from colossalai.quantization import BnbQuantizationConfig, quantize_model
 from colossalai.zero import LowLevelZeroOptimizer
 
 from .dp_plugin_base import DPPluginBase
 from .torch_ddp_plugin import TorchDDPCheckpointIO
 
 __all__ = ["LowLevelZeroPlugin"]
 
@@ -38,14 +42,20 @@
         return x.to(dtype)
     return x
 
 
 SUPPORTED_PRECISION = ["fp16", "bf16", "fp32"]
 
 
+class OptimizerParamCheckState(enum.Enum):
+    ORIGIN_PARAM_FINDED = 0
+    ORIGIN_PARAM_NOT_FIND = -1
+    LORA_PARM_EXISTED = -2
+
+
 class LowLevelZeroModel(ModelWrapper, AMPModelMixin):
     def __init__(self, module: nn.Module, precision: str) -> None:
         super().__init__(module)
         self.dtype = None
         if precision == "fp16":
             self.dtype = torch.float16
         elif precision == "bf16":
@@ -205,14 +215,27 @@
         use_safetensors: bool = False,
         load_sub_module: bool = True,
     ):
         assert isinstance(model, LowLevelZeroModel), "Please boost the model before loading!"
         super().load_sharded_model(model, checkpoint_index_file, strict, use_safetensors, load_sub_module)
         model.update_master_params()
 
+    def save_lora_as_pretrained(self, model, checkpoint, use_safetensors):
+        if os.path.isfile(checkpoint):
+            logging.error(f"Provided path ({checkpoint}) should be a directory, not a file")
+            return
+        from peft import PeftModel
+
+        assert isinstance(model, ModelWrapper), "Please boost the model before saving!"
+        peft_model = model.unwrap()
+        assert isinstance(
+            peft_model, PeftModel
+        ), "The model doesn't have lora adapters, please enable lora before saving."
+        return peft_model.save_pretrained(checkpoint, safe_serialization=use_safetensors)
+
 
 class LowLevelZeroPlugin(DPPluginBase):
     """
     Plugin for low level zero.
 
     ```python
     from colossalai.booster import Booster
@@ -284,42 +307,128 @@
             reduce_bucket_size=reduce_bucket_size_in_m * 1024 * 1024,
             communication_dtype=communication_dtype,
             overlap_communication=overlap_communication,
             partition_grad=(stage == 2),
             cpu_offload=cpu_offload,
             master_weights=master_weights,
         )
+        self.lora_enabled = False
         self.verbose = verbose
 
         # set class name with stage, for better error message
         setattr(self.__class__, "__name__", f"LowLevelZeroPlugin_ZeRO-{stage}")
 
     def support_no_sync(self) -> bool:
         return self.stage == 1
 
+    def support_lora(self) -> bool:
+        return False
+
     def control_precision(self) -> bool:
         return True
 
     def supported_precisions(self) -> List[str]:
         return SUPPORTED_PRECISION
 
     def control_device(self) -> bool:
         return True
 
     def supported_devices(self) -> List[str]:
         return ["cuda", "npu"]
 
+    def support_lora(self) -> bool:
+        return True
+
+    def enable_lora(
+        self,
+        model: nn.Module,
+        pretrained_dir: Optional[str] = None,
+        lora_config: Optional[Dict] = None,
+        bnb_quantization_config: Optional[BnbQuantizationConfig] = None,
+    ) -> nn.Module:
+        from peft import PeftModel, get_peft_model
+
+        assert not isinstance(model, LowLevelZeroModel), "Lora should be enabled before boosting the model."
+        self.lora_enabled = True
+        warnings.warn("You have enabled LoRa training. Please check the hyperparameters such as lr")
+
+        if bnb_quantization_config is not None:
+            model = quantize_model(model, bnb_quantization_config)
+
+        if pretrained_dir is None:
+            peft_model = get_peft_model(model, lora_config)
+        else:
+            peft_model = PeftModel.from_pretrained(model, pretrained_dir, is_trainable=True)
+        return peft_model
+
+    def get_param_group_id(self, optimizer: Optimizer, origin_param: Parameter):
+        origin_param_id = id(origin_param)
+        for group_id, param_group in enumerate(optimizer.param_groups):
+            for p in param_group["params"]:
+                if id(p) == origin_param_id:
+                    return group_id
+        return -1
+
+    def get_param_group_id(self, optimizer: Optimizer, origin_param: Parameter, lora_param: Parameter):
+        origin_param_id = id(origin_param)
+        lora_param_id = id(lora_param)
+        target_group_id = None
+        for group_id, param_group in enumerate(optimizer.param_groups):
+            for p in param_group["params"]:
+                if id(p) == lora_param_id:
+                    # check if the lora parameter exists.
+                    return target_group_id, OptimizerParamCheckState.LORA_PARM_EXISTED
+                if id(p) == origin_param_id:
+                    target_group_id = group_id
+        if target_group_id is not None:
+            return target_group_id, OptimizerParamCheckState.ORIGIN_PARAM_FINDED
+        else:
+            return target_group_id, OptimizerParamCheckState.ORIGIN_PARAM_NOT_FIND
+
+    def add_lora_params_to_optimizer(self, model, optimizer):
+        """add lora parameters to optimizer"""
+        name2param = {}
+        for name, param in model.named_parameters():
+            name2param[name] = param
+
+        for name, param in name2param.items():
+            if "lora_A" in name or "lora_B" in name:
+                origin_key = name.replace("lora_A.", "")
+                origin_key = origin_key.replace("lora_B.", "")
+                origin_key = origin_key.replace(f"{model.active_adapter}", "base_layer")
+                origin_param = name2param[origin_key]
+                group_id, check_state = self.get_param_group_id(optimizer, origin_param, param)
+                if check_state == OptimizerParamCheckState.ORIGIN_PARAM_NOT_FIND:
+                    warnings.warn(
+                        "Origin parameter {origin_key} related to {name} doesn't exist in optimizer param_groups."
+                    )
+                elif (
+                    check_state == OptimizerParamCheckState.ORIGIN_PARAM_FINDED
+                    and group_id is not None
+                    and group_id >= 0
+                ):
+                    optimizer.param_groups[group_id]["params"].append(param)
+
     def configure(
         self,
         model: nn.Module,
         optimizer: Optional[Optimizer] = None,
         criterion: Optional[Callable] = None,
         dataloader: Optional[DataLoader] = None,
         lr_scheduler: Optional[LRScheduler] = None,
     ) -> Tuple[nn.Module, OptimizerWrapper, Callable, DataLoader, LRScheduler]:
+        if self.lora_enabled:
+            from peft import PeftModel
+
+            assert isinstance(
+                model, PeftModel
+            ), "The model should have been wrapped as a PeftModel when self.lora_enabled is True"
+            if optimizer is not None:
+                self.add_lora_params_to_optimizer(model, optimizer)
+
         if not isinstance(model, ModelWrapper):
             model = LowLevelZeroModel(model, self.precision)
 
         if optimizer is not None and not isinstance(optimizer, OptimizerWrapper):
             optimizer: LowLevelZeroOptimizer = LowLevelZeroOptimizer(
                 optimizer, **self.zero_optim_kwargs, verbose=self.verbose
             )
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/moe_hybrid_parallel_plugin.py`

 * *Files 2% similar despite different names*

```diff
@@ -178,15 +178,15 @@
         static_graph: bool = False,
         zero_bucket_size_in_m: int = 12,
         cpu_offload: bool = False,
         communication_dtype: Optional[torch.dtype] = None,
         overlap_communication: bool = True,
         use_ep_inside: bool = True,
         custom_policy: Policy = None,
-        checkpoint_io: Optional[MoECheckpintIO] = None,
+        checkpoint_io: Optional[MoECheckpointIO] = None,
     ) -> None:
         assert (
             dist.get_world_size() % (tp_size * pp_size) == 0
         ), f"world size {dist.get_world_size()} is not divisible by tp_size {tp_size} * pp_size {pp_size}"
 
         if enable_sequence_parallelism:
             assert tp_size > 1, "Sequence parallelism must be enabled when using tensor parallelism"
@@ -250,14 +250,17 @@
             self.stage_manager = PipelineStageManager(self.pg_mesh, PP_AXIS)
             self.schedule = OneForwardOneBackwardSchedule(
                 self.stage_manager, num_microbatches=num_microbatches, microbatch_size=microbatch_size
             )
         self.tp_group = self.pg_mesh.get_group_along_axis(TP_AXIS)
         self.dp_group = self.pg_mesh.get_group_along_axis(DP_AXIS)
         self.pp_group = self.pg_mesh.get_group_along_axis(PP_AXIS)
+        # TODO: Currently moe only support partially sequence parallel
+        self.sp_group = self.pg_mesh.get_group_along_axis(TP_AXIS)
+
         self.shard_config = ShardConfig(
             tensor_parallel_process_group=self.tp_group,
             pipeline_stage_manager=self.stage_manager,
             enable_tensor_parallelism=self.tp_size > 1,
             enable_all_optimization=self.enable_all_optimization,
             enable_fused_normalization=self.enable_fused_normalization,
             enable_flash_attention=self.enable_flash_attention,
@@ -337,15 +340,14 @@
             worker_init_fn=seed_worker,
             drop_last=drop_last,
             pin_memory=pin_memory,
             num_workers=num_workers,
             **_kwargs,
         )
 
-
     def get_checkpoint_io(self) -> MoECheckpointIO:
         if self.checkpoint_io is None:
             self.checkpoint_io = MoECheckpointIO(self.dp_group, self.pp_group, self.tp_group, self.zero_stage)
         else:
             self.checkpoint_io = self.checkpoint_io(self.dp_group, self.pp_group, self.tp_group, self.zero_stage)
         return self.checkpoint_io
 
@@ -362,14 +364,15 @@
             use_ddp = self.dp_size > 1 and self.pp_size == 1 and self.zero_stage == 0
             model = HybridParallelModule(
                 module=model,
                 precision=self.precision,
                 shard_config=self.shard_config,
                 dp_group=self.dp_group,
                 tp_group=self.tp_group,
+                sp_group=self.sp_group,
                 use_ddp=use_ddp,
                 ddp_config=self.ddp_config,
                 custom_policy=self.custom_policy,
             )
         if optimizer is not None and not isinstance(optimizer, OptimizerWrapper):
             if self.zero_stage == 0:
                 if self.precision in ["fp16", "bf16"]:
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/plugin/plugin_base.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/plugin_base.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 from abc import ABC, abstractmethod
-from typing import Callable, Iterator, List, Optional, Tuple
+from typing import Callable, Dict, Iterator, List, Optional, Tuple
 
 import torch.nn as nn
 from torch.optim import Optimizer
 from torch.optim.lr_scheduler import _LRScheduler as LRScheduler
 from torch.utils.data import DataLoader, Dataset
 
 from colossalai.checkpoint_io import CheckpointIO
@@ -30,14 +30,18 @@
         pass
 
     @abstractmethod
     def support_no_sync(self) -> bool:
         pass
 
     @abstractmethod
+    def support_lora(self) -> bool:
+        pass
+
+    @abstractmethod
     def configure(
         self,
         model: nn.Module,
         optimizer: Optional[Optimizer] = None,
         criterion: Optional[Callable] = None,
         dataloader: Optional[DataLoader] = None,
         lr_scheduler: Optional[LRScheduler] = None,
@@ -60,14 +64,20 @@
     @abstractmethod
     def no_sync(self, model: nn.Module, optimizer: OptimizerWrapper) -> Iterator[None]:
         """
         Context manager to disable gradient synchronization.
         """
 
     @abstractmethod
+    def enable_lora(self, model: nn.Module, pretrained_dir: str, lora_config: Dict) -> nn.Module:
+        """
+        Add LoRA modules to the model passed in. Should only be called in booster.enable_lora().
+        """
+
+    @abstractmethod
     def prepare_dataloader(
         self,
         dataset: Dataset,
         batch_size: int,
         shuffle: bool = False,
         seed: int = 1024,
         drop_last: bool = False,
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/plugin/pp_plugin_base.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/pp_plugin_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/plugin/torch_ddp_plugin.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/torch_ddp_plugin.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,18 +1,20 @@
-from typing import Callable, Iterator, List, Optional, Tuple
+from typing import Callable, Dict, Iterator, List, Optional, Tuple, Union
 
 import torch.nn as nn
 from torch.nn.parallel import DistributedDataParallel as DDP
 from torch.optim import Optimizer
 from torch.optim.lr_scheduler import _LRScheduler as LRScheduler
 from torch.utils.data import DataLoader
 
 from colossalai.checkpoint_io import CheckpointIO, GeneralCheckpointIO
 from colossalai.cluster import DistCoordinator
 from colossalai.interface import ModelWrapper, OptimizerWrapper
+from colossalai.quantization import BnbQuantizationConfig, quantize_model
+from colossalai.utils import get_current_device
 
 from .dp_plugin_base import DPPluginBase
 
 __all__ = ["TorchDDPPlugin"]
 
 
 class TorchDDPCheckpointIO(GeneralCheckpointIO):
@@ -112,14 +114,30 @@
     ):
         """
         Load optimizer from sharded checkpoint.
         """
         assert isinstance(optimizer, OptimizerWrapper), "Please boost the optimizer before loading!"
         super().load_sharded_optimizer(optimizer.unwrap(), index_file_path, prefix)
 
+    def save_lora_as_pretrained(
+        self, model: Union[nn.Module, ModelWrapper], checkpoint: str, use_safetensors: bool = False
+    ) -> None:
+        """
+        Save the lora adapters and adapter configuration file to checkpoint directory.
+        """
+        from peft import PeftModel
+
+        assert isinstance(model, ModelWrapper), "Please boost the model before saving!"
+        if self.coordinator.is_master():
+            peft_model = model.unwrap()
+            assert isinstance(
+                peft_model, PeftModel
+            ), "The model doesn't have lora adapters, please enable lora before saving."
+            peft_model.save_pretrained(save_directory=checkpoint, safe_serialization=use_safetensors)
+
 
 class TorchDDPModel(ModelWrapper):
     def __init__(self, module: nn.Module, *args, **kwargs) -> None:
         super().__init__(module)
         self.module = DDP(module, *args, **kwargs)
 
     def unwrap(self):
@@ -169,36 +187,39 @@
             gradient_as_bucket_view=gradient_as_bucket_view,
             static_graph=static_graph,
         )
 
     def support_no_sync(self) -> bool:
         return True
 
+    def support_lora(self) -> bool:
+        return True
+
     def control_precision(self) -> bool:
         return False
 
     def supported_precisions(self) -> List[str]:
         return ["fp16", "fp16_apex", "bf16", "fp8"]
 
     def control_device(self) -> bool:
         return True
 
     def supported_devices(self) -> List[str]:
-        return ["cuda"]
+        return ["cuda", "npu"]
 
     def configure(
         self,
         model: nn.Module,
         optimizer: Optional[Optimizer] = None,
         criterion: Optional[Callable] = None,
         dataloader: Optional[DataLoader] = None,
         lr_scheduler: Optional[LRScheduler] = None,
     ) -> Tuple[nn.Module, OptimizerWrapper, Callable, DataLoader, LRScheduler]:
         # cast model to cuda
-        model = model.cuda()
+        model = model.to(get_current_device())
 
         # convert model to sync bn
         model = nn.SyncBatchNorm.convert_sync_batchnorm(model, None)
 
         # wrap the model with PyTorch DDP
         model = TorchDDPModel(model, **self.ddp_kwargs)
 
@@ -212,7 +233,25 @@
 
     def get_checkpoint_io(self) -> CheckpointIO:
         return TorchDDPCheckpointIO()
 
     def no_sync(self, model: nn.Module, optimizer: OptimizerWrapper) -> Iterator[None]:
         assert isinstance(model, TorchDDPModel), "Model is not boosted by TorchDDPPlugin."
         return model.module.no_sync()
+
+    def enable_lora(
+        self,
+        model: nn.Module,
+        pretrained_dir: Optional[str] = None,
+        lora_config: Optional[Dict] = None,
+        bnb_quantization_config: Optional[BnbQuantizationConfig] = None,
+    ) -> nn.Module:
+        from peft import PeftModel, get_peft_model
+
+        if bnb_quantization_config is not None:
+            model = quantize_model(model, bnb_quantization_config)
+
+        assert not isinstance(model, TorchDDPModel), "Lora should be enabled before boosting the model."
+        if pretrained_dir is None:
+            return get_peft_model(model, lora_config)
+        else:
+            return PeftModel.from_pretrained(model, pretrained_dir, is_trainable=True)
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/booster/plugin/torch_fsdp_plugin.py` & `colossalai-nightly-2024.5.4/colossalai/booster/plugin/torch_fsdp_plugin.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 import logging
 import os
 import warnings
 from pathlib import Path
-from typing import Callable, Iterable, Iterator, List, Optional, Tuple
+from typing import Callable, Dict, Iterable, Iterator, List, Optional, Tuple
 
 import torch
 import torch.nn as nn
 from packaging import version
 from torch.distributed import ProcessGroup
 
 if version.parse(torch.__version__) >= version.parse("1.12.0"):
@@ -23,15 +23,15 @@
 else:
     raise RuntimeError("FSDP is not supported while torch version under 1.12.0.")
 
 from torch.optim import Optimizer
 from torch.optim.lr_scheduler import _LRScheduler as LRScheduler
 from torch.utils.data import DataLoader
 
-from colossalai.checkpoint_io import CheckpointIO, GeneralCheckpointIO, utils, CheckpointIndexFile
+from colossalai.checkpoint_io import CheckpointIndexFile, CheckpointIO, GeneralCheckpointIO, utils
 from colossalai.cluster import DistCoordinator
 from colossalai.interface import ModelWrapper, OptimizerWrapper
 
 from .dp_plugin_base import DPPluginBase
 
 __all__ = ["TorchFSDPPlugin"]
 
@@ -89,17 +89,15 @@
         assert isinstance(model, TorchFSDPModel), "Please boost the model before saving!"
         if os.path.isfile(checkpoint_path):
             logging.error(f"Provided path ({checkpoint_path}) should be a directory, not a file")
             return
 
         Path(checkpoint_path).mkdir(parents=True, exist_ok=True)
         with FSDP.state_dict_type(
-            model.unwrap(),
-            StateDictType.FULL_STATE_DICT,
-            FullStateDictConfig(offload_to_cpu=True, rank0_only=True)
+            model.unwrap(), StateDictType.FULL_STATE_DICT, FullStateDictConfig(offload_to_cpu=True, rank0_only=True)
         ):
             state_dict = model.unwrap().state_dict()
 
         state_dict_shard = utils.shard_model_checkpoint(state_dict, max_shard_size=size_per_shard)
 
         weights_name, save_index_file = utils.get_model_base_filenames(prefix, use_safetensors)
         index_file = CheckpointIndexFile(checkpoint_path)
@@ -168,15 +166,15 @@
             return
 
         Path(checkpoint).mkdir(parents=True, exist_ok=True)
 
         with FSDP.state_dict_type(
             optimizer.unwrap_model().unwrap(),
             StateDictType.FULL_STATE_DICT,
-            FullStateDictConfig(offload_to_cpu=True, rank0_only=True)
+            FullStateDictConfig(offload_to_cpu=True, rank0_only=True),
         ):
             fsdp_optim_state = FSDP.full_optim_state_dict(
                 optimizer.unwrap_model().unwrap(), optim=optimizer, rank0_only=True
             )
 
         if self.coordinator.is_master():
             # Preparing file paths and index file.
@@ -237,15 +235,14 @@
 
         with FSDP.state_dict_type(optimizer.unwrap_model().unwrap(), StateDictType.FULL_STATE_DICT):
             fsdp_state = FSDP.optim_state_dict_to_load(
                 model=optimizer.unwrap_model().unwrap(), optim=optimizer, optim_state_dict=fsdp_optim_dict
             )
             optimizer.load_state_dict(fsdp_state)
 
-
     def save_lr_scheduler(self, lr_scheduler: LRScheduler, checkpoint: str):
         """
         Save model to checkpoint but only on master process.
         """
         if self.coordinator.is_master():
             super().save_lr_scheduler(lr_scheduler, checkpoint)
 
@@ -317,14 +314,17 @@
 
     else:
         raise RuntimeError("FSDP is not supported while torch version under 1.12.0.")
 
     def support_no_sync(self) -> bool:
         return False
 
+    def support_lora(self) -> bool:
+        return False
+
     def no_sync(self, model: nn.Module, optimizer: OptimizerWrapper) -> Iterator[None]:
         raise NotImplementedError("Torch fsdp no_sync func not supported yet.")
 
     def control_precision(self) -> bool:
         return True
 
     def supported_precisions(self) -> List[str]:
@@ -360,7 +360,12 @@
         return fsdp_model, optimizer, criterion, dataloader, lr_scheduler
 
     def control_checkpoint_io(self) -> bool:
         return True
 
     def get_checkpoint_io(self) -> CheckpointIO:
         return TorchFSDPCheckpointIO()
+
+    def enable_lora(
+        self, model: nn.Module, pretrained_dir: Optional[str] = None, lora_config: Optional[Dict] = None
+    ) -> nn.Module:
+        raise NotImplementedError
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/checkpoint_io/checkpoint_io_base.py` & `colossalai-nightly-2024.5.4/colossalai/checkpoint_io/checkpoint_io_base.py`

 * *Files 1% similar despite different names*

```diff
@@ -331,7 +331,24 @@
 
         Args:
             lr_scheduler (LRScheduler): lr scheduler to be loaded.
             checkpoint (str): the path for a single checkpoint file.
         """
         state_dict = torch.load(checkpoint)
         lr_scheduler.load_state_dict(state_dict)
+
+    # ================================================================================
+    # Abstract method for lora saving implementation.
+    # ================================================================================
+
+    @abstractmethod
+    def save_lora_as_pretrained(
+        self, model: Union[nn.Module, ModelWrapper], checkpoint: str, use_safetensors: bool = False
+    ) -> None:
+        """
+        Save the lora adapters and adapter configuration file to a pretrained checkpoint directory.
+
+        Args:
+            model (Union[nn.Module, ModelWrapper]): A model boosted by Booster.
+            checkpoint (str): Path to the checkpoint directory. It must be a local path.
+            use_safetensors (bool, optional): Whether to use safe tensors when saving. Defaults to False.
+        """
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/checkpoint_io/general_checkpoint_io.py` & `colossalai-nightly-2024.5.4/colossalai/checkpoint_io/general_checkpoint_io.py`

 * *Files 2% similar despite different names*

```diff
@@ -224,7 +224,10 @@
                     ", ".join('"{}"'.format(k) for k in missing_keys)
                 )
                 raise RuntimeError(
                     "Error(s) in loading state_dict for {}:\n\t{}".format(
                         self.__class__.__name__, "\n\t".join(error_msgs)
                     )
                 )
+
+    def save_lora_as_pretrained(self, model: nn.Module, checkpoint: str, use_safetensors: bool = False) -> None:
+        raise NotImplementedError
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/checkpoint_io/hybrid_parallel_checkpoint_io.py` & `colossalai-nightly-2024.5.4/colossalai/checkpoint_io/hybrid_parallel_checkpoint_io.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,14 +10,20 @@
 import torch.distributed as dist
 import torch.nn as nn
 from torch.distributed import ProcessGroup
 from torch.optim.lr_scheduler import _LRScheduler as LRScheduler
 
 from colossalai.cluster import DistCoordinator
 from colossalai.interface import ModelWrapper, OptimizerWrapper
+from colossalai.tensor.padded_tensor import (
+    init_as_padded_tensor,
+    is_padded_tensor,
+    to_padded_tensor,
+    to_unpadded_tensor,
+)
 from colossalai.utils import get_current_device
 
 from .general_checkpoint_io import GeneralCheckpointIO
 from .index_file import CheckpointIndexFile
 from .utils import (
     StateDictSharder,
     gather_distributed_param,
@@ -28,14 +34,15 @@
     load_state_dict,
     load_state_dict_into_model,
     load_states_into_optimizer,
     save_config_file,
     save_param_groups,
     save_state_dict,
     save_state_dict_shards,
+    search_padding_dim,
     search_tp_partition_dim,
     sharded_optimizer_loading_epilogue,
 )
 
 try:
     from torch.nn.modules.module import _EXTRA_STATE_KEY_SUFFIX
 except ImportError:
@@ -85,14 +92,16 @@
         state_dict_sharder = StateDictSharder(size_per_shard)
 
         # Save parameters.
         for name, param in model.named_parameters():
             if param is None:
                 continue
             # Gather tensor pieces when using tensor parallel.
+            if is_padded_tensor(param):
+                param = to_unpadded_tensor(param)
             param_ = gather_distributed_param(param, keep_vars=False)
             block, block_size = state_dict_sharder.append_param(prefix + name, param_)
             if block is not None:
                 yield block, block_size
 
         # Save buffers.
         for name, buf in model.named_buffers():
@@ -227,15 +236,14 @@
                         f"index located at {save_index_file}."
                     )
 
         else:
             # When pipeline is used, each stage produces its own shard files and index files.
             # Index files belonging to each stage are saved under a temporary folder ./tmp_index_files/
             # After all the state_dicts have been saved, the master rank integrates all the index files into one final index file and deletes the tmp folder.
-
             final_index_file_path = copy.deepcopy(save_index_file)
             tmp_index_file_folder = os.path.join(checkpoint, "tmp_index_files")
             Path(tmp_index_file_folder).mkdir(parents=True, exist_ok=True)
 
             # Manage filenames of sharded weights and index file for each pipeline stage.
             weights_name = weights_name.replace(".bin", f"-stage-{self.pp_rank+1:05d}-shard.bin")
             weights_name = weights_name.replace(".safetensors", f"-stage-{self.pp_rank+1:05d}-shard.safetensors")
@@ -247,14 +255,15 @@
                 checkpoint=checkpoint,
                 index_file=index_file,
                 base_filename=weights_name,
                 is_master=control_saving,
                 use_safetensors=use_safetensors,
                 use_pp_format=True,
             )
+
             if control_saving:
                 assert (
                     self.dp_rank == 0 and self.tp_rank == 0
                 ), "The saving process should have both dp_rank and tp_rank as 0."
                 index_file.append_meta_data("total_size", total_size)
                 index_file.write_index_file(save_index_file)
             else:
@@ -863,14 +872,19 @@
                 # Then gather TP shards.
                 partition_dim = search_tp_partition_dim(current_shape, original_shape, tp_size)
                 if partition_dim is not None:
                     gather_tensor = [torch.zeros_like(v) for _ in range(tp_size)]
                     dist.all_gather(gather_tensor, v, group=tp_group)
                     v = torch.cat(gather_tensor, dim=partition_dim)
 
+                padding_dim = search_padding_dim(v.shape, original_shape)
+                if padding_dim is not None:
+                    v = init_as_padded_tensor(v, v.shape[padding_dim], original_shape[padding_dim], padding_dim)
+                    v = to_unpadded_tensor(v)
+
                 state_[k] = v.detach().clone().to(device)
 
         return state_
 
     def shard_from_complete_optimizer_state(
         self,
         state: OrderedDict,
@@ -895,14 +909,27 @@
         """
         state_ = state if inplace else copy.deepcopy(state)
 
         for k, v in state_.items():
             if isinstance(v, torch.Tensor) and k != "step":
                 # Shard state along tensor parallel group.
                 partition_dim = search_tp_partition_dim(current_shape, original_shape, self.tp_size)
+                global_shape = current_shape
+                if partition_dim is not None:
+                    # pad embedding params
+                    global_shape = (
+                        *current_shape[:partition_dim],
+                        current_shape[partition_dim] * self.tp_size,
+                        *current_shape[partition_dim + 1 :],
+                    )
+
+                padding_dim = search_padding_dim(global_shape, original_shape)
+                if padding_dim is not None:
+                    v = to_padded_tensor(v, global_shape[padding_dim], padding_dim)
+
                 if partition_dim is not None:
                     slice_size = current_shape[partition_dim]
                     v = v.split(slice_size, dim=partition_dim)[self.tp_rank]
 
                 # Shard state along data parallel group when using Zero.
                 if self.use_zero:
                     padding_size = (self.dp_size - v.numel() % self.dp_size) % self.dp_size
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/checkpoint_io/index_file.py` & `colossalai-nightly-2024.5.4/colossalai/checkpoint_io/index_file.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/checkpoint_io/utils.py` & `colossalai-nightly-2024.5.4/colossalai/checkpoint_io/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -116,14 +116,23 @@
             original_shape[partition_dim] == tp_size * current_shape[partition_dim]
         ), f"The parameter isn't evenly distributed among tensor parallel group: \
                 shape before sharding {original_shape}, shape after sharding {current_shape}"
 
     return partition_dim
 
 
+def search_padding_dim(global_shape: torch.Size, original_shape: torch.Size) -> Optional[int]:
+    padding_dim = None
+    for dim, length in enumerate(global_shape):
+        if length > original_shape[dim]:
+            padding_dim = dim
+            break
+    return padding_dim
+
+
 # ======================================
 # Helper classes and functions for saving shard file
 # ======================================
 
 
 class StateDictSharder:
     def __init__(self, size_per_shard: int) -> None:
@@ -290,26 +299,27 @@
     yield state_dict_sharder.current_block, state_dict_sharder.current_block_size
 
 
 # ======================================
 # Helper functions for saving state dict
 # ======================================
 
+
 def save_state_dict(state_dict: dict, checkpoint_file_path: str, use_safetensors: bool) -> None:
     """
     Save state dict to checkpoint.
 
     Args:
         state_dict (dict): state dict.
         checkpoint_file_path (str): path to the checkpoint file.
         use_safetensors (bool): whether to use safetensors to save the checkpoint.
     """
     # Move all tensors in the state_dict to CPU before saving to avoid serialization issues
     state_dict_cpu = tree_map(lambda x: x.cpu() if torch.is_tensor(x) else x, state_dict)
-    
+
     if use_safetensors:
         assert is_safetensors_available(), "safetensors is not available."
         assert checkpoint_file_path.endswith(
             ".safetensors"
         ), "safetensors only supports .safetensors suffix for checkpoint file."
         from safetensors.torch import save_file as safe_save_file
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/cli/check/check_installation.py` & `colossalai-nightly-2024.5.4/colossalai/cli/check/check_installation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/cli/launcher/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/cli/launcher/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/cli/launcher/hostinfo.py` & `colossalai-nightly-2024.5.4/colossalai/cli/launcher/hostinfo.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/cli/launcher/multinode_runner.py` & `colossalai-nightly-2024.5.4/colossalai/cli/launcher/multinode_runner.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/cli/launcher/run.py` & `colossalai-nightly-2024.5.4/colossalai/cli/launcher/run.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/cluster/device_mesh_manager.py` & `colossalai-nightly-2024.5.4/colossalai/cluster/device_mesh_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/cluster/dist_coordinator.py` & `colossalai-nightly-2024.5.4/colossalai/cluster/dist_coordinator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/cluster/process_group_manager.py` & `colossalai-nightly-2024.5.4/colossalai/cluster/process_group_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/cluster/process_group_mesh.py` & `colossalai-nightly-2024.5.4/colossalai/cluster/process_group_mesh.py`

 * *Files 8% similar despite different names*

```diff
@@ -157,48 +157,81 @@
         Returns:
             List[int]: Ranks in the process group.
         """
         return list(self._group_to_ranks[group])
 
     @staticmethod
     def get_coords_along_axis(
-        base_coord: Tuple[int, ...], axis: int, indices_at_axis: List[int]
+        base_coord: Tuple[int, ...], axis: Union[int, List[int]], indices_at_axis: Union[List[int], List[List[int]]]
     ) -> List[Tuple[int, ...]]:
         """Get coordinates along the given axis.
 
         Args:
             base_coord (Tuple[int, ...]): Base coordinate which the coordinates along the axis are based on.
             axis (int): Axis along which the coordinates are generated.
             indices_at_axis (List[int]): Indices at the axis.
 
         Returns:
             List[Tuple[int, ...]]: Coordinates along the axis.
         """
-        coords_in_group = []
-        for idx in indices_at_axis:
-            coords_in_group.append(base_coord[:axis] + (idx,) + base_coord[axis + 1 :])
+        if isinstance(axis, int):
+            axis = [
+                axis,
+            ]
+            assert isinstance(indices_at_axis[0], int)
+            indices_at_axis = [
+                indices_at_axis,
+            ]
+
+        def add_index(base_coord, axis, indices_at_axis):
+            coords_in_group = []
+            for idx in indices_at_axis:
+                coords_in_group.append(base_coord[:axis] + (idx,) + base_coord[axis + 1 :])
+            return coords_in_group
+
+        coords_in_group = [base_coord]
+        for ax, indices_at_ax in zip(axis, indices_at_axis):
+            new_coords_in_group = []
+            for coords in coords_in_group:
+                new_coords_in_group += add_index(coords, ax, indices_at_ax)
+            coords_in_group = new_coords_in_group
+
         return coords_in_group
 
     def create_group_along_axis(
-        self, axis: int, indices_at_axis: Optional[List[int]] = None, backend: Optional[str] = None
+        self,
+        axis: Union[int, List[int]],
+        indices_at_axis: Optional[Union[List[int], List[List[int]]]] = None,
+        backend: Optional[str] = None,
     ) -> ProcessGroup:
         """Create all process groups along the given axis, and return the one which the current process belongs to.
 
         Args:
             axis (int): Axis along which the process groups are created.
             indices_at_axis (Optional[List[int]], optional): Indices at the axis. Defaults to None.
             backend (Optional[str], optional): Backend of the process group. Defaults to None.
 
         Returns:
             ProcessGroup: The process group along the given axis which the current process belongs to.
         """
-        indices_at_axis = indices_at_axis or list(range(self._shape[axis]))
+        if isinstance(axis, int):
+            axis = [
+                axis,
+            ]
+            if indices_at_axis is not None:
+                assert isinstance(indices_at_axis[0], int)
+                indices_at_axis = [
+                    indices_at_axis,
+                ]
+
+        indices_at_axis = indices_at_axis or [list(range(self._shape[ax])) for ax in axis]
         reduced_shape = list(self._shape)
         # the choices on the axis are reduced to 1, since it's determined by `indices_at_axis`
-        reduced_shape[axis] = 1
+        for ax in axis:
+            reduced_shape[ax] = 1
         target_group = None
         # use Cartesian product to generate all combinations of coordinates
         for base_coord in itertools.product(*[range(s) for s in reduced_shape]):
             coords_in_group = ProcessGroupMesh.get_coords_along_axis(base_coord, axis, indices_at_axis)
             ranks_in_group = tuple([ProcessGroupMesh.ravel(coord, self._shape) for coord in coords_in_group])
             group = self.get_group(ranks_in_group, backend=backend)
             if self._rank in ranks_in_group:
@@ -221,8 +254,7 @@
         indices_at_axis = indices_at_axis or list(range(self._shape[axis]))
         coords_in_group = ProcessGroupMesh.get_coords_along_axis(self._coord, axis, indices_at_axis)
         ranks_in_group = tuple([ProcessGroupMesh.ravel(coord, self._shape) for coord in coords_in_group])
         if ranks_in_group not in self._ranks_to_group:
             # no need to cache it explicitly, since it will be cached in `create_group_along_axis`
             return self.create_group_along_axis(axis, indices_at_axis, backend=backend)
         return self._ranks_to_group[ranks_in_group]
-
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/context/config.py` & `colossalai-nightly-2024.5.4/colossalai/context/config.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/context/singleton_meta.py` & `colossalai-nightly-2024.5.4/colossalai/context/singleton_meta.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,22 +1,27 @@
+import threading
+
+
 class SingletonMeta(type):
     """
-    The Singleton class can be implemented in different ways in Python. Some
-    possible methods include: base class, decorator, metaclass. We will use the
-    metaclass because it is best suited for this purpose.
+    Thread-safe Singleton Meta with double-checked locking.
+    Reference: https://en.wikipedia.org/wiki/Double-checked_locking
     """
 
     _instances = {}
+    _lock = threading.Lock()
 
     def __call__(cls, *args, **kwargs):
-        """
-        Possible changes to the value of the `__init__` argument do not affect
-        the returned instance.
-        """
+        # First check (without locking) for performance reasons
         if cls not in cls._instances:
-            instance = super().__call__(*args, **kwargs)
-            cls._instances[cls] = instance
+            # Acquire a lock before proceeding to the second check
+            with cls._lock:
+                # Second check with lock held to ensure thread safety
+                if cls not in cls._instances:
+                    instance = super().__call__(*args, **kwargs)
+                    cls._instances[cls] = instance
         else:
             assert (
                 len(args) == 0 and len(kwargs) == 0
-            ), f"{cls.__name__} is a singleton class and a instance has been created."
+            ), f"{cls.__name__} is a singleton class and an instance has been created."
+
         return cls._instances[cls]
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/device/alpha_beta_profiler.py` & `colossalai-nightly-2024.5.4/colossalai/device/alpha_beta_profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/device/calc_pipeline_strategy.py` & `colossalai-nightly-2024.5.4/colossalai/device/calc_pipeline_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/device/device_mesh.py` & `colossalai-nightly-2024.5.4/colossalai/device/device_mesh.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/_compatibility.py` & `colossalai-nightly-2024.5.4/colossalai/fx/_compatibility.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/_meta_regist_12.py` & `colossalai-nightly-2024.5.4/colossalai/fx/_meta_regist_12.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/_meta_regist_13.py` & `colossalai-nightly-2024.5.4/colossalai/fx/_meta_regist_13.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/codegen/activation_checkpoint_codegen.py` & `colossalai-nightly-2024.5.4/colossalai/fx/codegen/activation_checkpoint_codegen.py`

 * *Files 0% similar despite different names*

```diff
@@ -621,15 +621,15 @@
         if idx in offload_ends:
             within_offload_region = False
 
 
 if CODEGEN_AVAILABLE:
 
     class ActivationCheckpointCodeGen(CodeGen):
-        def _gen_python_code(self, nodes, root_module: str, namespace: _Namespace) -> PythonCode:
+        def _gen_python_code(self, nodes, root_module: str, namespace: _Namespace, verbose=None) -> PythonCode:
             free_vars: List[str] = []
             body: List[str] = []
             globals_: Dict[str, Any] = {}
             wrapped_fns: Dict[str, None] = {}
 
             # Wrap string in list to pass by reference
             maybe_return_annotation: List[str] = [""]
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/graph_module.py` & `colossalai-nightly-2024.5.4/colossalai/fx/graph_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/passes/adding_split_node_pass.py` & `colossalai-nightly-2024.5.4/colossalai/fx/passes/adding_split_node_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/passes/concrete_info_prop.py` & `colossalai-nightly-2024.5.4/colossalai/fx/passes/concrete_info_prop.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/passes/meta_info_prop.py` & `colossalai-nightly-2024.5.4/colossalai/fx/passes/meta_info_prop.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/passes/passes_for_gpt2_test.py` & `colossalai-nightly-2024.5.4/colossalai/fx/passes/passes_for_gpt2_test.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/passes/shard_1d_pass.py` & `colossalai-nightly-2024.5.4/colossalai/fx/passes/shard_1d_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/passes/split_module.py` & `colossalai-nightly-2024.5.4/colossalai/fx/passes/split_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/passes/utils.py` & `colossalai-nightly-2024.5.4/colossalai/fx/passes/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/constants.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/constants.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/dataflow.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/dataflow.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/constants.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/constants.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_function/activation_function.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/activation_function.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_function/arithmetic.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/arithmetic.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_function/embedding.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_function/normalization.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_function/pooling.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/pooling.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_function/torch_ops.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_function/torch_ops.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/activation_function.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/activation_function.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/attention.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/attention.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/convolution.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/convolution.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/normalization.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/pooling.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/pooling.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/profiler_module/rnn.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/profiler_module/rnn.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/registry.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/experimental/shard_utils.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/experimental/shard_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/memory_utils.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/memory_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/opcount.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/opcount.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/profiler.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/shard_utils.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/shard_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/profiler/tensor.py` & `colossalai-nightly-2024.5.4/colossalai/fx/profiler/tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/proxy.py` & `colossalai-nightly-2024.5.4/colossalai/fx/proxy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/_meta_trace.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/_meta_trace.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/_symbolic_trace.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/_symbolic_trace.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/_tracer_utils.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/_tracer_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addbmm.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addbmm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addmm.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/addmm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/bias_addition_function.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/bias_addition_function.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/linear.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_function/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/bias_addition_module.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/bias_addition_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/conv.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/bias_addition_patch/patched_bias_addition_module/conv.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/experimental.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/experimental.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_function/arithmetic.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/arithmetic.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_function/convolution.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/convolution.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_function/normalization.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_function/python_ops.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/python_ops.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_function/torch_ops.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_function/torch_ops.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_module/convolution.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/convolution.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_module/normalization.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_module/pooling.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/pooling.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/meta_patch/patched_module/rnn.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/meta_patch/patched_module/rnn.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/registry.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/fx/tracer/tracer.py` & `colossalai-nightly-2024.5.4/colossalai/fx/tracer/tracer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/engine/engine.py` & `colossalai-nightly-2024.5.4/colossalai/inference/engine/engine.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/engine/microbatch_manager.py` & `colossalai-nightly-2024.5.4/colossalai/inference/engine/microbatch_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/engine/modeling/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/engine/modeling/bloom.py` & `colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/bloom.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/engine/modeling/chatglm2.py` & `colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/chatglm2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/engine/modeling/llama.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/llama.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 import math
 from typing import List, Optional, Tuple
 
 import torch
 from transformers.models.llama.modeling_llama import LlamaAttention, LlamaDecoderLayer, LlamaForCausalLM, LlamaModel
 from transformers.utils import logging
 
-from colossalai.inference.kv_cache.batch_infer_state import BatchInferState
+from colossalai.inference.tensor_parallel.batch_infer_state import BatchInferState
 from colossalai.kernel.triton import llama_context_attn_fwd, token_attention_fwd
 from colossalai.kernel.triton.token_attention_kernel import Llama2TokenAttentionForwards
 from colossalai.pipeline.stage_manager import PipelineStageManager
 
 from ._utils import copy_kv_to_mem_cache
 
 try:
@@ -24,34 +24,29 @@
 
     HAS_LIGHTLLM_KERNEL = True
 except:
     print("please install lightllm from source to run inference: https://github.com/ModelTC/lightllm")
     HAS_LIGHTLLM_KERNEL = False
 
 try:
-    from colossalai.kernel.triton.flash_decoding import token_flash_decoding
-    HAS_TRITON_FLASH_DECODING_KERNEL = True
-except:
-    print("no triton flash decoding support, please install lightllm from https://github.com/ModelTC/lightllm/blob/ece7b43f8a6dfa74027adc77c2c176cff28c76c8")
-    HAS_TRITON_FLASH_DECODING_KERNEL = False
-    
-try:
     from flash_attn import flash_attn_with_kvcache
+
     HAS_FLASH_KERNEL = True
 except:
     HAS_FLASH_KERNEL = False
     print("please install flash attentiom from https://github.com/Dao-AILab/flash-attention")
 
 
 def rotate_half(x):
     """Rotates half the hidden dims of the input."""
     x1 = x[..., : x.shape[-1] // 2]
     x2 = x[..., x.shape[-1] // 2 :]
     return torch.cat((-x2, x1), dim=-1)
 
+
 def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
     # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.
     cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]
     sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]
     cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
     sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]
 
@@ -68,69 +63,66 @@
             llama_context_attn_fwd(
                 query_states,
                 key_states,
                 value_states,
                 attn_output,
                 infer_state.start_loc,
                 infer_state.seq_len,
+                # infer_state.cache_manager.past_key_values_length,
                 infer_state.max_len_in_batch,
             )
         else:
             lightllm_context_attention_fwd(
                 query_states,
                 key_states,
                 value_states,
                 attn_output,
                 infer_state.start_loc,
                 infer_state.seq_len,
+                # infer_state.cache_manager.past_key_values_length,
                 infer_state.max_len_in_batch,
             )
     else:
         assert HAS_LIGHTLLM_KERNEL is True, "You have to install lightllm kernels to run llama2 model"
         lightllm_llama2_context_attention_fwd(
             query_states,
             key_states,
             value_states,
             attn_output,
             infer_state.start_loc,
             infer_state.seq_len,
+            # infer_state.cache_manager.past_key_values_length,
             infer_state.max_len_in_batch,
         )
 
-def llama_triton_token_attention(query_states, attn_output, infer_state, num_key_value_groups=1, q_head_num = -1, head_dim = -1):
-    if HAS_TRITON_FLASH_DECODING_KERNEL and q_head_num != -1 and head_dim != -1:
-        token_flash_decoding(q = query_states, 
-                                o_tensor = attn_output, 
-                                infer_state = infer_state, 
-                                q_head_num = q_head_num, 
-                                head_dim = head_dim, 
-                                cache_k = infer_state.cache_manager.key_buffer[infer_state.decode_layer_id], 
-                                cache_v = infer_state.cache_manager.value_buffer[infer_state.decode_layer_id])
-        return 
-    
+
+def llama_triton_token_attention(query_states, attn_output, infer_state, num_key_value_groups=1):
+    assert HAS_LIGHTLLM_KERNEL is True, "You have to install lightllm kernel to run token attention for llama models"
     if num_key_value_groups == 1:
         token_attention_fwd(
             query_states,
             infer_state.cache_manager.key_buffer[infer_state.decode_layer_id],
             infer_state.cache_manager.value_buffer[infer_state.decode_layer_id],
             attn_output,
             infer_state.block_loc,
             infer_state.start_loc,
             infer_state.seq_len,
+            # infer_state.cache_manager.past_key_values_length,
             infer_state.max_len_in_batch,
         )
     else:
         Llama2TokenAttentionForwards.token_attn(
             query_states,
             infer_state.cache_manager.key_buffer[infer_state.decode_layer_id],
             infer_state.cache_manager.value_buffer[infer_state.decode_layer_id],
             attn_output,
             infer_state.block_loc,
             infer_state.start_loc,
             infer_state.seq_len,
+            # infer_state.cache_manager.past_key_values_length,
             infer_state.max_len_in_batch,
             infer_state.other_kv_index,
         )
 
 
 class LlamaInferenceForwards:
     """
@@ -171,15 +163,15 @@
         if output_attentions:
             logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
             output_attentions = False
         if output_hidden_states:
             logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
             output_hidden_states = False
 
-        # If is first stage and hidden_states is None, go throught lm_head first
+        # If is first stage and after warmup, go throught lm_head first
         if stage_manager.is_first_stage() and hidden_states is not None:
             lm_logits = self.lm_head(hidden_states)
             return {"logits": lm_logits}
 
         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
         outputs = LlamaInferenceForwards.llama_model_forward(
             self.model,
@@ -331,14 +323,23 @@
 
         # update indices
         # infer_state.block_loc[:, infer_state.max_len_in_batch-1] = infer_state.total_token_num + torch.arange(0, batch_size, dtype=torch.int32, device="cuda")
         infer_state.start_loc += torch.arange(0, batch_size, dtype=torch.int32, device="cuda")
         infer_state.seq_len += 1
         infer_state.max_len_in_batch += 1
 
+        # if not return_dict:
+        #     return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
+
+        # return BaseModelOutputWithPast(
+        #     last_hidden_state=hidden_states,
+        #     past_key_values=next_cache,
+        #     hidden_states=all_hidden_states,
+        #     attentions=all_self_attns,
+        # )
         return {"hidden_states": hidden_states}
 
     @staticmethod
     def llama_decoder_layer_forward(
         self: LlamaDecoderLayer,
         hidden_states: torch.Tensor,
         attention_mask: Optional[torch.Tensor] = None,
@@ -455,22 +456,18 @@
                     key_states,
                     value_states,
                     infer_state.decode_mem_index,
                     infer_state.cache_manager,
                 )
 
             if HAS_LIGHTLLM_KERNEL:
-                
                 attn_output = torch.empty_like(query_states)
-                llama_triton_token_attention(query_states = query_states, 
-                                             attn_output = attn_output, 
-                                             infer_state = infer_state, 
-                                             num_key_value_groups = self.num_key_value_groups, 
-                                             q_head_num = q_len * self.num_heads, 
-                                             head_dim = self.head_dim)
+                llama_triton_token_attention(
+                    query_states, attn_output, infer_state, num_key_value_groups=self.num_key_value_groups
+                )
             else:
                 self.num_heads // self.num_key_value_heads
                 cache_k = infer_state.cache_manager.key_buffer[infer_state.decode_layer_id]
                 cache_v = infer_state.cache_manager.value_buffer[infer_state.decode_layer_id]
 
                 query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim)
                 copy_cache_k = cache_k.view(bsz, -1, self.num_key_value_heads, self.head_dim)
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/engine/policies/bloom.py` & `colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/bloom.py`

 * *Files 2% similar despite different names*

```diff
@@ -110,18 +110,18 @@
         if self.model.__class__.__name__ == "BloomModel":
             module = self.model
         else:
             module = self.model.transformer
         stage_manager = self.pipeline_stage_manager
 
         held_layers = []
-        layers_per_stage = self.distribute_layers(len(module.h), stage_manager.num_stages)
+        layers_per_stage = stage_manager.distribute_layers(len(module.h))
         if stage_manager.is_first_stage():
             held_layers.append(module.word_embeddings)
             held_layers.append(module.word_embeddings_layernorm)
             held_layers.append(self.model.lm_head)
-        start_idx, end_idx = self.get_stage_index(layers_per_stage, stage_manager.stage)
+        start_idx, end_idx = stage_manager.get_stage_index(layers_per_stage)
         held_layers.extend(module.h[start_idx:end_idx])
         if stage_manager.is_last_stage():
             held_layers.append(module.ln_f)
 
         return held_layers
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/engine/policies/chatglm2.py` & `colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/chatglm2.py`

 * *Files 2% similar despite different names*

```diff
@@ -65,19 +65,19 @@
         return policy
 
     def get_held_layers(self) -> List[nn.Module]:
         module = self.model.transformer
         stage_manager = self.pipeline_stage_manager
 
         held_layers = []
-        layers_per_stage = self.distribute_layers(module.num_layers, stage_manager.num_stages)
+        layers_per_stage = stage_manager.distribute_layers(module.num_layers)
         if stage_manager.is_first_stage():
             held_layers.append(module.embedding)
             held_layers.append(module.output_layer)
-        start_idx, end_idx = self.get_stage_index(layers_per_stage, stage_manager.stage)
+        start_idx, end_idx = stage_manager.get_stage_index(layers_per_stage)
         held_layers.extend(module.encoder.layers[start_idx:end_idx])
         if stage_manager.is_last_stage():
             if module.encoder.post_layer_norm:
                 held_layers.append(module.encoder.final_layernorm)
 
         # rotary_pos_emb is needed for all stages
         held_layers.append(module.rotary_pos_emb)
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/engine/policies/llama.py` & `colossalai-nightly-2024.5.4/colossalai/inference/engine/policies/llama.py`

 * *Files 1% similar despite different names*

```diff
@@ -190,17 +190,17 @@
         if self.model.__class__.__name__ == "LlamaModel":
             module = self.model
         else:
             module = self.model.model
         stage_manager = self.pipeline_stage_manager
 
         held_layers = []
-        layers_per_stage = self.distribute_layers(len(module.layers), stage_manager.num_stages)
+        layers_per_stage = stage_manager.distribute_layers(len(module.layers))
         if stage_manager.is_first_stage():
             held_layers.append(module.embed_tokens)
             held_layers.append(self.model.lm_head)
-        start_idx, end_idx = self.get_stage_index(layers_per_stage, stage_manager.stage)
+        start_idx, end_idx = stage_manager.get_stage_index(layers_per_stage)
         held_layers.extend(module.layers[start_idx:end_idx])
         if stage_manager.is_last_stage():
             held_layers.append(module.norm)
 
         return held_layers
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/kv_cache/batch_infer_state.py` & `colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/batch_infer_state.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/kv_cache/kvcache_manager.py` & `colossalai-nightly-2024.5.4/colossalai/inference/kv_cache/kvcache_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/quant/gptq/cai_gptq/cai_quant_linear.py` & `colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/cai_gptq/cai_quant_linear.py`

 * *Files 14% similar despite different names*

```diff
@@ -14,87 +14,94 @@
 from colossalai.shardformer.layer import ParallelModule
 
 from .gptq_op import CaiGPTQLinearOp
 
 HAS_GPTQ_CUDA = False
 try:
     from colossalai.kernel.op_builder.gptq import GPTQBuilder
+
     gptq_cuda = GPTQBuilder().load()
     HAS_GPTQ_CUDA = True
 except ImportError:
-    warnings.warn('CUDA gptq is not installed')
+    warnings.warn("CUDA gptq is not installed")
     HAS_GPTQ_CUDA = False
 
 
 class CaiQuantLinear(nn.Module):
-
     def __init__(self, bits, groupsize, infeatures, outfeatures, bias, tp_size=1, tp_rank=0, row_split=False):
         super().__init__()
         if bits not in [2, 4, 8]:
             raise NotImplementedError("Only 2,4,8 bits are supported.")
         self.infeatures = infeatures
         self.outfeatures = outfeatures
         self.bits = bits
         self.maxq = 2**self.bits - 1
         self.groupsize = groupsize if groupsize != -1 else infeatures
 
-        self.register_buffer('qweight', torch.zeros((infeatures // 32 * self.bits, outfeatures), dtype=torch.int32))
+        self.register_buffer("qweight", torch.zeros((infeatures // 32 * self.bits, outfeatures), dtype=torch.int32))
+        self.register_buffer(
+            "qzeros",
+            torch.zeros((math.ceil(infeatures / self.groupsize), outfeatures // 32 * self.bits), dtype=torch.int32),
+        )
         self.register_buffer(
-            'qzeros',
-            torch.zeros((math.ceil(infeatures / self.groupsize), outfeatures // 32 * self.bits), dtype=torch.int32))
-        self.register_buffer('scales',
-                             torch.zeros((math.ceil(infeatures / self.groupsize), outfeatures), dtype=torch.float16))
+            "scales", torch.zeros((math.ceil(infeatures / self.groupsize), outfeatures), dtype=torch.float16)
+        )
         if row_split:
             self.register_buffer(
-                'g_idx',
-                torch.tensor([(i + (tp_rank * self.infeatures)) // self.groupsize for i in range(infeatures)],
-                             dtype=torch.int32))
+                "g_idx",
+                torch.tensor(
+                    [(i + (tp_rank * self.infeatures)) // self.groupsize for i in range(infeatures)], dtype=torch.int32
+                ),
+            )
         else:
-            self.register_buffer('g_idx',
-                                 torch.tensor([i // self.groupsize for i in range(infeatures)], dtype=torch.int32))
+            self.register_buffer(
+                "g_idx", torch.tensor([i // self.groupsize for i in range(infeatures)], dtype=torch.int32)
+            )
 
         if bias:
-            self.register_buffer('bias', torch.zeros((outfeatures), dtype=torch.float16))
+            self.register_buffer("bias", torch.zeros((outfeatures), dtype=torch.float16))
         else:
             self.bias = None
 
         self.gptq_linear = CaiGPTQLinearOp(groupsize, bits)
 
         self.q4 = None
         self.empty_tensor = torch.empty((1, 1), device="meta")
         self.tp_size = tp_size
         self.tp_rank = tp_rank
         self.row_split = row_split
 
     def pack(self, linear, scales, zeros, g_idx=None):
-
-        g_idx = g_idx.clone() if g_idx is not None else torch.tensor(
-            [i // self.groupsize for i in range(self.infeatures)], dtype=torch.int32)
+        g_idx = (
+            g_idx.clone()
+            if g_idx is not None
+            else torch.tensor([i // self.groupsize for i in range(self.infeatures)], dtype=torch.int32)
+        )
 
         scales = scales.t().contiguous()
         zeros = zeros.t().contiguous()
         scale_zeros = zeros * scales
         half_scales = scales.clone().half()
         # print("scale shape ", scales.shape, scale_zeros.shape, linear.weight.shape)
         self.scales = scales.clone().half()
         if linear.bias is not None:
             self.bias = linear.bias.clone().half()
 
-        wn = 8
         pbits = 32
         ptype = torch.int32
         unsign_type = np.uint32
         sign_type = np.int32
 
         intweight = []
         for idx in range(self.infeatures):
             intweight.append(
-                torch.round(
-                    (linear.weight.data[:, idx] + scale_zeros[g_idx[idx]]) / half_scales[g_idx[idx]]).to(ptype)[:,
-                                                                                                                None])
+                torch.round((linear.weight.data[:, idx] + scale_zeros[g_idx[idx]]) / half_scales[g_idx[idx]]).to(ptype)[
+                    :, None
+                ]
+            )
         intweight = torch.cat(intweight, dim=1)
         intweight = intweight.t().contiguous()
         intweight = intweight.numpy().astype(unsign_type)
         qweight = np.zeros((intweight.shape[0] // pbits * self.bits, intweight.shape[1]), dtype=unsign_type)
 
         i = 0
         row = 0
@@ -105,15 +112,15 @@
                     qweight[row] |= intweight[j] << (self.bits * (j - i))
                 i += pbits // self.bits
                 row += 1
             else:
                 raise NotImplementedError("Only 2,4,8 bits are supported.")
         qweight = qweight.astype(sign_type)
         qweight1 = torch.from_numpy(qweight)
-        qweight1 = qweight1.contiguous()    #.to("cuda")
+        qweight1 = qweight1.contiguous()  # .to("cuda")
         self.qweight.data.copy_(qweight1)
 
         qzeros = np.zeros((zeros.shape[0], zeros.shape[1] // pbits * self.bits), dtype=unsign_type)
         zeros -= 1
         zeros = zeros.numpy().astype(unsign_type)
         i = 0
         col = 0
@@ -136,40 +143,42 @@
             self.g_idx = g_idx
 
     def init_q4(self):
         assert self.qweight.device.type == "cuda"
         self.q4_width = self.qweight.shape[1]
         if self.g_idx is not None:
             if self.row_split and torch.equal(
-                    self.g_idx,
-                    torch.tensor(
-                        [(i + (self.tp_rank * self.infeatures)) // self.groupsize for i in range(self.infeatures)],
-                        dtype=torch.int32,
-                        device=self.g_idx.device)):
+                self.g_idx,
+                torch.tensor(
+                    [(i + (self.tp_rank * self.infeatures)) // self.groupsize for i in range(self.infeatures)],
+                    dtype=torch.int32,
+                    device=self.g_idx.device,
+                ),
+            ):
                 self.g_idx = None
             elif torch.equal(
-                    self.g_idx,
-                    torch.tensor([i // self.groupsize for i in range(self.infeatures)],
-                                 dtype=torch.int32,
-                                 device=self.g_idx.device)):
+                self.g_idx,
+                torch.tensor(
+                    [i // self.groupsize for i in range(self.infeatures)], dtype=torch.int32, device=self.g_idx.device
+                ),
+            ):
                 self.g_idx = None
 
         if self.g_idx is not None:
             g_idx = self.g_idx.to("cpu")
         else:
             g_idx = self.empty_tensor
 
         self.q4 = gptq_cuda.make_q4(self.qweight, self.qzeros, self.scales, g_idx, torch.cuda.current_device())
         torch.cuda.synchronize()
 
     def forward(self, x):
         outshape = x.shape[:-1] + (self.outfeatures,)
 
         if HAS_GPTQ_CUDA and self.bits == 4:
-
             if self.q4 is None:
                 self.init_q4()
 
             x = x.view(-1, x.shape[-1])
             output = torch.empty((x.shape[0], self.outfeatures), dtype=torch.float16, device=x.device)
             gptq_cuda.q4_matmul(x.half(), self.q4, output)
             if self.bias is not None and (not self.row_split or self.tp_size == 1):
@@ -187,114 +196,109 @@
                 g_idx=self.g_idx,
                 bias=bias,
             )
         return output.view(outshape)
 
 
 def split_column_copy(gptq_linear, cai_linear, tp_size=1, tp_rank=0, split_num=1):
-
     qweights = gptq_linear.qweight.split(gptq_linear.out_features // split_num, dim=-1)
     qzeros = gptq_linear.qzeros.split(gptq_linear.out_features // (32 // cai_linear.bits) // split_num, dim=-1)
     scales = gptq_linear.scales.split(gptq_linear.out_features // split_num, dim=-1)
     g_idx = gptq_linear.g_idx
     if gptq_linear.bias is not None:
         bias = gptq_linear.bias.split(gptq_linear.out_features // split_num, dim=-1)
 
     cai_split_out_features = cai_linear.outfeatures // split_num
     zero_split_block = cai_linear.outfeatures // (32 // cai_linear.bits) // split_num
 
     for i in range(split_num):
-        cai_linear.qweight[:, i * cai_split_out_features:(i + 1) *
-                           cai_split_out_features] = qweights[i][:, tp_rank * cai_split_out_features:(tp_rank + 1) *
-                                                                 cai_split_out_features]
-        cai_linear.qzeros[:, i * zero_split_block:(i + 1) *
-                          zero_split_block] = qzeros[i][:, tp_rank * zero_split_block:(tp_rank + 1) * zero_split_block]
-        cai_linear.scales[:, i * cai_split_out_features:(i + 1) *
-                          cai_split_out_features] = scales[i][:, tp_rank * cai_split_out_features:(tp_rank + 1) *
-                                                              cai_split_out_features]
+        cai_linear.qweight[:, i * cai_split_out_features : (i + 1) * cai_split_out_features] = qweights[i][
+            :, tp_rank * cai_split_out_features : (tp_rank + 1) * cai_split_out_features
+        ]
+        cai_linear.qzeros[:, i * zero_split_block : (i + 1) * zero_split_block] = qzeros[i][
+            :, tp_rank * zero_split_block : (tp_rank + 1) * zero_split_block
+        ]
+        cai_linear.scales[:, i * cai_split_out_features : (i + 1) * cai_split_out_features] = scales[i][
+            :, tp_rank * cai_split_out_features : (tp_rank + 1) * cai_split_out_features
+        ]
         if cai_linear.bias is not None:
-            cai_linear.bias[i * cai_split_out_features:(i + 1) *
-                            cai_split_out_features] = bias[i][tp_rank * cai_split_out_features:(tp_rank + 1) *
-                                                              cai_split_out_features]
+            cai_linear.bias[i * cai_split_out_features : (i + 1) * cai_split_out_features] = bias[i][
+                tp_rank * cai_split_out_features : (tp_rank + 1) * cai_split_out_features
+            ]
 
     cai_linear.g_idx.copy_(g_idx)
 
 
 def split_row_copy(gptq_linear, cai_linear, tp_rank=0, split_num=1):
-
     qweights = gptq_linear.qweight.split(gptq_linear.in_features // split_num, dim=0)
     qzeros = gptq_linear.qzeros.split(gptq_linear.in_features // split_num, dim=0)
     scales = gptq_linear.scales.split(gptq_linear.in_features // split_num, dim=0)
     g_idxs = gptq_linear.g_idx.split(gptq_linear.in_features // split_num, dim=0)
 
     cai_split_in_features = cai_linear.infeatures // (32 // cai_linear.bits) // split_num
     zero_split_block = cai_linear.infeatures // cai_linear.groupsize // split_num
     idx_split_features = cai_linear.infeatures // split_num
 
     for i in range(split_num):
-        cai_linear.qweight[i * cai_split_in_features:(i + 1) *
-                           cai_split_in_features, :] = qweights[i][tp_rank * cai_split_in_features:(tp_rank + 1) *
-                                                                   cai_split_in_features, :]
-        cai_linear.qzeros[i * zero_split_block:(i + 1) *
-                          zero_split_block, :] = qzeros[i][tp_rank * zero_split_block:(tp_rank + 1) *
-                                                           zero_split_block, :]
-        cai_linear.scales[i * zero_split_block:(i + 1) *
-                          zero_split_block, :] = scales[i][tp_rank * zero_split_block:(tp_rank + 1) *
-                                                           zero_split_block, :]
-        cai_linear.g_idx[i * idx_split_features:(i + 1) *
-                         idx_split_features] = g_idxs[i][tp_rank * idx_split_features:(tp_rank + 1) *
-                                                         idx_split_features]
+        cai_linear.qweight[i * cai_split_in_features : (i + 1) * cai_split_in_features, :] = qweights[i][
+            tp_rank * cai_split_in_features : (tp_rank + 1) * cai_split_in_features, :
+        ]
+        cai_linear.qzeros[i * zero_split_block : (i + 1) * zero_split_block, :] = qzeros[i][
+            tp_rank * zero_split_block : (tp_rank + 1) * zero_split_block, :
+        ]
+        cai_linear.scales[i * zero_split_block : (i + 1) * zero_split_block, :] = scales[i][
+            tp_rank * zero_split_block : (tp_rank + 1) * zero_split_block, :
+        ]
+        cai_linear.g_idx[i * idx_split_features : (i + 1) * idx_split_features] = g_idxs[i][
+            tp_rank * idx_split_features : (tp_rank + 1) * idx_split_features
+        ]
     if cai_linear.bias is not None:
         cai_linear.bias.copy_(gptq_linear.bias)
 
 
 class RowCaiQuantLinear(CaiQuantLinear, ParallelModule):
-
     def __init__(self, bits, groupsize, infeatures, outfeatures, bias, tp_size=1, tp_rank=0, row_split=False):
-
-        super().__init__(bits,
-                         groupsize,
-                         infeatures,
-                         outfeatures,
-                         bias,
-                         tp_size=tp_size,
-                         tp_rank=tp_rank,
-                         row_split=row_split)
+        super().__init__(
+            bits, groupsize, infeatures, outfeatures, bias, tp_size=tp_size, tp_rank=tp_rank, row_split=row_split
+        )
         self.process_group = None
 
     @staticmethod
-    def from_native_module(module: nn.Module, process_group: Union[ProcessGroup, List[ProcessGroup]], *args,
-                           **kwargs) -> ParallelModule:
+    def from_native_module(
+        module: nn.Module, process_group: Union[ProcessGroup, List[ProcessGroup]], *args, **kwargs
+    ) -> ParallelModule:
         LazyInitContext.materialize(module)
         # get the attributes
         in_features = module.in_features
 
         # ensure only one process group is passed
         if isinstance(process_group, (list, tuple)):
-            assert len(process_group) == 1, \
-                f'Expected only one process group, got {len(process_group)}.'
+            assert len(process_group) == 1, f"Expected only one process group, got {len(process_group)}."
             process_group = process_group[0]
 
         tp_size = dist.get_world_size(process_group)
         tp_rank = dist.get_rank(process_group)
 
         if in_features < tp_size:
             return module
 
         if in_features % tp_size != 0:
             raise ValueError(
-                f"The size of in_features:{in_features} is not integer multiples of tensor parallel size: {tp_size}!")
-        linear_1d = RowCaiQuantLinear(module.bits,
-                                      module.group_size,
-                                      module.in_features // tp_size,
-                                      module.out_features,
-                                      module.bias is not None,
-                                      tp_size=tp_size,
-                                      tp_rank=tp_rank,
-                                      row_split=True)
+                f"The size of in_features:{in_features} is not integer multiples of tensor parallel size: {tp_size}!"
+            )
+        linear_1d = RowCaiQuantLinear(
+            module.bits,
+            module.group_size,
+            module.in_features // tp_size,
+            module.out_features,
+            module.bias is not None,
+            tp_size=tp_size,
+            tp_rank=tp_rank,
+            row_split=True,
+        )
         linear_1d.process_group = process_group
 
         split_row_copy(module, linear_1d, tp_rank=tp_rank, **kwargs)
         return linear_1d
 
     def forward(self, x):
         output = super().forward(x)
@@ -302,53 +306,49 @@
             dist.all_reduce(output, op=dist.ReduceOp.SUM, group=self.process_group)
             if self.bias is not None:
                 output.add_(self.bias)
         return output
 
 
 class ColCaiQuantLinear(CaiQuantLinear, ParallelModule):
-
     def __init__(self, bits, groupsize, infeatures, outfeatures, bias, tp_size=1, tp_rank=0, row_split=False):
-
-        super().__init__(bits,
-                         groupsize,
-                         infeatures,
-                         outfeatures,
-                         bias,
-                         tp_size=tp_size,
-                         tp_rank=tp_rank,
-                         row_split=row_split)
+        super().__init__(
+            bits, groupsize, infeatures, outfeatures, bias, tp_size=tp_size, tp_rank=tp_rank, row_split=row_split
+        )
         self.process_group = None
 
     @staticmethod
-    def from_native_module(module: nn.Module, process_group: Union[ProcessGroup, List[ProcessGroup]], *args,
-                           **kwargs) -> ParallelModule:
+    def from_native_module(
+        module: nn.Module, process_group: Union[ProcessGroup, List[ProcessGroup]], *args, **kwargs
+    ) -> ParallelModule:
         LazyInitContext.materialize(module)
         # get the attributes
         in_features = module.in_features
 
         # ensure only one process group is passed
         if isinstance(process_group, (list, tuple)):
-            assert len(process_group) == 1, \
-                f'Expected only one process group, got {len(process_group)}.'
+            assert len(process_group) == 1, f"Expected only one process group, got {len(process_group)}."
             process_group = process_group[0]
 
         tp_size = dist.get_world_size(process_group)
         tp_rank = dist.get_rank(process_group)
 
         if in_features < tp_size:
             return module
 
         if in_features % tp_size != 0:
             raise ValueError(
-                f"The size of in_features:{in_features} is not integer multiples of tensor parallel size: {tp_size}!")
-        linear_1d = ColCaiQuantLinear(module.bits,
-                                      module.group_size,
-                                      module.in_features,
-                                      module.out_features // tp_size,
-                                      module.bias is not None,
-                                      tp_size=tp_size,
-                                      tp_rank=tp_rank)
+                f"The size of in_features:{in_features} is not integer multiples of tensor parallel size: {tp_size}!"
+            )
+        linear_1d = ColCaiQuantLinear(
+            module.bits,
+            module.group_size,
+            module.in_features,
+            module.out_features // tp_size,
+            module.bias is not None,
+            tp_size=tp_size,
+            tp_rank=tp_rank,
+        )
         linear_1d.process_group = process_group
 
         split_column_copy(module, linear_1d, tp_rank=tp_rank, **kwargs)
         return linear_1d
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/quant/gptq/cai_gptq/gptq_op.py` & `colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/cai_gptq/gptq_op.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/quant/gptq/gptq_manager.py` & `colossalai-nightly-2024.5.4/colossalai/inference/quant/gptq/gptq_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/quant/smoothquant/models/base_model.py` & `colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/base_model.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/quant/smoothquant/models/linear.py` & `colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/quant/smoothquant/models/llama.py` & `colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/llama.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/inference/quant/smoothquant/models/parallel_linear.py` & `colossalai-nightly-2024.5.4/colossalai/inference/quant/smoothquant/models/parallel_linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/initialize.py` & `colossalai-nightly-2024.5.4/colossalai/initialize.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,25 +1,20 @@
 #!/usr/bin/env python
 # -*- encoding: utf-8 -*-
 
 import os
-import warnings
-from pathlib import Path
-from typing import Dict, Union
 
 import torch.distributed as dist
 
 from colossalai.accelerator import get_accelerator
-from colossalai.context import Config
 from colossalai.logging import get_dist_logger
 from colossalai.utils import set_seed
 
 
 def launch(
-    config: Union[str, Path, Config, Dict],
     rank: int,
     world_size: int,
     host: str,
     port: int,
     backend: str = "nccl",
     local_rank: int = None,
     seed: int = 1024,
@@ -40,16 +35,14 @@
             defaults to None. If local_rank = None, the default device ordinal will be calculated automatically.
         seed (int, optional): Specified random seed for every process. Defaults to 1024.
         verbose (bool, optional): Whether to print logs. Defaults to True.
 
     Raises:
         Exception: Raise exception when config type is wrong
     """
-    if rank == 0:
-        warnings.warn("`config` is deprecated and will be removed soon.")
 
     cur_accelerator = get_accelerator()
 
     backend = cur_accelerator.communication_backend
 
     # init default process group
     init_method = f"tcp://[{host}]:{port}"
@@ -64,15 +57,14 @@
 
     if verbose:
         logger = get_dist_logger()
         logger.info(f"Distributed environment is initialized, world size: {dist.get_world_size()}", ranks=[0])
 
 
 def launch_from_slurm(
-    config: Union[str, Path, Config, Dict],
     host: str,
     port: int,
     backend: str = "nccl",
     seed: int = 1024,
     verbose: bool = True,
 ):
     """A wrapper for colossalai.launch for SLURM launcher by reading rank and world size from the environment variables
@@ -91,27 +83,25 @@
         world_size = int(os.environ["SLURM_NPROCS"])
     except KeyError as e:
         raise RuntimeError(
             f"Could not find {e} in the SLURM environment, visit https://www.colossalai.org/ for more information on launching with SLURM"
         )
 
     launch(
-        config=config,
         rank=rank,
         world_size=world_size,
         host=host,
         port=port,
         backend=backend,
         seed=seed,
         verbose=verbose,
     )
 
 
 def launch_from_openmpi(
-    config: Union[str, Path, Config, Dict],
     host: str,
     port: int,
     backend: str = "nccl",
     seed: int = 1024,
     verbose: bool = True,
 ):
     """A wrapper for colossalai.launch for OpenMPI launcher by reading rank and world size from the environment variables
@@ -131,29 +121,26 @@
         world_size = int(os.environ["OMPI_COMM_WORLD_SIZE"])
     except KeyError as e:
         raise RuntimeError(
             f"Could not find {e} in the OpenMPI environment, visit https://www.colossalai.org/ for more information on launching with OpenMPI"
         )
 
     launch(
-        config=config,
         local_rank=local_rank,
         rank=rank,
         world_size=world_size,
         host=host,
         port=port,
         backend=backend,
         seed=seed,
         verbose=verbose,
     )
 
 
-def launch_from_torch(
-    config: Union[str, Path, Config, Dict], backend: str = "nccl", seed: int = 1024, verbose: bool = True
-):
+def launch_from_torch(backend: str = "nccl", seed: int = 1024, verbose: bool = True):
     """A wrapper for colossalai.launch for torchrun or torch.distributed.launch by reading rank and world size
     from the environment variables set by PyTorch
 
     Args:
         config (Union[str, dict, Config]): Config file or config file path are both acceptable
         backend (str, optional): Backend for ``torch.distributed``, defaults to ``nccl``
         seed (int, optional): Specified random seed for every process. Defaults to 1024.
@@ -167,15 +154,14 @@
         port = int(os.environ["MASTER_PORT"])
     except KeyError as e:
         raise RuntimeError(
             f"Could not find {e} in the torch environment, visit https://www.colossalai.org/ for more information on launching with torch"
         )
 
     launch(
-        config=config,
         local_rank=local_rank,
         rank=rank,
         world_size=world_size,
         host=host,
         port=port,
         backend=backend,
         seed=seed,
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/interface/model.py` & `colossalai-nightly-2024.5.4/colossalai/interface/model.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/interface/optimizer.py` & `colossalai-nightly-2024.5.4/colossalai/interface/optimizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,36 +1,32 @@
 from .cpu_adam import CpuAdamArmExtension, CpuAdamX86Extension
-from .flash_attention import (
-    FlashAttentionDaoCudaExtension,
-    FlashAttentionNpuExtension,
-    FlashAttentionXformersCudaExtension,
-)
+from .flash_attention import FlashAttentionDaoCudaExtension, FlashAttentionNpuExtension, FlashAttentionSdpaCudaExtension
 from .layernorm import LayerNormCudaExtension
 from .moe import MoeCudaExtension
 from .optimizer import FusedOptimizerCudaExtension
 from .softmax import ScaledMaskedSoftmaxCudaExtension, ScaledUpperTriangleMaskedSoftmaxCudaExtension
 
 ALL_EXTENSIONS = [
     CpuAdamArmExtension,
     CpuAdamX86Extension,
     LayerNormCudaExtension,
     MoeCudaExtension,
     FusedOptimizerCudaExtension,
     ScaledMaskedSoftmaxCudaExtension,
     ScaledUpperTriangleMaskedSoftmaxCudaExtension,
     FlashAttentionDaoCudaExtension,
-    FlashAttentionXformersCudaExtension,
+    FlashAttentionSdpaCudaExtension,
     FlashAttentionNpuExtension,
 ]
 
 __all__ = [
     "CpuAdamArmExtension",
     "CpuAdamX86Extension",
     "LayerNormCudaExtension",
     "MoeCudaExtension",
     "FusedOptimizerCudaExtension",
     "ScaledMaskedSoftmaxCudaExtension",
     "ScaledUpperTriangleMaskedSoftmaxCudaExtension",
     "FlashAttentionDaoCudaExtension",
-    "FlashAttentionXformersCudaExtension",
+    "FlashAttentionSdpaCudaExtension",
     "FlashAttentionNpuExtension",
 ]
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/base_extension.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/base_extension.py`

 * *Files 2% similar despite different names*

```diff
@@ -54,21 +54,21 @@
         # concat
         home_directory = os.path.expanduser("~")
         extension_directory = f".cache/colossalai/torch_extensions/torch{torch_version_major}.{torch_version_minor}_{device_name}-{device_version}-{hash_suffix}"
         cache_directory = os.path.join(home_directory, extension_directory)
         return cache_directory
 
     @abstractmethod
-    def is_hardware_available(self) -> bool:
+    def is_available(self) -> bool:
         """
         Check if the hardware required by the kernel is available.
         """
 
     @abstractmethod
-    def assert_hardware_compatible(self) -> None:
+    def assert_compatible(self) -> None:
         """
         Check if the hardware required by the kernel is compatible.
         """
 
     @abstractmethod
     def build_aot(self) -> Union["CppExtension", "CUDAExtension"]:
         pass
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/cpp_extension.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpp_extension.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/cpu_adam/cpu_adam_arm.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpu_adam/cpu_adam_arm.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,19 +3,19 @@
 from ..cpp_extension import _CppExtension
 
 
 class CpuAdamArmExtension(_CppExtension):
     def __init__(self):
         super().__init__(name="cpu_adam_arm")
 
-    def is_hardware_available(self) -> bool:
+    def is_available(self) -> bool:
         # only arm allowed
         return platform.machine() == "aarch64"
 
-    def assert_hardware_compatible(self) -> None:
+    def assert_compatible(self) -> None:
         arch = platform.machine()
         assert (
             arch == "aarch64"
         ), f"[extension] The {self.name} kernel requires the CPU architecture to be aarch64 but got {arch}"
 
     # necessary 4 functions
     def sources_files(self):
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/cpu_adam/cpu_adam_x86.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cpu_adam/cpu_adam_x86.py`

 * *Files 7% similar despite different names*

```diff
@@ -4,23 +4,23 @@
 from ..utils import append_nvcc_threads
 
 
 class CpuAdamX86Extension(_CudaExtension):
     def __init__(self):
         super().__init__(name="cpu_adam_x86")
 
-    def is_hardware_available(self) -> bool:
-        return platform.machine() == "x86_64" and super().is_hardware_available()
+    def is_available(self) -> bool:
+        return platform.machine() == "x86_64" and super().is_available()
 
-    def assert_hardware_compatible(self) -> None:
+    def assert_compatible(self) -> None:
         arch = platform.machine()
         assert (
             arch == "x86_64"
         ), f"[extension] The {self.name} kernel requires the CPU architecture to be x86_64 but got {arch}"
-        super().assert_hardware_compatible()
+        super().assert_compatible()
 
     # necessary 4 functions
     def sources_files(self):
         ret = [
             self.csrc_abs_path("cuda/cpu_adam.cpp"),
         ]
         return ret
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.cpp` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.h` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/arm/cpu_adam_arm.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/colossal_C_frontend.cpp` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/colossal_C_frontend.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/cpu_adam.cpp` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/cpu_adam.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/cpu_adam.h` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/cpu_adam.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/include/block_reduce.h` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/include/block_reduce.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda.cpp` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda_kernel.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/layer_norm_cuda_kernel.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/moe_cuda.cpp` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/moe_cuda.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/moe_cuda_kernel.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/moe_cuda_kernel.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/multi_tensor_adam.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_adam.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/multi_tensor_apply.cuh` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_apply.cuh`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/multi_tensor_lamb.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_lamb.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/multi_tensor_scale_kernel.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_scale_kernel.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.cpp` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.h` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax_cuda.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_masked_softmax_cuda.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/cuda/type_shim.h` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/cuda/type_shim.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/csrc/scaled_softmax.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/csrc/scaled_softmax.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/cuda_extension.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/cuda_extension.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,25 +18,25 @@
 class _CudaExtension(_CppExtension):
     @abstractmethod
     def nvcc_flags(self) -> List[str]:
         """
         This function should return a list of nvcc compilation flags for extensions.
         """
 
-    def is_hardware_available(self) -> bool:
+    def is_available(self) -> bool:
         # cuda extension can only be built if cuda is available
         try:
             import torch
 
             cuda_available = torch.cuda.is_available()
         except:
             cuda_available = False
         return cuda_available
 
-    def assert_hardware_compatible(self) -> None:
+    def assert_compatible(self) -> None:
         from torch.utils.cpp_extension import CUDA_HOME
 
         if not CUDA_HOME:
             raise AssertionError(
                 "[extension] CUDA_HOME is not found. You need to export CUDA_HOME environment variable or install CUDA Toolkit first in order to build/load CUDA extensions"
             )
         check_system_pytorch_cuda_match(CUDA_HOME)
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/layernorm/layernorm_cuda.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/layernorm/layernorm_cuda.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/moe/moe_cuda.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/moe/moe_cuda.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/optimizer/fused_optimizer_cuda.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/optimizer/fused_optimizer_cuda.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/softmax/scaled_masked_softmax_cuda.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/scaled_masked_softmax_cuda.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/triton_extension.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/triton_extension.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/extensions/utils.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/jit/bias_dropout_add.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/jit/bias_dropout_add.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/jit/bias_gelu.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/jit/bias_gelu.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/jit/option.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/jit/option.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/kernel_loader.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/kernel_loader.py`

 * *Files 10% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 from typing import List
 
 from .extensions import (
     CpuAdamArmExtension,
     CpuAdamX86Extension,
     FlashAttentionDaoCudaExtension,
     FlashAttentionNpuExtension,
-    FlashAttentionXformersCudaExtension,
+    FlashAttentionSdpaCudaExtension,
     FusedOptimizerCudaExtension,
     LayerNormCudaExtension,
     MoeCudaExtension,
     ScaledMaskedSoftmaxCudaExtension,
     ScaledUpperTriangleMaskedSoftmaxCudaExtension,
 )
 from .extensions.base_extension import _Extension
@@ -61,17 +61,17 @@
         # look for exts which can be built/loaded on the current machine
 
         if ext_name:
             usable_exts = list(filter(lambda ext: ext.name == ext_name, exts))
         else:
             usable_exts = []
             for ext in exts:
-                if ext.is_hardware_available():
+                if ext.is_available():
                     # make sure the machine is compatible during kernel loading
-                    ext.assert_hardware_compatible()
+                    ext.assert_compatible()
                     usable_exts.append(ext)
 
         assert len(usable_exts) != 0, f"No usable kernel found for {self.__class__.__name__} on the current machine."
 
         if len(usable_exts) > 1:
             # if more than one usable kernel is found, we will try to load the kernel with the highest priority
             usable_exts = sorted(usable_exts, key=lambda ext: ext.priority, reverse=True)
@@ -102,8 +102,20 @@
 
 
 class ScaledUpperTriangleMaskedSoftmaxLoader(KernelLoader):
     REGISTRY = [ScaledUpperTriangleMaskedSoftmaxCudaExtension]
 
 
 class FlashAttentionLoader(KernelLoader):
-    REGISTRY = [FlashAttentionNpuExtension, FlashAttentionDaoCudaExtension, FlashAttentionXformersCudaExtension]
+    REGISTRY = [
+        FlashAttentionNpuExtension,
+        FlashAttentionDaoCudaExtension,
+        FlashAttentionSdpaCudaExtension,
+    ]
+
+
+class FlashAttentionWithCustomMaskLoader(KernelLoader):
+    REGISTRY = [FlashAttentionNpuExtension, FlashAttentionSdpaCudaExtension]
+
+
+class FlashAttentionForFloatAndCustomMaskLoader(KernelLoader):
+    REGISTRY = [FlashAttentionSdpaCudaExtension]
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/triton/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/triton/context_attention.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/context_attention.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,25 +1,27 @@
 import math
 
 import torch
 
 try:
     import triton
     import triton.language as tl
+
     HAS_TRITON = True
 except ImportError:
     HAS_TRITON = False
     print("please install triton from https://github.com/openai/triton")
 
 if HAS_TRITON:
     """
     this function is modified from
     https://github.com/ModelTC/lightllm/blob/f093edc20683ac3ea1bca3fb5d8320a0dd36cf7b/lightllm/models/llama/triton_kernel/context_flashattention_nopad.py#L10
     """
     if triton.__version__ < "2.1.0":
+
         @triton.jit
         def _context_flash_attention_kernel(
             Q,
             K,
             V,
             sm_scale,
             B_Start_Loc,
@@ -127,50 +129,72 @@
                 p = p.to(v.dtype)
                 acc += tl.dot(p, v)
                 # update m_i and l_i
                 l_i = l_i_new
                 m_i = m_i_new
 
             off_o = (
-                (cur_batch_start_index + offs_m[:, None]) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od
+                (cur_batch_start_index + offs_m[:, None]) * stride_obs
+                + cur_head * stride_oh
+                + offs_d[None, :] * stride_od
             )
             out_ptrs = Out + off_o
             tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)
             return
+
     else:
         # this function is modified from https://github.com/ModelTC/lightllm/blob/main/lightllm/models/llama/triton_kernel/context_flashattention_nopad.py#L11
         @triton.jit
         def _context_flash_attention_kernel_2(
-            Q, K, V, sm_scale, Alibi, B_Start_Loc, B_Seqlen,
-            Out, 
-            kv_group_num, 
-            stride_qbs, stride_qh, stride_qd,
-            stride_kbs, stride_kh, stride_kd,
-            stride_vbs, stride_vh, stride_vd,
-            stride_obs, stride_oh, stride_od,
-            BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,
+            Q,
+            K,
+            V,
+            sm_scale,
+            Alibi,
+            B_Start_Loc,
+            B_Seqlen,
+            Out,
+            kv_group_num,
+            stride_qbs,
+            stride_qh,
+            stride_qd,
+            stride_kbs,
+            stride_kh,
+            stride_kd,
+            stride_vbs,
+            stride_vh,
+            stride_vd,
+            stride_obs,
+            stride_oh,
+            stride_od,
+            BLOCK_M: tl.constexpr,
+            BLOCK_DMODEL: tl.constexpr,
             BLOCK_N: tl.constexpr,
         ):
             cur_batch = tl.program_id(0)
             cur_head = tl.program_id(1)
             start_m = tl.program_id(2)
-            
+
             if kv_group_num is not None:
                 cur_kv_head = cur_head // kv_group_num
 
             cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)
             cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)
 
             block_start_loc = BLOCK_M * start_m
 
             # initialize offsets
             offs_n = tl.arange(0, BLOCK_N)
             offs_d = tl.arange(0, BLOCK_DMODEL)
             offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)
-            off_q = (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs + cur_head * stride_qh + offs_d[None, :] * stride_qd
+            off_q = (
+                (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs
+                + cur_head * stride_qh
+                + offs_d[None, :] * stride_qd
+            )
             if kv_group_num is None or kv_group_num == 1:
                 off_k = offs_n[None, :] * stride_kbs + cur_head * stride_kh + offs_d[:, None] * stride_kd
                 off_v = offs_n[:, None] * stride_vbs + cur_head * stride_vh + offs_d[None, :] * stride_vd
             else:
                 off_k = offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh + offs_d[:, None] * stride_kd
                 off_v = offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh + offs_d[None, :] * stride_vd
 
@@ -187,16 +211,19 @@
                 alibi_m = tl.load(Alibi + cur_head)
 
             block_mask = tl.where(block_start_loc < cur_batch_seq_len, 1, 0)
 
             for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):
                 start_n = tl.multiple_of(start_n, BLOCK_N)
                 # -- compute qk ----
-                k = tl.load(k_ptrs + (cur_batch_in_all_start_index + start_n) * stride_kbs,
-                            mask=(start_n + offs_n[None, :]) < cur_batch_seq_len, other=0.0)
+                k = tl.load(
+                    k_ptrs + (cur_batch_in_all_start_index + start_n) * stride_kbs,
+                    mask=(start_n + offs_n[None, :]) < cur_batch_seq_len,
+                    other=0.0,
+                )
 
                 qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)
                 qk += tl.dot(q, k)
                 qk *= sm_scale
 
                 if Alibi is not None:
                     alibi_loc = offs_m[:, None] - (start_n + offs_n[None, :])
@@ -216,24 +243,31 @@
                 # scale p
                 p_scale = beta / l_i_new
                 p = p * p_scale[:, None]
                 # scale acc
                 acc_scale = l_i / l_i_new * alpha
                 acc = acc * acc_scale[:, None]
                 # update acc
-                v = tl.load(v_ptrs + (cur_batch_in_all_start_index + start_n) * stride_vbs,
-                            mask=(start_n + offs_n[:, None]) < cur_batch_seq_len, other=0.0)
+                v = tl.load(
+                    v_ptrs + (cur_batch_in_all_start_index + start_n) * stride_vbs,
+                    mask=(start_n + offs_n[:, None]) < cur_batch_seq_len,
+                    other=0.0,
+                )
 
                 p = p.to(v.dtype)
                 acc += tl.dot(p, v)
                 # update m_i and l_i
                 l_i = l_i_new
                 m_i = m_i_new
             # initialize pointers to output
-            off_o = (cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs + cur_head * stride_oh + offs_d[None, :] * stride_od
+            off_o = (
+                (cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs
+                + cur_head * stride_oh
+                + offs_d[None, :] * stride_od
+            )
             out_ptrs = Out + off_o
             tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_batch_seq_len)
             return
 
     @torch.no_grad()
     def bloom_context_attn_fwd(q, k, v, o, b_start_loc, b_seq_len, max_input_len, alibi=None):
         BLOCK = 128
@@ -245,15 +279,15 @@
 
         sm_scale = 1.0 / math.sqrt(Lk)
         batch, head = b_seq_len.shape[0], q.shape[1]
 
         grid = (batch, head, triton.cdiv(max_input_len, BLOCK))
 
         num_warps = 4 if Lk <= 64 else 8
-        
+
         if triton.__version__ < "2.1.0":
             tmp = torch.empty((batch, head, max_input_len + 256), device=q.device, dtype=torch.float32)
             _context_flash_attention_kernel[grid](
                 q,
                 k,
                 v,
                 sm_scale,
@@ -282,36 +316,42 @@
                 BLOCK_DMODEL=Lk,
                 BLOCK_N=BLOCK,
                 num_warps=num_warps,
                 num_stages=1,
             )
         else:
             _context_flash_attention_kernel_2[grid](
-                q, k, v, sm_scale, alibi, b_start_loc, b_seq_len,
+                q,
+                k,
+                v,
+                sm_scale,
+                alibi,
+                b_start_loc,
+                b_seq_len,
                 o,
                 None,
-                q.stride(0), 
-                q.stride(1), 
+                q.stride(0),
+                q.stride(1),
                 q.stride(2),
-                k.stride(0), 
-                k.stride(1), 
+                k.stride(0),
+                k.stride(1),
                 k.stride(2),
-                v.stride(0), 
-                v.stride(1), 
+                v.stride(0),
+                v.stride(1),
                 v.stride(2),
-                o.stride(0), 
-                o.stride(1), 
+                o.stride(0),
+                o.stride(1),
                 o.stride(2),
                 BLOCK_M=BLOCK,
                 BLOCK_DMODEL=Lk,
                 BLOCK_N=BLOCK,
                 num_warps=num_warps,
                 num_stages=1,
             )
-            
+
         return
 
     @torch.no_grad()
     def llama_context_attn_fwd(q, k, v, o, b_start_loc, b_seq_len, max_input_len):
         BLOCK = 128
         # shape constraints
         Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]
@@ -323,25 +363,25 @@
         batch, head = b_seq_len.shape[0], q.shape[1]
 
         grid = (batch, head, triton.cdiv(max_input_len, BLOCK))
 
         tmp = torch.empty((batch, head, max_input_len + 256), device=q.device, dtype=torch.float32)
         num_warps = 4 if Lk <= 64 else 8
         # num_warps = 4
-        
+
         if triton.__version__ < "2.1.0":
             _context_flash_attention_kernel[grid](
                 q,
                 k,
                 v,
                 sm_scale,
                 b_start_loc,
                 b_seq_len,
                 tmp,
-                None, 
+                None,
                 o,
                 q.stride(0),
                 q.stride(1),
                 q.stride(2),
                 k.stride(0),
                 k.stride(1),
                 k.stride(2),
@@ -358,36 +398,37 @@
                 BLOCK_DMODEL=Lk,
                 BLOCK_N=BLOCK,
                 num_warps=num_warps,
                 num_stages=1,
             )
         else:
             kv_group_num = q.shape[1] // k.shape[1]
-            _context_flash_attention_kernel_2[grid](                
-                q, 
-                k, 
-                v, 
-                sm_scale, 
+            _context_flash_attention_kernel_2[grid](
+                q,
+                k,
+                v,
+                sm_scale,
                 None,
-                b_start_loc, 
+                b_start_loc,
                 b_seq_len,
                 o,
                 kv_group_num,
-                q.stride(0), 
-                q.stride(1), 
+                q.stride(0),
+                q.stride(1),
                 q.stride(2),
-                k.stride(0), 
-                k.stride(1), 
+                k.stride(0),
+                k.stride(1),
                 k.stride(2),
-                v.stride(0), 
-                v.stride(1), 
+                v.stride(0),
+                v.stride(1),
                 v.stride(2),
-                o.stride(0), 
-                o.stride(1), 
+                o.stride(0),
+                o.stride(1),
                 o.stride(2),
                 BLOCK_M=BLOCK,
                 BLOCK_DMODEL=Lk,
                 BLOCK_N=BLOCK,
                 num_warps=num_warps,
-                num_stages=1,)
-            
-        return
+                num_stages=1,
+            )
+
+        return
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/triton/copy_kv_cache_dest.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/copy_kv_cache_dest.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/triton/custom_autotune.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/custom_autotune.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/triton/flash_decoding.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/flash_decoding.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,50 +1,47 @@
 # adepted from https://github.com/ModelTC/lightllm/blob/ece7b43f8a6dfa74027adc77c2c176cff28c76c8/lightllm/models/llama/triton_kernel/flash_decoding.py
 import torch
+
 try:
     from lightllm.models.llama.triton_kernel.flash_decoding_stage1 import flash_decode_stage1
     from lightllm.models.llama.triton_kernel.flash_decoding_stage2 import flash_decode_stage2
+
     HAS_LIGHTLLM_KERNEL = True
 except:
     print("install lightllm from https://github.com/ModelTC/lightllm/blob/ece7b43f8a6dfa74027adc77c2c176cff28c76c8")
     HAS_LIGHTLLM_KERNEL = False
 
 
 if HAS_LIGHTLLM_KERNEL:
+
     def token_flash_decoding(q, o_tensor, infer_state, q_head_num, head_dim, cache_k, cache_v):
         BLOCK_SEQ = 256
         batch_size = infer_state.batch_size
         max_len_in_batch = infer_state.max_len_in_batch
 
-
         calcu_shape1 = (batch_size, q_head_num, head_dim)
 
-        if getattr(infer_state, 'mid_o', None) is None:
-            infer_state.mid_o = torch.empty([batch_size, 
-                                            q_head_num, 
-                                            max_len_in_batch // BLOCK_SEQ + 1, 
-                                            head_dim], 
-                                            dtype=torch.float32, 
-                                            device="cuda")
-            infer_state.mid_o_logexpsum = torch.empty([batch_size, 
-                                            q_head_num,
-                                            max_len_in_batch // BLOCK_SEQ + 1], 
-                                            dtype=torch.float32, 
-                                            device="cuda")
+        if getattr(infer_state, "mid_o", None) is None:
+            infer_state.mid_o = torch.empty(
+                [batch_size, q_head_num, max_len_in_batch // BLOCK_SEQ + 1, head_dim],
+                dtype=torch.float32,
+                device="cuda",
+            )
+            infer_state.mid_o_logexpsum = torch.empty(
+                [batch_size, q_head_num, max_len_in_batch // BLOCK_SEQ + 1], dtype=torch.float32, device="cuda"
+            )
 
         mid_o = infer_state.mid_o
         mid_o_logexpsum = infer_state.mid_o_logexpsum
 
-        flash_decode_stage1(q.view(calcu_shape1),
-                                    cache_k,
-                                    cache_v,
-                                    infer_state.block_loc,
-                                    infer_state.seq_len,
-                                    infer_state.max_len_in_batch,
-                                    mid_o,
-                                    mid_o_logexpsum,
-                                    BLOCK_SEQ)
-        flash_decode_stage2(mid_o,
-                            mid_o_logexpsum, 
-                            infer_state.seq_len, 
-                            o_tensor.view(calcu_shape1), 
-                            BLOCK_SEQ)
+        flash_decode_stage1(
+            q.view(calcu_shape1),
+            cache_k,
+            cache_v,
+            infer_state.block_loc,
+            infer_state.seq_len,
+            infer_state.max_len_in_batch,
+            mid_o,
+            mid_o_logexpsum,
+            BLOCK_SEQ,
+        )
+        flash_decode_stage2(mid_o, mid_o_logexpsum, infer_state.seq_len, o_tensor.view(calcu_shape1), BLOCK_SEQ)
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/triton/fused_layernorm.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/fused_layernorm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/triton/gptq_triton.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/gptq_triton.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/triton/int8_rotary_embedding_kernel.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/int8_rotary_embedding_kernel.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/triton/llama_act_combine_kernel.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/llama_act_combine_kernel.py`

 * *Files 3% similar despite different names*

```diff
@@ -4,14 +4,15 @@
 import torch
 from torch import Tensor
 from torch.cuda.amp import custom_bwd, custom_fwd
 
 try:
     import triton
     import triton.language as tl
+
     HAS_TRITON = True
 except ImportError:
     HAS_TRITON = False
     print("please install triton from https://github.com/openai/triton")
 
 if HAS_TRITON:
     PRECISION_MAP = {
@@ -22,48 +23,48 @@
 
     @triton.jit
     def _llama_act_combine_forward(
         X_GATE1,
         X_GATE2,
         X_UP,
         Y,
-        stride,    # how much to increase the pointer when moving by 1 row
-        N,    # number of columns in X
+        stride,  # how much to increase the pointer when moving by 1 row
+        N,  # number of columns in X
         BLOCK_SIZE: tl.constexpr,
     ):
         # Map the program id to the row of X and Y it should compute.
         row = tl.program_id(0)
         X_GATE1 += row * stride
         X_GATE2 += row * stride
         X_UP += row * stride
         Y += row * stride
 
         # do activation and combine, and store in y
         for off in range(0, N, BLOCK_SIZE):
             cols = off + tl.arange(0, BLOCK_SIZE)
             mask = cols < N
-            x_gate1 = tl.load(X_GATE1 + cols, mask=mask, other=0.)
-            x_gate2 = tl.load(X_GATE2 + cols, mask=mask, other=0.)
-            x_up = tl.load(X_UP + cols, mask=mask, other=0.)
+            x_gate1 = tl.load(X_GATE1 + cols, mask=mask, other=0.0)
+            x_gate2 = tl.load(X_GATE2 + cols, mask=mask, other=0.0)
+            x_up = tl.load(X_UP + cols, mask=mask, other=0.0)
             x_gate2_sigmoid = tl.sigmoid(x_gate2.to(tl.float32)).to(x_gate2.dtype)
             y = x_gate1 * x_gate2 * x_gate2_sigmoid * x_up
             # Write output
             tl.store(Y + cols, y, mask=mask)
 
     @triton.jit
     def _llama_act_combine_backward(
         X_GATE1,
         X_GATE2,
         X_UP,
         X_GATE1_GRAD,
         X_GATE2_GRAD,
         X_UP_GRAD,
         Y_GRAD,
-        stride,    # how much to increase the pointer when moving by 1 row
-        N,    # number of columns in X
+        stride,  # how much to increase the pointer when moving by 1 row
+        N,  # number of columns in X
         BLOCK_SIZE: tl.constexpr,
     ):
         # Map the program id to the row of X and Y it should compute.
         row = tl.program_id(0)
         X_GATE1 += row * stride
         X_GATE2 += row * stride
         X_UP += row * stride
@@ -72,18 +73,18 @@
         X_UP_GRAD += row * stride
         Y_GRAD += row * stride
 
         # do activation and combine, and store in y
         for off in range(0, N, BLOCK_SIZE):
             cols = off + tl.arange(0, BLOCK_SIZE)
             mask = cols < N
-            x_gate1 = tl.load(X_GATE1 + cols, mask=mask, other=0.)
-            x_gate2 = tl.load(X_GATE2 + cols, mask=mask, other=0.)
-            x_up = tl.load(X_UP + cols, mask=mask, other=0.)
-            y_grad = tl.load(Y_GRAD + cols, mask=mask, other=0.)
+            x_gate1 = tl.load(X_GATE1 + cols, mask=mask, other=0.0)
+            x_gate2 = tl.load(X_GATE2 + cols, mask=mask, other=0.0)
+            x_up = tl.load(X_UP + cols, mask=mask, other=0.0)
+            y_grad = tl.load(Y_GRAD + cols, mask=mask, other=0.0)
 
             # forward: y = x_gate1 * x_gate2 * tl.sigmoid(x_gate2) * x_up
             x_gate2_sigmoid = tl.sigmoid(x_gate2.to(tl.float32)).to(x_gate2.dtype)
             x_gate2_act = y_grad * x_gate2 * x_gate2_sigmoid
             x_up_grad = x_gate2_act * x_gate1
             x_gate1_grad = x_gate2_act * x_up
             # grad(x*sigmoid(x)) = sigmoid(x) + x * sigmoid(x) * [1  sigmoid(x)]
@@ -143,43 +144,43 @@
             if N > BLOCK_SIZE:
                 raise RuntimeError("This layer norm doesn't support feature dim >= 64KB.")
             # heuristics for number of warps
             num_warps = min(max(BLOCK_SIZE // 256, 1), 8)
             # restore setting
             ctx.M, ctx.N, ctx.BLOCK_SIZE, ctx.num_warps = M, N, BLOCK_SIZE, num_warps
             # enqueue kernel
-            _llama_act_combine_forward[(M,)](x_gate1,
-                                             x_gate2,
-                                             x_up,
-                                             y,
-                                             x_up.stride(-2),
-                                             N,
-                                             BLOCK_SIZE=BLOCK_SIZE,
-                                             num_warps=num_warps)
+            _llama_act_combine_forward[(M,)](
+                x_gate1, x_gate2, x_up, y, x_up.stride(-2), N, BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps
+            )
             return y
 
         @staticmethod
         @custom_bwd
         def backward(ctx: Any, *grad_outputs: Tensor) -> Tuple[Tensor, Tensor, None, None]:
             # restore from ctx
             (x_gate1, x_gate2, x_up) = ctx.saved_tensors
             M, N, BLOCK_SIZE, num_warps = ctx.M, ctx.N, ctx.BLOCK_SIZE, ctx.num_warps
 
             # init grad
             y_grad = grad_outputs[0]
-            x_gate1_grad, x_gate2_grad, x_up_grad = torch.empty_like(x_gate1), torch.empty_like(
-                x_gate2), torch.empty_like(x_up)
+            x_gate1_grad, x_gate2_grad, x_up_grad = (
+                torch.empty_like(x_gate1),
+                torch.empty_like(x_gate2),
+                torch.empty_like(x_up),
+            )
 
             # enqueue kernel
-            _llama_act_combine_backward[(M,)](x_gate1,
-                                              x_gate2,
-                                              x_up,
-                                              x_gate1_grad,
-                                              x_gate2_grad,
-                                              x_up_grad,
-                                              y_grad,
-                                              x_up.stride(-2),
-                                              N,
-                                              BLOCK_SIZE=BLOCK_SIZE,
-                                              num_warps=num_warps)
+            _llama_act_combine_backward[(M,)](
+                x_gate1,
+                x_gate2,
+                x_up,
+                x_gate1_grad,
+                x_gate2_grad,
+                x_up_grad,
+                y_grad,
+                x_up.stride(-2),
+                N,
+                BLOCK_SIZE=BLOCK_SIZE,
+                num_warps=num_warps,
+            )
             x_gate_grad = torch.cat([x_gate1_grad, x_gate2_grad], dim=-1)
             return x_gate_grad, x_up_grad, None, None
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/triton/qkv_matmul_kernel.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/qkv_matmul_kernel.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/triton/self_attention_nofusion.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/self_attention_nofusion.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/triton/smooth_attention.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/smooth_attention.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/triton/softmax.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/softmax.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/kernel/triton/token_attention_kernel.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/triton/token_attention_kernel.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,18 +9,26 @@
 
     HAS_TRITON = True
 except ImportError:
     HAS_TRITON = False
     print("please install triton from https://github.com/openai/triton")
 
 try:
-    from lightllm.models.llama.triton_kernel.token_attention_nopad_reduceV import token_att_fwd2 as lightllm_llama_token_att_fwd2
-    from lightllm.models.llama.triton_kernel.token_attention_nopad_att1 import token_att_fwd as lightllm_llama_token_att_fwd
-    from lightllm.models.llama.triton_kernel.token_attention_nopad_softmax import token_softmax_fwd as lightllm_llama_token_softmax_fwd
-    from lightllm.models.bloom.triton_kernel.token_attention_nopad_att1 import token_att_fwd as lightllm_bloom_token_att_fwd
+    from lightllm.models.bloom.triton_kernel.token_attention_nopad_att1 import (
+        token_att_fwd as lightllm_bloom_token_att_fwd,
+    )
+    from lightllm.models.llama.triton_kernel.token_attention_nopad_att1 import (
+        token_att_fwd as lightllm_llama_token_att_fwd,
+    )
+    from lightllm.models.llama.triton_kernel.token_attention_nopad_reduceV import (
+        token_att_fwd2 as lightllm_llama_token_att_fwd2,
+    )
+    from lightllm.models.llama.triton_kernel.token_attention_nopad_softmax import (
+        token_softmax_fwd as lightllm_llama_token_softmax_fwd,
+    )
 
     HAS_TRITON_TOKEN_ATTENTION = True
 except ImportError:
     print("unable to import lightllm kernels")
     HAS_TRITON_TOKEN_ATTENTION = False
 
 if HAS_TRITON:
@@ -201,17 +209,15 @@
             kv_cache_start_loc,
             kv_cache_seq_len,
             max_len_in_batch,
         )
 
         if triton.__version__ == "2.0.0":
             prob = torch.empty_like(att_m_tensor)
-            lightllm_llama_token_softmax_fwd(
-                att_m_tensor, kv_cache_start_loc, kv_cache_seq_len, prob, max_len_in_batch
-            )
+            lightllm_llama_token_softmax_fwd(att_m_tensor, kv_cache_start_loc, kv_cache_seq_len, prob, max_len_in_batch)
             att_m_tensor = None
 
             lightllm_llama_token_att_fwd2(
                 prob,
                 v,
                 attn_out.view(calcu_shape1),
                 kv_cache_loc,
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/lazy/construction.py` & `colossalai-nightly-2024.5.4/colossalai/lazy/construction.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/lazy/lazy_init.py` & `colossalai-nightly-2024.5.4/colossalai/lazy/lazy_init.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/lazy/pretrained.py` & `colossalai-nightly-2024.5.4/colossalai/lazy/pretrained.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/amp/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/amp/apex_amp/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/apex_amp/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/amp/apex_amp/apex_amp.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/apex_amp/apex_amp.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/amp/naive_amp/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/amp/naive_amp/_fp16_optimizer.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/_fp16_optimizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/amp/naive_amp/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/amp/naive_amp/naive_amp.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/naive_amp/naive_amp.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/amp/torch_amp/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/amp/torch_amp/_grad_scaler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/_grad_scaler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/amp/torch_amp/torch_amp.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/amp/torch_amp/torch_amp.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/builder/builder.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/builder/builder.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/communication/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/communication/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/communication/collective.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/communication/collective.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/communication/p2p.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/communication/p2p.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/communication/p2p_v2.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/communication/p2p_v2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/communication/ring.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/communication/ring.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/communication/utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/communication/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/constants.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/constants.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/context/parallel_context.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/parallel_context.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/context/parallel_mode.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/parallel_mode.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_1d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_1d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_2d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_2d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_2p5d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_2p5d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_3d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_3d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_data.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_data.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_model.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_model.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_pipeline.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_pipeline.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_sequence.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_sequence.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/initializer_tensor.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/initializer_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/context/process_group_initializer/process_group_initializer.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/process_group_initializer/process_group_initializer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/context/random/_helper.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/random/_helper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/context/random/seed_manager.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/context/random/seed_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/engine/_base_engine.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/_base_engine.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_accumulation/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_accumulation/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_accumulation/_gradient_accumulation.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_accumulation/_gradient_accumulation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/_base_gradient_handler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_base_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/_data_parallel_gradient_handler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_data_parallel_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/_moe_gradient_handler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_moe_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/_pipeline_parallel_gradient_handler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_pipeline_parallel_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/_sequence_parallel_gradient_handler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_sequence_parallel_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/_zero_gradient_handler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/_zero_gradient_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/engine/gradient_handler/utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/gradient_handler/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/engine/schedule/_base_schedule.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_base_schedule.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/engine/schedule/_non_pipeline_schedule.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_non_pipeline_schedule.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/engine/schedule/_pipeline_schedule.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_pipeline_schedule.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/engine/schedule/_pipeline_schedule_v2.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/engine/schedule/_pipeline_schedule_v2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/global_variables.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/global_variables.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/async_engine.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/async_engine.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/async_manager.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/async_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/get_tokenizer.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/get_tokenizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/infer_batch.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/infer_batch.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/io_struct.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/io_struct.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/ray_dist_init.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/ray_dist_init.py`

 * *Files 1% similar despite different names*

```diff
@@ -52,15 +52,15 @@
         self.max_output_len = max_output_len
         self.router_config = router_config
 
     def setup(self, world_size, rank, port):
         # initialize a ray collective group, otherwise colossalai distributed env won't be built successfully
         collective.init_collective_group(world_size, rank, "nccl", "default")
         # initialize and set distributed environment
-        colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+        colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
         ray_serve_logger.info(f"Worker with rank {rank} (world size {world_size}) setting up..")
         log_cuda_info("Worker.setup")
 
         # Load model
         self.tokenizer = get_tokenizer(tokenizer_name=self.model_path)
         if self.tokenizer.pad_token is None:
             self.tokenizer.pad_token = self.tokenizer.eos_token
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/ray_init_config.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/ray_init_config.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/req_queue.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/req_queue.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/sampling_params.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/sampling_params.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/dynamic_batching/stats.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/dynamic_batching/stats.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/hybridengine/engine.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/engine.py`

 * *Files 2% similar despite different names*

```diff
@@ -38,19 +38,19 @@
 
     ```python
     from colossalai.inference import InferEngine
     from colossalai.inference.pipeline.policies import LlamaModelInferPolicy
     import colossalai
     from transformers import LlamaForCausalLM, LlamaTokenizer
 
-    colossalai.launch_from_torch(config={})
+    colossalai.launch_from_torch()
 
     model = LlamaForCausalLM.from_pretrained("your_path_to_model")
     tokenizer = LlamaTokenizer.from_pretrained("/home/lczyh/share/models/llama-7b-hf")
-    # assume the model is infered with 2 pipeline stages
+    # assume the model is inferred with 2 pipeline stages
     inferengine = CaiInferEngine(pp_size=2, model=model, model_policy=LlamaModelInferPolicy())
 
     input = ["Introduce a landmark in China ","Introduce a landmark in China "]
     data = tokenizer(input, return_tensors='pt')
     output = inferengine.inference([data.to('cuda').data])
 
     ```
@@ -66,15 +66,15 @@
         model_policy: Policy = None,
         micro_batch_size: int = 1,
         micro_batch_buffer_size: int = None,
         max_batch_size: int = 4,
         max_input_len: int = 32,
         max_output_len: int = 32,
         verbose: bool = False,
-        # TODO: implement early_stopping, and various gerneration options
+        # TODO: implement early_stopping, and various generation options
         early_stopping: bool = False,
         do_sample: bool = False,
         num_beams: int = 1,
     ) -> None:
         assert model.__class__.__name__ in _supported_models, f"Model {model.__class__.__name__} is not supported."
         assert (
             tp_size * pp_size == dist.get_world_size()
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/hybridengine/modeling/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/modeling/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/hybridengine/modeling/llama.py` & `colossalai-nightly-2024.5.4/colossalai/inference/engine/modeling/llama.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 import math
 from typing import List, Optional, Tuple
 
 import torch
 from transformers.models.llama.modeling_llama import LlamaAttention, LlamaDecoderLayer, LlamaForCausalLM, LlamaModel
 from transformers.utils import logging
 
-from colossalai.inference.tensor_parallel.batch_infer_state import BatchInferState
+from colossalai.inference.kv_cache.batch_infer_state import BatchInferState
 from colossalai.kernel.triton import llama_context_attn_fwd, token_attention_fwd
 from colossalai.kernel.triton.token_attention_kernel import Llama2TokenAttentionForwards
 from colossalai.pipeline.stage_manager import PipelineStageManager
 
 from ._utils import copy_kv_to_mem_cache
 
 try:
@@ -24,14 +24,24 @@
 
     HAS_LIGHTLLM_KERNEL = True
 except:
     print("please install lightllm from source to run inference: https://github.com/ModelTC/lightllm")
     HAS_LIGHTLLM_KERNEL = False
 
 try:
+    from colossalai.kernel.triton.flash_decoding import token_flash_decoding
+
+    HAS_TRITON_FLASH_DECODING_KERNEL = True
+except:
+    print(
+        "no triton flash decoding support, please install lightllm from https://github.com/ModelTC/lightllm/blob/ece7b43f8a6dfa74027adc77c2c176cff28c76c8"
+    )
+    HAS_TRITON_FLASH_DECODING_KERNEL = False
+
+try:
     from flash_attn import flash_attn_with_kvcache
 
     HAS_FLASH_KERNEL = True
 except:
     HAS_FLASH_KERNEL = False
     print("please install flash attentiom from https://github.com/Dao-AILab/flash-attention")
 
@@ -63,66 +73,74 @@
             llama_context_attn_fwd(
                 query_states,
                 key_states,
                 value_states,
                 attn_output,
                 infer_state.start_loc,
                 infer_state.seq_len,
-                # infer_state.cache_manager.past_key_values_length,
                 infer_state.max_len_in_batch,
             )
         else:
             lightllm_context_attention_fwd(
                 query_states,
                 key_states,
                 value_states,
                 attn_output,
                 infer_state.start_loc,
                 infer_state.seq_len,
-                # infer_state.cache_manager.past_key_values_length,
                 infer_state.max_len_in_batch,
             )
     else:
         assert HAS_LIGHTLLM_KERNEL is True, "You have to install lightllm kernels to run llama2 model"
         lightllm_llama2_context_attention_fwd(
             query_states,
             key_states,
             value_states,
             attn_output,
             infer_state.start_loc,
             infer_state.seq_len,
-            # infer_state.cache_manager.past_key_values_length,
             infer_state.max_len_in_batch,
         )
 
 
-def llama_triton_token_attention(query_states, attn_output, infer_state, num_key_value_groups=1):
-    assert HAS_LIGHTLLM_KERNEL is True, "You have to install lightllm kernel to run token attention for llama models"
+def llama_triton_token_attention(
+    query_states, attn_output, infer_state, num_key_value_groups=1, q_head_num=-1, head_dim=-1
+):
+    if HAS_TRITON_FLASH_DECODING_KERNEL and q_head_num != -1 and head_dim != -1:
+        token_flash_decoding(
+            q=query_states,
+            o_tensor=attn_output,
+            infer_state=infer_state,
+            q_head_num=q_head_num,
+            head_dim=head_dim,
+            cache_k=infer_state.cache_manager.key_buffer[infer_state.decode_layer_id],
+            cache_v=infer_state.cache_manager.value_buffer[infer_state.decode_layer_id],
+        )
+        return
+
     if num_key_value_groups == 1:
         token_attention_fwd(
             query_states,
             infer_state.cache_manager.key_buffer[infer_state.decode_layer_id],
             infer_state.cache_manager.value_buffer[infer_state.decode_layer_id],
             attn_output,
             infer_state.block_loc,
             infer_state.start_loc,
             infer_state.seq_len,
-            # infer_state.cache_manager.past_key_values_length,
             infer_state.max_len_in_batch,
         )
     else:
         Llama2TokenAttentionForwards.token_attn(
             query_states,
             infer_state.cache_manager.key_buffer[infer_state.decode_layer_id],
             infer_state.cache_manager.value_buffer[infer_state.decode_layer_id],
             attn_output,
             infer_state.block_loc,
             infer_state.start_loc,
             infer_state.seq_len,
-            # infer_state.cache_manager.past_key_values_length,
             infer_state.max_len_in_batch,
             infer_state.other_kv_index,
         )
 
 
 class LlamaInferenceForwards:
     """
@@ -163,15 +181,15 @@
         if output_attentions:
             logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
             output_attentions = False
         if output_hidden_states:
             logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
             output_hidden_states = False
 
-        # If is first stage and after warmup, go throught lm_head first
+        # If is first stage and hidden_states is None, go throught lm_head first
         if stage_manager.is_first_stage() and hidden_states is not None:
             lm_logits = self.lm_head(hidden_states)
             return {"logits": lm_logits}
 
         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
         outputs = LlamaInferenceForwards.llama_model_forward(
             self.model,
@@ -323,23 +341,14 @@
 
         # update indices
         # infer_state.block_loc[:, infer_state.max_len_in_batch-1] = infer_state.total_token_num + torch.arange(0, batch_size, dtype=torch.int32, device="cuda")
         infer_state.start_loc += torch.arange(0, batch_size, dtype=torch.int32, device="cuda")
         infer_state.seq_len += 1
         infer_state.max_len_in_batch += 1
 
-        # if not return_dict:
-        #     return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
-
-        # return BaseModelOutputWithPast(
-        #     last_hidden_state=hidden_states,
-        #     past_key_values=next_cache,
-        #     hidden_states=all_hidden_states,
-        #     attentions=all_self_attns,
-        # )
         return {"hidden_states": hidden_states}
 
     @staticmethod
     def llama_decoder_layer_forward(
         self: LlamaDecoderLayer,
         hidden_states: torch.Tensor,
         attention_mask: Optional[torch.Tensor] = None,
@@ -458,15 +467,20 @@
                     infer_state.decode_mem_index,
                     infer_state.cache_manager,
                 )
 
             if HAS_LIGHTLLM_KERNEL:
                 attn_output = torch.empty_like(query_states)
                 llama_triton_token_attention(
-                    query_states, attn_output, infer_state, num_key_value_groups=self.num_key_value_groups
+                    query_states=query_states,
+                    attn_output=attn_output,
+                    infer_state=infer_state,
+                    num_key_value_groups=self.num_key_value_groups,
+                    q_head_num=q_len * self.num_heads,
+                    head_dim=self.head_dim,
                 )
             else:
                 self.num_heads // self.num_key_value_heads
                 cache_k = infer_state.cache_manager.key_buffer[infer_state.decode_layer_id]
                 cache_v = infer_state.cache_manager.value_buffer[infer_state.decode_layer_id]
 
                 query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim)
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/hybridengine/polices/llama.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/hybridengine/polices/llama.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/manager.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/pipeline/microbatch_manager.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/pipeline/microbatch_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/batch_infer_state.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/batch_infer_state.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/engine.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/engine.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/kvcache_manager.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/kvcache_manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/modeling/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/modeling/bloom.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/bloom.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/modeling/chatglm2.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/chatglm2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/modeling/llama.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/modeling/llama.py`

 * *Files 1% similar despite different names*

```diff
@@ -4,15 +4,17 @@
 import torch
 from transformers.modeling_outputs import BaseModelOutputWithPast
 from transformers.models.llama.modeling_llama import LlamaAttention, LlamaDecoderLayer, LlamaModel
 
 from colossalai.inference.tensor_parallel.batch_infer_state import BatchInferState
 from colossalai.kernel.triton import llama_context_attn_fwd, token_attention_fwd
 from colossalai.kernel.triton.token_attention_kernel import Llama2TokenAttentionForwards
+
 from ._utils import copy_kv_to_mem_cache
+
 try:
     from lightllm.models.llama.triton_kernel.context_flashattention_nopad import (
         context_attention_fwd as lightllm_llama_context_attention_fwd,
     )
     from lightllm.models.llama.triton_kernel.rotary_emb import rotary_emb_fwd as llama_rotary_embedding_fwd
 
     HAS_LIGHTLLM_KERNEL = True
@@ -86,15 +88,15 @@
             attn_output,
             infer_state.block_loc,
             infer_state.start_loc,
             infer_state.seq_len,
             # infer_state.cache_manager.past_key_values_length,
             infer_state.max_len_in_batch,
         )
-        
+
     else:
         Llama2TokenAttentionForwards.token_attn(
             query_states,
             infer_state.cache_manager.key_buffer[infer_state.decode_layer_id],
             infer_state.cache_manager.value_buffer[infer_state.decode_layer_id],
             attn_output,
             infer_state.block_loc,
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/policies/bloom.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/bloom.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/policies/chatglm2.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/chatglm2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/inference/tensor_parallel/policies/llama.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/inference/tensor_parallel/policies/llama.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/initialize.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/initialize.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/_ops/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/_ops/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/base_layer.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/base_layer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/colossalai_layer/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/colossalai_layer/dropout.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/dropout.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/colossalai_layer/embedding.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/colossalai_layer/linear.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/colossalai_layer/normalization.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/colossalai_layer/normalization.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_1d/_operation.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_1d/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_1d/layers.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_1d/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_2d/_operation.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_2d/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_2d/layers.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2d/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_2p5d/_operation.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_2p5d/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_2p5d/layers.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_2p5d/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_3d/_operation.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_3d/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_3d/layers.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_3d/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_sequence/_operation.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/parallel_sequence/layers.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/parallel_sequence/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/utils/common.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/utils/common.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/vanilla/layers.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/vanilla/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/layer/wrapper/pipeline_wrapper.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/layer/wrapper/pipeline_wrapper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/loss/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/loss/loss_1d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_1d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/loss/loss_2d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_2d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/loss/loss_2p5d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_2p5d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/loss/loss_3d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/loss/loss_3d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/metric/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/metric/accuracy_2d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_2d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/metric/accuracy_2p5d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_2p5d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/metric/accuracy_3d.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/metric/accuracy_3d.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/data_parallel.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/data_parallel.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/base_embedding.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/base_embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/cache_mgr.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/cache_mgr.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/cached_embedding.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/cached_embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/copyer.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/copyer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/embedding_config.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/embedding_config.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise_split_cache.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/cache_embedding/parallel_cached_embedding_tablewise_split_cache.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/colo_module.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/colo_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/embedding.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/embedding.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/linear.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/layers/module_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/layers/module_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/nn/parallel/reducer.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/nn/parallel/reducer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/layer_spec.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/layer_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/middleware/adaptor/fx.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/adaptor/fx.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/middleware/topo.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/middleware/topo.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/pipelinable.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/pipelinable.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/pipeline_process_group.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/pipeline_process_group.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/rpc/_pipeline_base.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/_pipeline_base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/rpc/_pipeline_schedule.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/_pipeline_schedule.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/rpc/utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/rpc/utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -110,15 +110,15 @@
     dp_degree = args.dp_degree
     tp_degree = args.tp_degree
     num_worker_threads = args.num_worker_threads
     host = args.master_addr
     port = args.master_port
     backend = "nccl" if device == "cuda" else "gloo"
 
-    launch(dict(), rank, world_size, host, int(port), backend, verbose=False)
+    launch(rank, world_size, host, int(port), backend, verbose=False)
     ppg.set_global_info(
         rank=rank,
         world_size=world_size,
         dp_degree=dp_degree,
         tp_degree=tp_degree,
         num_worker_threads=num_worker_threads,
         device=device,
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/pipeline/utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/pipeline/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/registry/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/registry/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/registry/registry.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/registry/registry.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/tensor/compute_spec.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/compute_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/tensor/dist_spec_mgr.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/dist_spec_mgr.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/tensor/distspec.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/distspec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/tensor/op_wrapper.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/op_wrapper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/tensor/process_group.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/process_group.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/tensor/tensor_spec.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/tensor/tensor_spec.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from dataclasses import dataclass
+from dataclasses import dataclass, field
 from typing import Optional
 
 from colossalai.legacy.tensor.distspec import DistPlacementPattern, _DistSpec
 from colossalai.legacy.tensor.process_group import ProcessGroup
 
 from .compute_spec import ComputeSpec
 
@@ -13,9 +13,9 @@
 
     A data class for specifications of the `ColoTensor`.
     It contains attributes of `ProcessGroup`, `_DistSpec`, `ComputeSpec`.
     The latter two attributes are optional. If not set, they are default value is `Replicate()` and `None`.
     """
 
     pg: ProcessGroup
-    dist_attr: Optional[_DistSpec] = _DistSpec(DistPlacementPattern.REPLICATE)
+    dist_attr: Optional[_DistSpec] = field(default_factory=lambda: _DistSpec(DistPlacementPattern.REPLICATE))
     compute_attr: Optional[ComputeSpec] = None
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/trainer/_trainer.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/_trainer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/trainer/hooks/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/trainer/hooks/_base_hook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_base_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/trainer/hooks/_checkpoint_hook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_checkpoint_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/trainer/hooks/_log_hook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_log_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/trainer/hooks/_lr_scheduler_hook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_lr_scheduler_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/trainer/hooks/_metric_hook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/trainer/hooks/_metric_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/utils/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/utils/activation_checkpoint.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/activation_checkpoint.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/utils/checkpoint/module_checkpoint.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/module_checkpoint.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/utils/checkpoint/utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpoint/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/utils/checkpointing.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/checkpointing.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/utils/common.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/common.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/utils/data_sampler/data_parallel_sampler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/data_sampler/data_parallel_sampler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/utils/memory.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/memory.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/utils/profiler/legacy/comm_profiler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/comm_profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/utils/profiler/legacy/pcie_profiler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/pcie_profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/utils/profiler/legacy/prof_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/legacy/prof_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/utils/profiler/profiler.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/profiler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/utils/profiler/stateful_tensor_mem_extention.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/utils/profiler/stateful_tensor_mem_extention.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/colo_init_context.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/colo_init_context.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/gemini_context.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/gemini_context.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/ophooks/_shard_grad_ophook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/_shard_grad_ophook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/ophooks/_shard_param_ophook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/_shard_param_ophook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/ophooks/runtime_mem_tracer_hook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/runtime_mem_tracer_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/ophooks/utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/ophooks/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/paramhooks/_param_hookmgr.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/paramhooks/_param_hookmgr.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/stateful_tensor.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/stateful_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/stateful_tensor_mgr.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/stateful_tensor_mgr.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/tensor_placement_policy.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/tensor_placement_policy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/gemini/tensor_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/gemini/tensor_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/init_ctx/init_context.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/init_ctx/init_context.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/shard_utils/base_shard_strategy.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/base_shard_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/shard_utils/bucket_tensor_shard_strategy.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/bucket_tensor_shard_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/shard_utils/commons.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/commons.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/shard_utils/tensor_shard_strategy.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/shard_utils/tensor_shard_strategy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_model/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_model/reduce_scatter.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/reduce_scatter.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_model/sharded_model_v2.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/sharded_model_v2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_model/utils.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_model/zero_hook.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_model/zero_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_optim/sharded_optim_v2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_param/sharded_param.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/sharded_param.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/legacy/zero/sharded_param/sharded_tensor.py` & `colossalai-nightly-2024.5.4/colossalai/legacy/zero/sharded_param/sharded_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/logging/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/logging/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/logging/logger.py` & `colossalai-nightly-2024.5.4/colossalai/logging/logger.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/moe/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/moe/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/moe/_operation.py` & `colossalai-nightly-2024.5.4/colossalai/moe/_operation.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/moe/checkpoint.py` & `colossalai-nightly-2024.5.4/colossalai/moe/checkpoint.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/moe/experts.py` & `colossalai-nightly-2024.5.4/colossalai/moe/experts.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/moe/layers.py` & `colossalai-nightly-2024.5.4/colossalai/moe/layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/moe/load_balance.py` & `colossalai-nightly-2024.5.4/colossalai/moe/load_balance.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/moe/loss.py` & `colossalai-nightly-2024.5.4/colossalai/moe/loss.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/moe/manager.py` & `colossalai-nightly-2024.5.4/colossalai/moe/manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/moe/routers.py` & `colossalai-nightly-2024.5.4/colossalai/moe/routers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/moe/utils.py` & `colossalai-nightly-2024.5.4/colossalai/moe/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/init.py` & `colossalai-nightly-2024.5.4/colossalai/nn/init.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/layer/layernorm.py` & `colossalai-nightly-2024.5.4/colossalai/nn/layer/layernorm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/layer/scaled_softmax.py` & `colossalai-nightly-2024.5.4/colossalai/nn/layer/scaled_softmax.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,14 +4,22 @@
 import enum
 
 import torch
 import torch.nn as nn
 
 from colossalai.kernel.kernel_loader import ScaledMaskedSoftmaxLoader, ScaledUpperTriangleMaskedSoftmaxLoader
 
+# NOTE: These kernels are compiled on specific GPU arch and not widely applicable.
+# try:
+#     from colossalai._C import scaled_masked_softmax as scaled_masked_softmax, scaled_upper_triangle_masked_softmax_cuda as scaled_upper_triang_masked_softmax
+# except ImportError:
+
+scaled_masked_softmax = None
+scaled_upper_triang_masked_softmax = None
+
 
 class AttnMaskType(enum.Enum):
     padding = 1
     causal = 2
     paddedcausal = 3
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/cosine.py` & `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/cosine.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/delayed.py` & `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/delayed.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/linear.py` & `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/linear.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/multistep.py` & `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/multistep.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/onecycle.py` & `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/onecycle.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/poly.py` & `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/poly.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/lr_scheduler/torch.py` & `colossalai-nightly-2024.5.4/colossalai/nn/lr_scheduler/torch.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/optimizer/cpu_adam.py` & `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/cpu_adam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/optimizer/fused_adam.py` & `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_adam.py`

 * *Files 3% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 
 Copyright NVIDIA/apex
 This file is adapted from fused adam in NVIDIA/apex, commit a109f85
 Licensed under the MIT License.
 """
 import torch
 
-from colossalai.utils import multi_tensor_applier
+from colossalai.utils import get_current_device, multi_tensor_applier
 
 
 class FusedAdam(torch.optim.Optimizer):
     """Implements Adam algorithm.
 
     `FusedAdam` requires CUDA extensions which can be built during installation or runtime.
 
@@ -71,15 +71,15 @@
         self.set_grad_none = set_grad_none
         if multi_tensor_applier.available:
             from colossalai.kernel.kernel_loader import FusedOptimizerLoader
 
             fused_optim = FusedOptimizerLoader().load()
 
             # Skip buffer
-            self._dummy_overflow_buf = torch.cuda.IntTensor([0])
+            self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
             self.multi_tensor_adam = fused_optim.multi_tensor_adam
         else:
             raise RuntimeError("FusedAdam requires cuda extensions")
 
     def zero_grad(self, set_to_none=False):
         if set_to_none:
             for group in self.param_groups:
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/optimizer/fused_lamb.py` & `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_lamb.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/optimizer/fused_sgd.py` & `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/fused_sgd.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/optimizer/hybrid_adam.py` & `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/hybrid_adam.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from typing import Any, Optional
 
 import torch
 
 from colossalai.kernel.kernel_loader import FusedOptimizerLoader
-from colossalai.utils import multi_tensor_applier
+from colossalai.utils import get_current_device, multi_tensor_applier
 
 from .cpu_adam import CPUAdam
 
 
 class HybridAdam(CPUAdam):
     """Implements Adam algorithm.
 
@@ -83,15 +83,15 @@
             adamw_mode,
             nvme_offload_fraction,
             nvme_offload_dir,
         )
         if torch.cuda.is_available():
             fused_optim = FusedOptimizerLoader().load()
             self.gpu_adam_op = fused_optim.multi_tensor_adam
-            self._dummy_overflow_buf = torch.cuda.IntTensor([0])
+            self._dummy_overflow_buf = torch.tensor([0], dtype=torch.int, device=get_current_device())
 
     @torch.no_grad()
     def step(self, closure=None, div_scale: float = -1):
         loss = None
         if closure is not None:
             with torch.enable_grad():
                 loss = closure()
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/optimizer/lamb.py` & `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/lamb.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/optimizer/lars.py` & `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/lars.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/nn/optimizer/nvme_optimizer.py` & `colossalai-nightly-2024.5.4/colossalai/nn/optimizer/nvme_optimizer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/pipeline/p2p.py` & `colossalai-nightly-2024.5.4/colossalai/pipeline/p2p.py`

 * *Files 2% similar despite different names*

```diff
@@ -41,14 +41,26 @@
     io_bytes = io.BytesIO(buf)
     byte_pickler = pickle.Unpickler(io_bytes)
     unpickle = byte_pickler.load()
 
     return unpickle
 
 
+def check_for_nccl_backend(group):
+    pg = group or c10d._get_default_group()
+    # Gate PG wrapper check on Gloo availability.
+    if c10d._GLOO_AVAILABLE:
+        # It is not expected for PG to be wrapped many times, but support it just
+        # in case
+        while isinstance(pg, c10d._ProcessGroupWrapper):
+            pg = pg.wrapped_pg
+
+    return c10d.is_nccl_available() and pg.name() == c10d.Backend.NCCL
+
+
 # NOTE: FIXME: NPU DOES NOT support isend nor irecv, so broadcast is kept for future use
 def _broadcast_object_list(
     object_list: List[Any], src: int, group: ProcessGroup, device: Optional[Union[torch.device, str, int]] = None
 ):
     """This is a modified version of the broadcast_object_list in torch.distribution
     The only difference is that object will be move to correct device after unpickled.
     If local_rank = src, then object list will be sent to rank src. Otherwise, object list will
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/pipeline/schedule/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/pipeline/schedule/base.py` & `colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/pipeline/schedule/generate.py` & `colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/generate.py`

 * *Files 0% similar despite different names*

```diff
@@ -160,15 +160,15 @@
         interval_inputs = {"hidden_states": hidden_states, "infer_state": self.mb_manager.cur_infer_state}
         logits = model_forward(model, None, interval_inputs)
         if self.verbose and self.stage_manager.is_first_stage():
             torch.cuda.synchronize()
             self.timestamps[self.mb_manager.idx].append(time.time())
         assert (
             "logits" in logits
-        ), f"When first stage in GENERATE phase, the ouput should have attribute `logits`, but has {logits.keys()}"
+        ), f"When first stage in GENERATE phase, the output should have attribute `logits`, but has {logits.keys()}"
         new_token = self._get_token_id(logits["logits"])
 
         self.mb_manager.step(new_token)
         self.action_interval_buffer.new_token = new_token
         self.action_interval_buffer.hidden_states = None
 
     def _head_encoding_action(self, model: Module):
@@ -397,15 +397,15 @@
                         }
                         logits = model_forward(model, None, interval_inputs)
                         if self.verbose and self.stage_manager.is_first_stage():
                             torch.cuda.synchronize()
                             self.timestamps[self.mb_manager.idx].append(time.time())
                         assert (
                             "logits" in logits
-                        ), f"When first stage in GENERATE phase, the ouput should have attribute `logits`, but has {logits.keys()}"
+                        ), f"When first stage in GENERATE phase, the output should have attribute `logits`, but has {logits.keys()}"
                         new_token = self._get_token_id(logits["logits"])
                         self.mb_manager.step(new_token)
                         # If the current micro batch is not DONE, go through blocks
                         if self.mb_manager.cur_state in (Status.GENERATE, Status.COOLDOWN):
                             inputs_dict = self._prepare_inputs_for_new_token(new_token)
                             interval_inputs = {"infer_state": self.mb_manager.cur_infer_state}
                             output_dict = model_forward(model, inputs_dict, interval_inputs)
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/pipeline/schedule/interleaved_pp.py` & `colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/interleaved_pp.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/pipeline/schedule/one_f_one_b.py` & `colossalai-nightly-2024.5.4/colossalai/pipeline/schedule/one_f_one_b.py`

 * *Files 1% similar despite different names*

```diff
@@ -325,15 +325,16 @@
             input_obj = self.recv_forward()
             output_obj = self.forward_step(model, input_obj, criterion, accum_loss, outputs)
             self.send_forward(output_obj)
 
         if outputs is not None:
             if isinstance(model, ModelWrapper):
                 model = model.unwrap()
-            outputs = merge_batch(outputs, getattr(model, "batch_size_dim", 0))
+            batch_size_dim = getattr(model, "batch_size_dim", 0)
+            outputs = merge_batch(outputs, batch_size_dim)
         return {"loss": accum_loss, "outputs": outputs}
 
     def run_forward_backward(
         self,
         model: Module,
         data_iter: Iterable,
         criterion: Callable[..., Any],
@@ -410,15 +411,16 @@
             self.send_backward(input_obj_grad)
 
         assert all(len(v) == 0 for v in input_objs) and all(len(v) == 0 for v in output_objs)
 
         if outputs is not None:
             if isinstance(model, ModelWrapper):
                 model = model.unwrap()
-            outputs = merge_batch(outputs, getattr(model, "batch_size_dim", 0))
+            batch_size_dim = getattr(model, "batch_size_dim", 0)
+            outputs = merge_batch(outputs, batch_size_dim)
         return {"loss": accum_loss, "outputs": outputs}
 
     def forward_backward_step(
         self,
         model: Module,
         data_iter: Iterable,
         criterion: Callable[..., Any],
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/pipeline/stage_manager.py` & `colossalai-nightly-2024.5.4/colossalai/pipeline/stage_manager.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 import contextlib
-from typing import Dict, List, Optional, Tuple
+from typing import Dict, List, Optional, Tuple, Union
 
+import numpy as np
 import torch.distributed as dist
 from torch.distributed import ProcessGroup
 
 from colossalai.cluster import ProcessGroupMesh
 
 
 class PipelineStageManager:
@@ -22,22 +23,26 @@
 
     def __init__(
         self,
         pg_mesh: ProcessGroupMesh,
         pipeline_axis: int,
         enable_interleave: bool = False,
         num_model_chunks: int = 1,
+        num_layers_per_stage: Optional[List[int]] = None,
     ) -> None:
         assert enable_interleave or num_model_chunks == 1, "num_model_chunks must be 1 when enable_interleave is False"
 
         self.pg_mesh = pg_mesh
         self.pipeline_axis = pipeline_axis
         self.prev_rank: Optional[Tuple[int, ...]] = None
         self.next_rank: Optional[Tuple[int, ...]] = None
         self.p2p_groups: Dict[Tuple[int, int], ProcessGroup] = {}
+        if num_layers_per_stage is not None:
+            assert len(num_layers_per_stage) == self.num_stages
+        self.num_layers_per_stage = num_layers_per_stage
 
         # init prev and next coord
         coord = self.pg_mesh.coordinate()
         # the prev rank of rank0 is the last rank
         prev_coord = coord[: self.pipeline_axis] + (coord[self.pipeline_axis] - 1,) + coord[self.pipeline_axis + 1 :]
         self.prev_rank = self.pg_mesh.ravel(prev_coord, self.pg_mesh.shape, mode="wrap")
         # the next rank of the last rank is rank0
@@ -49,30 +54,64 @@
         for prev, cur in zip(stages[:-1], stages[1:]):
             group = self.pg_mesh.get_group_along_axis(self.pipeline_axis, [prev, cur])
             if self.stage in [prev, cur]:
                 ranks_in_group = self.pg_mesh.get_ranks_in_group(group)
                 self.p2p_groups[tuple(ranks_in_group)] = group
 
         self.is_interleave = enable_interleave
+        # for interleaved pipeline parallel, each device is responsible for multiple chunk of layers
+        self.num_model_chunks: int = num_model_chunks
         if enable_interleave:
             # use circle p2p communication
             # add the process group of the first rank and the last rank
             group = self.pg_mesh.get_group_along_axis(self.pipeline_axis, [stages[0], stages[-1]])
             if self.stage in [stages[0], stages[-1]]:
                 ranks_in_group = self.pg_mesh.get_ranks_in_group(group)
                 self.p2p_groups[tuple(ranks_in_group)] = group
 
-            # for interleaved pipeline parallel, each device is responsible for multiple chunk of layers
-            self.num_model_chunks: int = num_model_chunks
-
             # for shardformer, hold stage indices of model
             self.stage_indices: List[Tuple[int, int]]
             # for shardformer, hold model chunk id
             self.model_chunk_id: Optional[int] = None
 
+    def get_stage_index(
+        self,
+        layers_per_stage: List[int],
+        stage: Optional[int] = None,
+        num_model_chunks: Optional[int] = None,
+        num_stages: Optional[int] = None,
+    ) -> Union[Tuple[int, int], List[Tuple[int, int]]]:
+        """
+        Get the start index and end index of layers for each stage.
+
+        Args:
+            layers_per_stage (List[int]): number of layers for each stage
+            stage (int): the stage index
+            num_stages (int): number of stages
+            num_model_chunks (int): number of model chunks
+
+        Returns:
+            - Tuple[int, int]: the start index and end index of this stage
+            - List[Tuple[int, int]]: the start index and end index of this stage for each model chunk
+
+        """
+        stage = self.stage if stage is None else stage
+        num_model_chunks = self.num_model_chunks if num_model_chunks is None else num_model_chunks
+        num_stages = self.num_stages if num_stages is None else num_stages
+
+        num_layers_per_stage_accumulated = np.insert(np.cumsum(layers_per_stage), 0, 0)
+
+        stage_indices = []
+        for model_chunk in range(num_model_chunks):
+            start_idx = num_layers_per_stage_accumulated[stage + model_chunk * num_stages]
+            end_idx = num_layers_per_stage_accumulated[stage + model_chunk * num_stages + 1]
+            stage_indices.append([start_idx, end_idx])
+
+        return stage_indices[0] if num_model_chunks == 1 else stage_indices
+
     def is_first_stage(self, ignore_chunk: bool = False) -> bool:
         """Is the current stage the first stage.
 
         NOTE:
             1. if using interleaved pipeline parallel, the first stage is the first chunk of the first device.
             2. invoke is_first_stage() with ignore_chunk=True is equivalent to invoke is_first_device()
 
@@ -172,7 +211,29 @@
 
     @contextlib.contextmanager
     def switch_model_chunk_id(self, model_chunk_id: int):
         old_model_chunk_id = self.model_chunk_id
         self.model_chunk_id = model_chunk_id
         yield
         self.model_chunk_id = old_model_chunk_id
+
+    def distribute_layers(
+        self, num_layers: int, num_stages: Optional[int] = None, num_model_chunks: Optional[int] = None
+    ) -> List[int]:
+        if self.num_layers_per_stage is not None:
+            assert sum(self.num_layers_per_stage) == num_layers
+            return self.num_layers_per_stage
+
+        num_stages = self.num_stages if num_stages is None else num_stages
+        num_model_chunks = self.num_model_chunks if num_model_chunks is None else num_model_chunks
+        quotient = num_layers // (num_stages * num_model_chunks)
+        remainder = num_layers % (num_stages * num_model_chunks)
+
+        # calculate the num_layers per stage
+        layers_per_stage = [quotient] * num_stages * num_model_chunks
+
+        # deal with the rest layers
+        if remainder > 0:
+            start_position = (num_stages * num_model_chunks) // 2 - remainder // 2
+            for i in range(start_position, start_position + remainder):
+                layers_per_stage[i] += 1
+        return layers_per_stage
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/layer/_operation.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/comm_spec.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,652 +1,528 @@
+import operator
+from enum import Enum
+from functools import reduce
+
 import torch
 import torch.distributed as dist
-import torch.nn.functional as F
-
-try:
-    import fused_mix_prec_layer_norm_cuda
-except:
-    fused_mix_prec_layer_norm_cuda = None
-
-try:
-    import fused_weight_gradient_mlp_cuda
+from torch.distributed import ReduceOp
 
-    _grad_accum_fusion_available = True
-except ImportError:
-    _grad_accum_fusion_available = False
+__all__ = [
+    "CollectiveCommPattern",
+    "CommSpec",
+]
 
 
-class FusedLayerNormAffineFunction1D(torch.autograd.Function):
-    r"""Layernorm
-
-    Args:
-        input: input matrix.
-        weight: weight matrix.
-        bias: bias matrix.
-        normalized_shape: input shape from an expected input of size.
-            :math:`[* \times \text{normalized_shape}[0] \times \text{normalized_shape}[1] \times \ldots \times \text{normalized_shape}[-1]]`
-            If a single integer is used, it is treated as a singleton list, and this module will
-            normalize over the last dimension which is expected to be of that specific size.
-        eps: a value added to the denominator for numerical stability
+def _all_gather(tensor, comm_spec):
     """
+    Implement all gather operation on device mesh based on information provided by comm_spec.
+    """
+    process_groups = comm_spec.device_mesh.get_process_group_for_all_axes()
+    process_group = process_groups[comm_spec.logical_process_axis]
 
-    @staticmethod
-    def forward(ctx, input, weight, bias, normalized_shape, eps):
-        ctx.normalized_shape = normalized_shape
-        ctx.eps = eps
-        input_ = input.contiguous()
-        weight_ = weight.contiguous()
-        bias_ = bias.contiguous()
-        output, mean, invvar = fused_mix_prec_layer_norm_cuda.forward_affine(
-            input_, ctx.normalized_shape, weight_, bias_, ctx.eps
-        )
-        ctx.save_for_backward(input_, weight_, bias_, mean, invvar)
-        return output
-
-    @staticmethod
-    def backward(ctx, grad_output):
-        input_, weight_, bias_, mean, invvar = ctx.saved_tensors
-        grad_input = grad_weight = grad_bias = None
-        grad_input, grad_weight, grad_bias = fused_mix_prec_layer_norm_cuda.backward_affine(
-            grad_output.contiguous(), mean, invvar, input_, ctx.normalized_shape, weight_, bias_, ctx.eps
-        )
-
-        return grad_input, grad_weight, grad_bias, None, None
+    tensor_list = [
+        torch.zeros(tensor.shape, dtype=tensor.dtype, device=tensor.device)
+        for _ in range(comm_spec.device_mesh.shape[comm_spec.logical_process_axis])
+    ]
+    # without this contiguous operation, the all gather may get some unexpected results.
+    tensor = tensor.contiguous()
+    dist.all_gather(tensor_list, tensor, group=process_group)
+    output = torch.cat(tuple(tensor_list), comm_spec.gather_dim).contiguous()
+    return output
 
 
-class MatmulWithAsyncCommunication(torch.autograd.Function):
+def _split(tensor, comm_spec):
     """
-    Linear layer execution with asynchronous communication in backprop.
+    Implement shard operation on device mesh based on information provided by comm_spec.
     """
+    process_groups = comm_spec.device_mesh.get_process_group_for_all_axes()
+    process_group = process_groups[comm_spec.logical_process_axis]
 
-    @staticmethod
-    def forward(ctx, input_, weight, bias, process_group, async_grad_allreduce):
-        ctx.save_for_backward(input_, weight, bias)
-        ctx.use_bias = bias is not None
-        ctx.process_group = process_group
-        ctx.async_grad_allreduce = async_grad_allreduce
-
-        output = torch.matmul(input_, weight)
-
-        if bias is not None:
-            output = output + bias
-
-        return output
-
-    @staticmethod
-    def backward(ctx, grad_output):
-        input, weight, bias = ctx.saved_tensors
-        use_bias = ctx.use_bias
-
-        # In order to be hooked into Gemini's '__torch_function__', adding a view operation to weight and bias.
-        weight = weight.view(weight.shape)
-        if bias is not None:
-            bias = bias.view(bias.shape)
-
-        total_input = input
-        grad_input = grad_output.matmul(weight.T)
-        grad_output = grad_output.contiguous()
-        # Convert the tensor shapes to 2D for execution compatibility
-        if len(grad_output.shape) > 2:
-            grad_output = grad_output.view(-1, grad_output.shape[-1])
-            total_input = total_input.view(-1, total_input.shape[-1])
-
-        if ctx.async_grad_allreduce:
-            # Asynchronous all-reduce
-            handle = dist.all_reduce(grad_input, group=ctx.process_group, async_op=True)
-            # Relay on CUDA_DEVICE_MAX_CONNECTIONS=1 to have
-            # all-reduce scheduled first and have GPU resources allocated, CUDA_DEVICE_MAX_CONNECTIONS=1 is set in shardformer.py
-
-        grad_weight = total_input.t().matmul(grad_output)
-        grad_bias = grad_output.sum(dim=0) if use_bias else None
-
-        if ctx.async_grad_allreduce:
-            handle.wait()
-
-        return grad_input, grad_weight, grad_bias, None, None, None
+    dim = comm_spec.shard_dim
+    length = tensor.shape[comm_spec.shard_dim] // dist.get_world_size(process_group)
+    start = length * dist.get_rank(process_group)
+    output = torch.narrow(tensor, dim, start, length).contiguous()
+    return output
 
 
-class LinearWithAsyncCommunication(torch.autograd.Function):
+def _all_to_all(tensor, comm_spec):
     """
-    Linear layer execution with asynchronous communication in backprop.
+    Implement all to all operation on device mesh based on information provided by comm_spec.
     """
+    process_groups = comm_spec.device_mesh.get_process_group_for_all_axes()
+    process_group = process_groups[comm_spec.logical_process_axis]
+    world_size = dist.get_world_size(process_group)
 
-    @staticmethod
-    def forward(ctx, input_, weight, bias, process_group, async_grad_allreduce):
-        ctx.save_for_backward(input_, weight, bias)
-        ctx.use_bias = bias is not None
-        ctx.process_group = process_group
-        ctx.async_grad_allreduce = async_grad_allreduce
-        if bias is not None:
-            output = F.linear(input_, weight, bias)
-        else:
-            output = F.linear(input_, weight)
-
-        return output
-
-    @staticmethod
-    def backward(ctx, grad_output):
-        input, weight, bias = ctx.saved_tensors
-        use_bias = ctx.use_bias
-
-        # In order to be hooked into Gemini's '__torch_function__', adding a view operation to bias.
-        if use_bias:
-            bias.view(bias.shape)
-
-        total_input = input
-        grad_input = grad_output.matmul(weight)
-        grad_output = grad_output.contiguous()
-        # Convert the tensor shapes to 2D for execution compatibility
-        if len(grad_output.shape) > 2:
-            grad_output = grad_output.view(-1, grad_output.shape[-1])
-            total_input = total_input.view(-1, total_input.shape[-1])
-
-        if ctx.async_grad_allreduce:
-            # Asynchronous all-reduce
-            handle = dist.all_reduce(grad_input, group=ctx.process_group, async_op=True)
-            # Relay on CUDA_DEVICE_MAX_CONNECTIONS=1 to have
-            # all-reduce scheduled first and have GPU resources allocated, CUDA_DEVICE_MAX_CONNECTIONS=1 is set in shardformer.py
-
-        if _grad_accum_fusion_available and weight.grad is not None:
-            grad = weight.grad
-            if grad.dtype == torch.float32:
-                fused_weight_gradient_mlp_cuda.wgrad_gemm_accum_fp32(total_input, grad_output, grad)
-                grad_weight = None
-            elif grad.dtype == torch.float16:
-                fused_weight_gradient_mlp_cuda.wgrad_gemm_accum_fp16(total_input, grad_output, grad)
-                grad_weight = None
-            else:
-                grad_weight = grad_output.t().matmul(total_input)
-        else:
-            grad_weight = grad_output.t().matmul(total_input)
-
-        grad_bias = grad_output.sum(dim=0) if use_bias else None
-
-        if ctx.async_grad_allreduce:
-            handle.wait()
-
-        return grad_input, grad_weight, grad_bias, None, None, None
-
-
-class _LinearWithGatherForwardReduceScatterBackward(torch.autograd.Function):
-    """Gather input from sequence parallel in forward and reduce-scatter gradient in backward
+    new_shape = list(tensor.shape)
+    new_shape[comm_spec.shard_dim] = new_shape[comm_spec.shard_dim] // world_size
+    new_shape = torch.Size(new_shape)
+    output_tensor_list = [torch.zeros(new_shape, dtype=tensor.dtype, device=tensor.device) for _ in range(world_size)]
+    dim = comm_spec.shard_dim
+    length = tensor.shape[comm_spec.shard_dim] // world_size
+    input_tensor_list = [torch.narrow(tensor, dim, length * i, length).contiguous() for i in range(world_size)]
+    group = process_group
+    dist.all_to_all(output_tensor_list, input_tensor_list, group)
+    output = torch.cat(tuple(output_tensor_list), comm_spec.gather_dim).contiguous()
+    return output
 
-    Args:
-        input_ (`torch.Tensor`): The input tensor from sequence parallel region.
-        process_group (`torch.distributed.ProcessGroup`): The process group used for collective communication.
-        overlap (`bool`): Whether to overlap the all_gather op and gradient calculate in backward.
 
+def _all_reduce(tensor, comm_spec, async_op=False):
     """
+    Implement all reduce operation on device mesh based on information provided by comm_spec.
+    """
+    process_groups = comm_spec.device_mesh.get_process_group_for_all_axes()
+    process_group = process_groups[comm_spec.logical_process_axis]
 
-    @staticmethod
-    def forward(ctx, input_, weight, bias, process_group, async_grad_reduce_scatter, dim, overlap=True):
-        ctx.save_for_backward(input_, weight, bias)
-        ctx.use_bias = bias is not None
-        ctx.process_group = process_group
-        ctx.async_grad_reduce_scatter = async_grad_reduce_scatter
-        ctx.dim = dim
-        ctx.overlap = overlap
-
-        input_parallel = _gather(input_, dim, process_group)
-
-        if bias is not None:
-            output = F.linear(input_parallel, weight, bias)
-        else:
-            output = F.linear(input_parallel, weight)
-
-        return output
+    if not tensor.is_contiguous():
+        tensor = tensor.contiguous()
+    dist.all_reduce(tensor, op=ReduceOp.SUM, group=process_group, async_op=async_op)
+    return tensor
+
+
+def _mix_gather(tensor, comm_spec):
+    """
+    Implement mix gather operation on device mesh based on information provided by comm_spec.
+    Mix gather is the all-gather operation on all devices in the device_mesh(FlattenDeviceMesh) of the comm_spec. It is
+    different from _all_gather because _mix_gather does all-gather in two dimensions of device mesh, while _all_gather
+    only does all-gather in one dimension.
+    Assume index of f and b target pairs are 'f' and 'b'
+    ShardingSpec => gather_dim, logical_process_axes
+    S0S1 => [b, f], (1, 0)
+    S1S0 => [b, f], (0, 1)
+    S01R => [f], (1, 1)
+    RS01 => [b], (1, 1)
+    Example:
+    mesh_shape = (2,4)
+            # [[0, 1, 2, 3],
+            #  [4, 5, 6, 7]]
+            # return {0: [0, 4, 1, 5, 2, 6, 3, 7], 1: [0, 1, 2, 3, 4, 5, 6, 7]}
+    S0S1:
+    leading_group_dim = 1
+    process_group = "[0, 1, 2, 3, 4, 5, 6, 7]"
+    tensor_list = [(0,0),(0,1),(0,2),(0,3),(1,0),(1,1),(1,2),(1,3)] # [(slice_id_f, slice_id_b),...]
+    mesh_shape = (2,4)
+    cat_slice = [4,2]
+    tmp_tensor_list = [(...,shape[f],shape[b]*4,...),(...,shape[f],shape[b]*4,...)]
+    tmp_tensor_list[0] = torch.cat(((0,0),(0,1),(0,2),(0,3)), dim=b)
+    tmp_tensor_list[1] = torch.cat(((1,0),(1,1),(1,2),(1,3)), dim=b)
+    output = torch.cat((tmp_tensor_list[0],tmp_tensor_list[1]), dim=a)
+    S1S0:
+    leading_group_dim = 0
+    process_group = "[0, 4, 1, 5, 2, 6, 3, 7]"
+    tensor_list = [(0,0),(0,1),(1,0),(1,1),(2,0),(2,1),(3,0),(3,1)]
+    mesh_shape = (2,4)
+    cat_slice = [2,4]
+    tmp_tensor_list = [(...,shape[f],shape[b]*2,...),(...,shape[f],shape[b]*2,...),(...,shape[f],shape[b]*2,...),(...,shape[f],shape[b]*2,...)]
+    tmp_tensor_list[0] = torch.cat(((0,0),(0,1)), dim=b)
+    tmp_tensor_list[1] = torch.cat(((1,0),(1,1)), dim=b)
+    tmp_tensor_list[2] = torch.cat(((2,0),(2,1)), dim=b)
+    tmp_tensor_list[3] = torch.cat(((3,0),(3,1)), dim=b)
+    S10R:
+    leading_group_dim = 0
+    process_group = "[0, 4, 1, 5, 2, 6, 3, 7]"
+    tensor_list = [(0,0),(1,0),(2,0),(3,0),(4,0),(5,0),(6,0),(7,0)]
+    S01R:
+    leading_group_dim = 1
+    process_group = "[0, 1, 2, 3, 4, 5, 6, 7]"
+    tensor_list = [(0,0),(1,0),(2,0),(3,0),(4,0),(5,0),(6,0),(7,0)]
+    """
+    total_slices = comm_spec.device_mesh.shape[0]
+    tensor_list = [torch.zeros(tensor.shape, dtype=tensor.dtype, device=tensor.device) for _ in range(total_slices)]
+    leading_group_dim = comm_spec.logical_process_axes[0]
+    assert len(comm_spec.device_mesh.process_groups_dict) == 1
+    _, process_group = comm_spec.device_mesh.process_groups_dict[0][0]
+    process_number_list = comm_spec.device_meshes.process_number_dict[leading_group_dim]
+
+    # Global all_gather
+    dist.all_gather(tensor_list, tensor, group=process_group)
+
+    # This is very ugly. I'm figuring out more elegant methods
+    tensor_list_sorted = [
+        torch.zeros(tensor.shape, dtype=tensor.dtype, device=tensor.device) for _ in range(total_slices)
+    ]
+    for i in range(total_slices):
+        tensor_list_sorted[i] = tensor_list[process_number_list[i]]
+    tensor_list = tensor_list_sorted
 
-    @staticmethod
-    def backward(ctx, grad_output):
-        input_, weight, bias = ctx.saved_tensors
-        use_bias = ctx.use_bias
-        dim = ctx.dim
-        process_group = ctx.process_group
-        overlap = ctx.overlap
-
-        # In order to be hooked into Gemini's '__torch_function__', adding a view operation to weight and bias. Used in FusedLayerNorm
-        if use_bias:
-            bias = bias.view(bias.shape)
-
-        if not overlap:
-            input_parallel = _gather(input_, dim, process_group)
-
-            total_input = input_parallel
-            grad_input = grad_output.matmul(weight)
-            grad_output = grad_output.contiguous()
-            # Convert the tensor shapes to 2D for execution compatibility
-            if len(grad_output.shape) > 2:
-                grad_output = grad_output.view(-1, grad_output.shape[-1])
-                total_input = total_input.view(-1, total_input.shape[-1])
-
-            if ctx.async_grad_reduce_scatter:
-                # Asynchronous reduce-scatter
-                input_list = [
-                    item.contiguous() for item in torch.chunk(grad_input, dist.get_world_size(process_group), dim=dim)
-                ]
-                output = torch.empty(
-                    input_.shape, dtype=input_parallel.dtype, device=input_parallel.device
-                ).contiguous()
-                handle = dist.reduce_scatter(output, input_list, group=process_group, async_op=True)
-                # Relay on CUDA_DEVICE_MAX_CONNECTIONS=1 to have
-                # all-reduce scheduled first and have GPU resources allocated, CUDA_DEVICE_MAX_CONNECTIONS=1 is set in shardformer.py
-
-            if _grad_accum_fusion_available and weight.grad is not None:
-                grad = weight.grad
-                if grad.dtype == torch.float32:
-                    fused_weight_gradient_mlp_cuda.wgrad_gemm_accum_fp32(total_input, grad_output, grad)
-                    grad_weight = None
-                elif grad.dtype == torch.float16:
-                    fused_weight_gradient_mlp_cuda.wgrad_gemm_accum_fp16(total_input, grad_output, grad)
-                    grad_weight = None
-                else:
-                    grad_weight = grad_output.t().matmul(total_input)
-            else:
-                grad_weight = grad_output.t().matmul(total_input)
+    if comm_spec.logical_process_axes[0] == comm_spec.logical_process_axes[1]:
+        output = torch.cat(tuple(tensor_list), comm_spec.gather_dim[0]).contiguous()
+    else:
+        mesh_shape = comm_spec.device_meshes.shape
+        cat_slice = [mesh_shape[comm_spec.logical_process_axes[0]], mesh_shape[comm_spec.logical_process_axes[1]]]
+        tmp_tensor_shape = list(tensor.shape)
+        tmp_tensor_shape[comm_spec.gather_dim[0]] *= cat_slice[0]
+        tmp_tensor_shape = torch.Size(tmp_tensor_shape)
+        tmp_tensor_list = [
+            torch.zeros(tmp_tensor_shape, dtype=tensor.dtype, device=tensor.device) for _ in range(cat_slice[1])
+        ]
+        for i in range(cat_slice[1]):
+            tmp_tensor_list[i] = torch.cat(
+                tuple(tensor_list[i * cat_slice[0] : (i + 1) * cat_slice[0]]), comm_spec.gather_dim[0]
+            ).contiguous()
+        output = torch.cat(tuple(tmp_tensor_list), comm_spec.gather_dim[1]).contiguous()
 
-            grad_bias = grad_output.sum(dim=0) if use_bias else None
+    return output
 
-            if ctx.async_grad_reduce_scatter:
-                handle.wait()
 
-        else:
-            input_ = input_.contiguous()
-            world_size = dist.get_world_size(process_group)
-            tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
-
-            # do all gather in is async way
-            gather_handle = dist.all_gather(tensor_list, input_, group=process_group, async_op=True)
-            # calculate gradient and prepare data asynchronously with all-gather
-            # calculate
-            grad_input = grad_output.matmul(weight)
-            grad_output = grad_output.contiguous()
-            # Convert the tensor shapes to 2D for execution compatibility
-            if len(grad_output.shape) > 2:
-                grad_output = grad_output.view(-1, grad_output.shape[-1])
-            grad_bias = grad_output.sum(dim=0) if use_bias else None
-            # prepare data
-            input_list = [
-                item.contiguous() for item in torch.chunk(grad_input, dist.get_world_size(process_group), dim=dim)
-            ]
-            output = torch.empty(input_.shape, dtype=input_.dtype, device=input_.device).contiguous()
-            # wait until all-gather finished
-            gather_handle.wait()
-
-            # do reduce-scatter in async way
-            reducescatter_handle = dist.reduce_scatter(output, input_list, group=process_group, async_op=True)
-            input_parallel = torch.cat(tensor_list, dim=dim).contiguous()
-            # calculate gradient
-            if len(input_parallel.shape) > 2:
-                input_parallel = input_parallel.view(-1, input_parallel.shape[-1])
-
-            if _grad_accum_fusion_available and weight.grad is not None:
-                grad = weight.grad
-                if grad.dtype == torch.float32:
-                    fused_weight_gradient_mlp_cuda.wgrad_gemm_accum_fp32(input_parallel, grad_output, grad)
-                    grad_weight = None
-                elif grad.dtype == torch.float16:
-                    fused_weight_gradient_mlp_cuda.wgrad_gemm_accum_fp16(input_parallel, grad_output, grad)
-                    grad_weight = None
-                else:
-                    grad_weight = grad_output.t().matmul(input_parallel)
-            else:
-                grad_weight = grad_output.t().matmul(input_parallel)
-            # grad_weight = grad_output.t().matmul(input_parallel)
-            # wait until reduce-scatter finished
-            reducescatter_handle.wait()
+def _mix_split(tensor, comm_spec):
+    """
+    Implement mix split operation. Mix split is only called for the backward of mix gather (Use ctx to keep consistent)
+    Mix split shards the tensor on device mesh based on information provided by comm_spec. It is different from split
+    because _mix_split shards the tensor in two dimensions of device mesh, while _split only shards in one dimension.
+    Assume index of f and b target pairs are 'f' and 'b'
+    S0S1 => [b, f], (1, 0)
+    S1S0 => [b, f], (0, 1)
+    S01R => [f], (0, 0)
+    RS01 => [b], (0, 0)
+    Example:
+    mesh_shape = (2,4)
+            # [[0, 1, 2, 3],
+            #  [4, 5, 6, 7]]
+            # return {0: [0, 4, 1, 5, 2, 6, 3, 7], 1: [0, 1, 2, 3, 4, 5, 6, 7]}
+    """
+    mesh_shape = comm_spec.device_meshes.shape
+    dim = comm_spec.gather_dim
+    total_slices = comm_spec.device_mesh.shape[0]
+
+    # Get global rank
+    rank = dist.get_rank()
+
+    leading_group_dim = comm_spec.logical_process_axes[0]
+    process_number_list = comm_spec.device_meshes.process_number_dict[leading_group_dim]
+    rank = process_number_list.index(rank)
+
+    if comm_spec.logical_process_axes[0] == comm_spec.logical_process_axes[1]:
+        length = tensor.shape[dim[0]] // total_slices
+        start = length * rank
+        output = torch.narrow(tensor, dim[0], start, length).contiguous()
+    else:
+        tensor_shape = [tensor.shape[dim[0]], tensor.shape[dim[1]]]
+        rank_slice = [mesh_shape[comm_spec.logical_process_axes[0]], mesh_shape[comm_spec.logical_process_axes[1]]]
+        length = [tensor_shape[0] // rank_slice[0], tensor_shape[1] // rank_slice[1]]
+        start = [(rank % rank_slice[0]) * length[0], (rank // rank_slice[0]) * length[1]]
+        tmp_output = torch.narrow(tensor, dim[0], start[0], length[0]).contiguous()
+        output = torch.narrow(tmp_output, dim[1], start[1], length[1]).contiguous()
 
-        return output, grad_weight, grad_bias, None, None, None, None
+    return output
 
 
-class _LinearWithReduceScatterForwardGatherBackward(torch.autograd.Function):
-    """Gather input from sequence parallel in forward and reduce-scatter gradient in backward
+class _ReduceGrad(torch.autograd.Function):
+    """
+    A customized communication operation which forward is an identity operation,
+    backward is all_reduce operation.
 
     Args:
-        input_ (`torch.Tensor`): The input tensor from sequence parallel region.
-        process_group (`torch.distributed.ProcessGroup`): The process group used for collective communication.
-
+        input_: input matrix.
+        comm_spec: comm_spec will give information like process group, rank list, etc.
     """
 
     @staticmethod
-    def forward(ctx, input_, process_group, dim):
-        ctx.dim = dim
-        ctx.process_group = process_group
-
-        # do reduce-scatter
-        new_shape = list(input_.shape)
-        assert (
-            new_shape[dim] % dist.get_world_size(process_group) == 0
-        ), f"The dimension to split ({new_shape[dim]}) is not a multiple of tensor parallel size ({dist.get_world_size(process_group)}). "
-        new_shape[dim] = new_shape[dim] // dist.get_world_size(process_group)
-        input_list = [item.contiguous() for item in torch.chunk(input_, dist.get_world_size(process_group), dim=dim)]
-        output = torch.empty(new_shape, dtype=input_.dtype, device=input_.device)
-        dist.reduce_scatter(output, input_list, group=process_group)
+    def symbolic(graph, input_):
+        return input_
 
-        return output
+    @staticmethod
+    def forward(ctx, input_, comm_spec):
+        ctx.comm_spec = comm_spec
+        return input_
 
     @staticmethod
     def backward(ctx, grad_output):
-        dim = ctx.dim
-        process_group = ctx.process_group
-
-        return _gather(grad_output, dim, process_group), None, None
+        return _all_reduce(grad_output, ctx.comm_spec), None
 
 
-class _MatmulWithGatherForwardReduceScatterBackward(torch.autograd.Function):
+class _ReduceInput(torch.autograd.Function):
     """
-    This class is designed for matmul operation with gather forward and reduce-scatter backward.
+    A customized communication operation which forward is all_reduce operation,
+    backward is an identity operation.
 
     Args:
-        input_ (`torch.Tensor`): input matrix.
-        dim (int): the dimension to perform split and gather
-        process_group (`torch.distributed.ProcessGroup`): the process group used for collective communication
-
+        input_: input matrix.
+        comm_spec: comm_spec will give information like process group, rank list, etc.
     """
 
     @staticmethod
-    def forward(ctx, input_, weight, bias, process_group, async_grad_reduce_scatter, dim, overlap):
-        ctx.save_for_backward(input_, weight, bias)
-        ctx.use_bias = bias is not None
-        ctx.process_group = process_group
-        ctx.async_grad_reduce_scatter = async_grad_reduce_scatter
-        ctx.dim = dim
-        ctx.overlap = overlap
-
-        input_parallel = _gather(input_, dim, process_group)
+    def symbolic(graph, input_):
+        return _all_reduce(input_)
 
-        output = torch.matmul(input_parallel, weight)
-
-        if bias is not None:
-            output = output + bias
-        return output
+    @staticmethod
+    def forward(ctx, input_, comm_spec):
+        return _all_reduce(input_, comm_spec)
 
     @staticmethod
     def backward(ctx, grad_output):
-        input_, weight, bias = ctx.saved_tensors
-        use_bias = ctx.use_bias
-        dim = ctx.dim
-        process_group = ctx.process_group
-        overlap = ctx.overlap
-
-        # In order to be hooked into Gemini's '__torch_function__', adding a view operation to weight and bias. Used in FusedLayerNorm
-        weight = weight.view(weight.shape)
-        if use_bias:
-            bias = bias.view(bias.shape)
-
-        if not overlap:
-            input_parallel = _gather(input_, dim, process_group)
-
-            total_input = input_parallel
-            grad_input = grad_output.matmul(weight.T)
-            grad_output = grad_output.contiguous()
-            # Convert the tensor shapes to 2D for execution compatibility
-            if len(grad_output.shape) > 2:
-                grad_output = grad_output.view(-1, grad_output.shape[-1])
-                total_input = total_input.view(-1, total_input.shape[-1])
-
-            if ctx.async_grad_reduce_scatter:
-                # Asynchronous reduce-scatter
-                input_list = [
-                    item.contiguous() for item in torch.chunk(grad_input, dist.get_world_size(process_group), dim=dim)
-                ]
-                output = torch.empty(
-                    input_.shape, dtype=input_parallel.dtype, device=input_parallel.device
-                ).contiguous()
-                handle = dist.reduce_scatter(output, input_list, group=process_group, async_op=True)
-                # Relay on CUDA_DEVICE_MAX_CONNECTIONS=1 to have
-                # all-reduce scheduled first and have GPU resources allocated, CUDA_DEVICE_MAX_CONNECTIONS=1 is set in shardformer.py
-
-            grad_weight = total_input.t().matmul(grad_output)
-            grad_bias = grad_output.sum(dim=0) if use_bias else None
-
-            if ctx.async_grad_reduce_scatter:
-                handle.wait()
-
-        else:
-            world_size = dist.get_world_size(process_group)
-            tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
-
-            # do all gather in is async way
-            gather_handle = dist.all_gather(tensor_list, input_, group=process_group, async_op=True)
-            # calculate gradient and prepare data asynchronously with all-gather
-            # calculate
-            grad_input = grad_output.matmul(weight.T)
-            grad_output = grad_output.contiguous()
-            # Convert the tensor shapes to 2D for execution compatibility
-            if len(grad_output.shape) > 2:
-                grad_output = grad_output.view(-1, grad_output.shape[-1])
-            grad_bias = grad_output.sum(dim=0) if use_bias else None
-            # prepare data
-            input_list = [
-                item.contiguous() for item in torch.chunk(grad_input, dist.get_world_size(process_group), dim=dim)
-            ]
-            output = torch.empty(input_.shape, dtype=input_.dtype, device=input_.device).contiguous()
-            # wait until all-gather finished
-            gather_handle.wait()
-
-            # do reduce-scatter in async way
-            reducescatter_handle = dist.reduce_scatter(output, input_list, group=process_group, async_op=True)
-            input_parallel = torch.cat(tensor_list, dim=dim).contiguous()
-            # calculate gradient
-            if len(input_parallel.shape) > 2:
-                input_parallel = input_parallel.view(-1, input_parallel.shape[-1])
-            grad_weight = input_parallel.t().matmul(grad_output)
-            # wait until reduce-scatter finished
-            reducescatter_handle.wait()
-
-        return output, grad_weight, grad_bias, None, None, None, None
+        return grad_output, None
 
 
 class _SplitForwardGatherBackward(torch.autograd.Function):
     """
-    Split the input and keep only the corresponding chuck to the rank.
+    A customized communication operation which forward is split operation,
+    backward is an all gather operation.
 
     Args:
-        input_ (`torch.Tensor`): input matrix.
-        dim (int): the dimension to perform split and gather
-        process_group (`torch.distributed.ProcessGroup`): the process group used for collective communication
-
+        input_: input matrix.
+        comm_spec: comm_spec will give information like process group, rank list, etc.
     """
 
     @staticmethod
-    def forward(ctx, input_, dim, process_group):
-        ctx.process_group = process_group
-        ctx.dim = dim
-        return _split(input_, dim, process_group)
+    def symbolic(graph, input_):
+        return _split(input_)
+
+    @staticmethod
+    def forward(ctx, input_, comm_spec):
+        ctx.comm_spec = comm_spec
+        return _split(input_, comm_spec)
 
     @staticmethod
     def backward(ctx, grad_output):
-        return _gather(grad_output, ctx.dim, ctx.process_group), None, None
+        return _all_gather(grad_output, ctx.comm_spec), None
 
 
-class _ReduceForward(torch.autograd.Function):
+class _GatherForwardSplitBackward(torch.autograd.Function):
     """
-    All-reduce the input from the model parallel region.
+    A customized communication operation which forward is an all gather operation,
+    backward is split operation.
 
     Args:
         input_: input matrix.
-        parallel_mode: parallel mode.
+        comm_spec: comm_spec will give information like process group, rank list, etc.
     """
 
     @staticmethod
-    def forward(ctx, input_, process_group):
-        return _reduce(input_, process_group)
+    def symbolic(graph, input_):
+        return _all_gather(input_)
+
+    @staticmethod
+    def forward(ctx, input_, comm_spec):
+        ctx.comm_spec = comm_spec
+        return _all_gather(input_, comm_spec)
 
     @staticmethod
     def backward(ctx, grad_output):
-        return grad_output, None
+        return _split(grad_output, ctx.comm_spec), None
 
 
-class _ReduceBackward(torch.autograd.Function):
+class _AllToAll(torch.autograd.Function):
     """
-    All-reduce the input from the model parallel region.
+    A customized communication operation which forward is an all to all operation,
+    backward is an all to all operation.
 
     Args:
         input_: input matrix.
-        parallel_mode: parallel mode.
+        comm_spec: comm_spec will give information like process group, rank list, etc.
     """
 
     @staticmethod
-    def forward(ctx, input_, process_group):
-        ctx.process_group = process_group
-        return input_
-
-    @staticmethod
-    def backward(ctx, grad_output):
-        return _reduce(grad_output, ctx.process_group), None
-
-
-class _GatherForwardSplitBackward(torch.autograd.Function):
-    """Gather the input from model parallel region and concatenate.
-
-    Args:
-        input_: input matrix.
-        parallel_mode: parallel mode.
-        dim: dimension
-    """
+    def symbolic(graph, input_):
+        return _all_to_all(input_)
 
     @staticmethod
-    def forward(ctx, input_, dim, process_group):
-        ctx.process_group = process_group
-        ctx.dim = dim
-        return _gather(input_, dim, process_group)
+    def forward(ctx, input_, comm_spec):
+        output = _all_to_all(input_, comm_spec)
+        comm_spec_for_backward = CommSpec(
+            comm_pattern=comm_spec.comm_pattern,
+            sharding_spec=comm_spec.sharding_spec,
+            gather_dim=comm_spec.shard_dim,
+            shard_dim=comm_spec.gather_dim,
+            logical_process_axis=comm_spec.logical_process_axis,
+        )
+        ctx.comm_spec = comm_spec_for_backward
+        return output
 
     @staticmethod
-    def backward(ctx, grad_output):
-        return _split(grad_output, ctx.dim, ctx.process_group), None, None
+    def backward(ctx, grad_outputs):
+        return _all_to_all(grad_outputs, ctx.comm_spec), None
 
 
-class HookParameter(torch.autograd.Function):
-    """In order to be hooked into Gemini's '__torch_function__', adding a view operation to weight and bias. Used in FusedLayerNorm"""
+class _MixGatherForwardMixSplitBackward(torch.autograd.Function):
+    @staticmethod
+    def symbolic(graph, input_):
+        return _mix_gather(input_)
 
     @staticmethod
-    def forward(ctx, input, weight, bias):
-        ctx.save_for_backward(weight, bias)
-        output = input
-        return output
+    def forward(ctx, input_, comm_spec):
+        ctx.comm_spec = comm_spec
+        return _mix_gather(input_, comm_spec)
 
     @staticmethod
     def backward(ctx, grad_output):
-        weight, bias = ctx.saved_tensors
-        if weight is not None:
-            weight = weight.view(weight.shape)
-        if bias is not None:
-            bias = bias.view(bias.shape)
-        return grad_output, None, None
+        return _mix_split(grad_output, ctx.comm_spec), None
 
 
-def hook_parameter_in_backward(input, weight=None, bias=None):
-    return HookParameter.apply(input, weight, bias)
+def reduce_grad(input_, comm_spec):
+    return _ReduceGrad.apply(input_, comm_spec)
 
 
-def _reduce(input_, process_group):
-    # skip if only one rank involved
-    if dist.get_world_size(process_group) == 1:
-        return input_
-    else:
-        dist.all_reduce(input_, group=process_group)
-        return input_
+def reduce_input(input_, comm_spec):
+    return _ReduceInput.apply(input_, comm_spec)
 
 
-def _split(input_, dim=-1, process_group=None):
-    # skip if only one rank involved
-    world_size = dist.get_world_size(process_group)
-    if world_size == 1:
-        return input_
+def split_forward_gather_backward(input_, comm_spec):
+    return _SplitForwardGatherBackward.apply(input_, comm_spec)
 
-    # Split along last dimension.
-    dim_size = input_.size(dim)
-    assert dim_size % world_size == 0, (
-        f"The dimension to split ({dim_size}) is not a multiple of world size ({world_size}), "
-        f"cannot split tensor evenly"
-    )
-
-    tensor_list = torch.split(input_, dim_size // world_size, dim=dim)
-    rank = dist.get_rank(process_group)
-    output = tensor_list[rank].clone().contiguous()
 
-    return output
+def gather_forward_split_backward(input_, comm_spec):
+    return _GatherForwardSplitBackward.apply(input_, comm_spec)
 
 
-def _gather(input_, dim=-1, process_group=None):
-    # skip if only one rank involved
-    world_size = dist.get_world_size(process_group)
-    if world_size == 1:
-        return input_
+def all_to_all(input_, comm_spec):
+    return _AllToAll.apply(input_, comm_spec)
 
-    # all gather
-    input_ = input_.contiguous()
-    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]
-    torch.distributed.all_gather(tensor_list, input_, group=process_group)
 
-    # concat
-    output = torch.cat(tensor_list, dim=dim).contiguous()
+def mixgather_forward_split_backward(input_, comm_spec):
+    return _MixGatherForwardMixSplitBackward.apply(input_, comm_spec)
 
-    return output
 
+class CollectiveCommPattern(Enum):
+    GATHER_FWD_SPLIT_BWD = "gather_fwd_split_bwd"
+    ALL2ALL_FWD_ALL2ALL_BWD = "all2all_fwd_all2all_bwd"
+    SPLIT_FWD_GATHER_BWD = "split_fwd_gather_bwd"
+    ALLREDUCE_FWD_IDENTITY_BWD = "all_reduce_fwd_identity_bwd"
+    IDENTITY_FWD_ALLREDUCE_BWD = "identity_fwd_all_reduce_bwd"
+    MIXGATHER_FWD_SPLIT_BWD = "mixgather_fwd_split_bwd"
 
-def _reduce_scatter(input_, dim=1, process_group=None):
-    """Do reduce-scatter operation.
 
-    Args:
-        input_ (`torch.Tensor`): The input tensor from sequence parallel region.
-        dim (int): The dimension to perform reduce-scatter.
-        process_group (`torch.distributed.ProcessGroup`): The process group used for collective communication.
+class CommSpec:
     """
-    world_size = dist.get_world_size(process_group)
-    if world_size == 1:
-        return input_
-
-    # reduce-scatter
-    new_shape = list(input_.shape)
-    assert (
-        new_shape[dim] % dist.get_world_size(process_group) == 0
-    ), f"The dimension to split ({new_shape[dim]}) is not a multiple of tensor parallel size ({dist.get_world_size(process_group)}). "
-    new_shape[dim] = new_shape[dim] // world_size
-    output = torch.empty(new_shape, dtype=input_.dtype, device=input_.device)
-    dist.reduce_scatter(output, input_, group=process_group)
-
-    return output
+    Communication spec is used to record the communication action. It has two main functions:
+    1. Compute the communication cost which will be used in auto parallel solver.
+    2. Convert the communication spec to real action which will be used in runtime.
+    It contains comm_pattern to determine the
+    communication method, sharding_spec to determine the communication size, gather_dim and shard_dim
+    to determine the buffer shape, and logical_process_axis
 
+    Argument:
+        comm_pattern(CollectiveCommPattern): describe the communication method used in this spec.
+        sharding_spec(ShardingSpec): This is sharding spec of the tensor which will join the communication action.
+        gather_dim(int, Optional): The gather_dim of the tensor will be gathered.
+        shard_dim(int, Optional): The shard_dim of the tensor will be sharded.
+        logical_process_axis(Union(int, List[int]), Optional): The mesh_dim to implement the communication action.
+    """
 
-def matmul_with_async_comm(input_, weight, bias, process_group, async_grad_allreduce):
-    return MatmulWithAsyncCommunication.apply(input_, weight, bias, process_group, async_grad_allreduce)
-
-
-def linear_with_async_comm(input_, weight, bias, process_group, async_grad_allreduce):
-    return LinearWithAsyncCommunication.apply(input_, weight, bias, process_group, async_grad_allreduce)
-
-
-def linear_gather_forward_reducescatter_backward(
-    input_, weight, bias, process_group, async_grad_reduce_scatter, dim, overlap
-):
-    return _LinearWithGatherForwardReduceScatterBackward.apply(
-        input_, weight, bias, process_group, async_grad_reduce_scatter, dim, overlap
-    )
-
-
-def linear_reducescatter_forward_gather_backward(input_, process_group, dim):
-    return _LinearWithReduceScatterForwardGatherBackward.apply(input_, process_group, dim)
-
-
-def matmul_gather_forward_reducescatter_backward(
-    input_, weight, bias, process_group, async_grad_reduce_scatter, dim, overlap
-):
-    return _MatmulWithGatherForwardReduceScatterBackward.apply(
-        input_, weight, bias, process_group, async_grad_reduce_scatter, dim, overlap
-    )
-
-
-def gather_forward_split_backward(input_, dim, process_group):
-    return _GatherForwardSplitBackward.apply(input_, dim, process_group)
-
-
-def split_forward_gather_backward(input_, dim, process_group):
-    return _SplitForwardGatherBackward.apply(input_, dim, process_group)
-
+    def __init__(
+        self,
+        comm_pattern,
+        sharding_spec,
+        gather_dim=None,
+        shard_dim=None,
+        logical_process_axis=None,
+        forward_only=False,
+        mix_gather=False,
+    ):
+        self.comm_pattern = comm_pattern
+        self.sharding_spec = sharding_spec
+        self.gather_dim = gather_dim
+        self.shard_dim = shard_dim
+        self.logical_process_axis = logical_process_axis
+        self.forward_only = forward_only
+        if isinstance(self.logical_process_axis, list):
+            if not mix_gather:
+                self.device_mesh = self.sharding_spec.device_mesh.flatten()
+                self.logical_process_axis = 0
+            else:
+                self.device_meshes = self.sharding_spec.device_mesh.flatten_device_meshes
+                self.device_mesh = self.sharding_spec.device_mesh.flatten_device_mesh
+                # Create a new member `logical_process_axes` to distinguish from original flatten
+                self.logical_process_axes = logical_process_axis
+        else:
+            self.device_mesh = self.sharding_spec.device_mesh
 
-def reduce_forward(input_, process_group):
-    return _ReduceForward.apply(input_, process_group)
+    def __repr__(self):
+        res_list = ["CommSpec:("]
+        if self.comm_pattern == CollectiveCommPattern.GATHER_FWD_SPLIT_BWD:
+            res_list.append(f"comm_pattern:GATHER_FWD_SPLIT_BWD, ")
+            res_list.append(f"gather_dim:{self.gather_dim}, ")
+            res_list.append(f"shard_dim:{self.shard_dim}, ")
+            res_list.append(f"logical_process_axis:{self.logical_process_axis})")
+        elif self.comm_pattern == CollectiveCommPattern.ALL2ALL_FWD_ALL2ALL_BWD:
+            res_list.append(f"comm_pattern:ALL2ALL_FWD_ALL2ALL_BWD, ")
+            res_list.append(f"gather_dim:{self.gather_dim}, ")
+            res_list.append(f"shard_dim:{self.shard_dim}, ")
+            res_list.append(f"logical_process_axis: {self.logical_process_axis})")
+        elif self.comm_pattern == CollectiveCommPattern.SPLIT_FWD_GATHER_BWD:
+            res_list.append(f"comm_pattern:SPLIT_FWD_GATHER_BWD, ")
+            res_list.append(f"gather_dim:{self.gather_dim}, ")
+            res_list.append(f"shard_dim:{self.shard_dim}, ")
+            res_list.append(f"logical_process_axis:{self.logical_process_axis})")
+        elif self.comm_pattern == CollectiveCommPattern.ALLREDUCE_FWD_IDENTITY_BWD:
+            res_list.append(f"comm_pattern:ALLREDUCE_FWD_IDENTITY_BWD, ")
+            res_list.append(f"logical_process_axis:{self.logical_process_axis})")
+        elif self.comm_pattern == CollectiveCommPattern.IDENTITY_FWD_ALLREDUCE_BWD:
+            res_list.append(f"comm_pattern:IDENTITY_FWD_ALLREDUCE_BWD, ")
+            res_list.append(f"logical_process_axis:{self.logical_process_axis})")
+        elif self.comm_pattern == CollectiveCommPattern.MIXGATHER_FWD_SPLIT_BWD:
+            res_list.append(f"comm_pattern:MIXGATHER_FWD_SPLIT_BWD, ")
+            res_list.append(f"gather_dim:{self.gather_dim}, ")
+            res_list.append(f"logical_process_axes:{self.logical_process_axes})")
+
+        return "".join(res_list)
+
+    def get_comm_cost(self):
+        """
+        For all_gather, all2all, and all_reduce operation, the formula provided in DeviceMesh with alpha-beta model is used to
+        compute the communication cost.
+        For shard operation, it is an on-chip operation, so the communication cost is zero.
+        """
+        comm_size = reduce(operator.mul, self.sharding_spec.get_sharded_shape_per_device(), 1)
+        cost_dict = {}
+        if self.comm_pattern == CollectiveCommPattern.GATHER_FWD_SPLIT_BWD:
+            forward_communication_cost = self.device_mesh.all_gather_cost(comm_size, self.logical_process_axis)
+            # give a tiny cost to shard
+            backward_communication_cost = 100
+
+        if self.comm_pattern == CollectiveCommPattern.ALL2ALL_FWD_ALL2ALL_BWD:
+            forward_communication_cost = self.device_mesh.all_to_all_cost(comm_size, self.logical_process_axis)
+            # grad should have same shape as input tensor
+            # all to all operation has same logical process axis as forward.
+            backward_communication_cost = self.device_mesh.all_to_all_cost(comm_size, self.logical_process_axis)
+
+        if self.comm_pattern == CollectiveCommPattern.ALLREDUCE_FWD_IDENTITY_BWD:
+            forward_communication_cost = self.device_mesh.all_reduce_cost(comm_size, self.logical_process_axis)
+            backward_communication_cost = 0
+
+        if self.comm_pattern == CollectiveCommPattern.IDENTITY_FWD_ALLREDUCE_BWD:
+            forward_communication_cost = 0
+            backward_communication_cost = self.device_mesh.all_reduce_cost(comm_size, self.logical_process_axis)
+
+        if self.comm_pattern == CollectiveCommPattern.SPLIT_FWD_GATHER_BWD:
+            # give a tiny cost to shard
+            forward_communication_cost = 100
+            backward_communication_cost = self.device_mesh.all_gather_cost(comm_size, self.logical_process_axis)
+
+        if self.comm_pattern == CollectiveCommPattern.MIXGATHER_FWD_SPLIT_BWD:
+            # no need for axis because all devices are used in mix_gather
+            forward_communication_cost = self.device_mesh.mix_gather_cost(comm_size)
+            backward_communication_cost = 100
+
+        if self.forward_only:
+            cost_dict["forward"] = forward_communication_cost
+            cost_dict["backward"] = 0
+            cost_dict["total"] = cost_dict["forward"] + cost_dict["backward"]
+        else:
+            cost_dict["forward"] = forward_communication_cost
+            cost_dict["backward"] = backward_communication_cost
+            cost_dict["total"] = cost_dict["forward"] + cost_dict["backward"]
+
+        return cost_dict
+
+    def covert_spec_to_action(self, tensor):
+        """
+        Convert CommSpec into runtime action, implement real collection communication to target tensor.
+        The collection communication action is directed by the CommSpec.
+
+        Argument:
+            tensor(torch.Tensor): Tensor stored in each device, which could be different in different ranks.
+        """
+        if self.comm_pattern in pattern_to_func_dict:
+            tensor = pattern_to_func_dict[self.comm_pattern](tensor, self)
+        else:
+            tensor = tensor
+        return tensor
 
 
-def reduce_backward(input_, process_group):
-    return _ReduceBackward.apply(input_, process_group)
+pattern_to_func_dict = {
+    CollectiveCommPattern.GATHER_FWD_SPLIT_BWD: gather_forward_split_backward,
+    CollectiveCommPattern.ALL2ALL_FWD_ALL2ALL_BWD: all_to_all,
+    CollectiveCommPattern.SPLIT_FWD_GATHER_BWD: split_forward_gather_backward,
+    CollectiveCommPattern.ALLREDUCE_FWD_IDENTITY_BWD: reduce_input,
+    CollectiveCommPattern.IDENTITY_FWD_ALLREDUCE_BWD: reduce_grad,
+    CollectiveCommPattern.MIXGATHER_FWD_SPLIT_BWD: mixgather_forward_split_backward,
+}
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/layer/dropout.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/dropout.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/layer/embedding.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/embedding.py`

 * *Files 13% similar despite different names*

```diff
@@ -17,18 +17,18 @@
     is_distributed_tensor,
     shard_colwise,
     shard_rowwise,
     sharded_tensor_to_existing_param,
 )
 
 from ._operation import gather_forward_split_backward, reduce_forward
-from .parallel_module import ParallelModule
+from .parallel_module import PaddingParallelModule, ParallelModule
 from .utils import create_randomizer_with_offset
 
-__all__ = ["Embedding1D", "VocabParallelEmbedding1D"]
+__all__ = ["Embedding1D", "VocabParallelEmbedding1D", "PaddingEmbedding"]
 
 
 class Embedding1D(ParallelModule):
     r"""Embedding for 1D parallelism.
 
     Args:
         num_embeddings (int): number of embeddings.
@@ -157,15 +157,88 @@
         if self.gather_output:
             output = gather_forward_split_backward(output_parallel, dim=-1, process_group=self.process_group)
             return output
         else:
             return output_parallel
 
 
-class VocabParallelEmbedding1D(ParallelModule):
+class PaddingEmbedding(PaddingParallelModule):
+    def __init__(
+        self,
+        num_embeddings: int,
+        embedding_dim: int,
+        padding_idx: int = None,
+        dtype: torch.dtype = None,
+        device: torch.device = None,
+        weight: Optional[nn.Parameter] = None,
+        make_vocab_size_divisible_by: int = 64,
+        *args,
+        **kwargs,
+    ):
+        self.num_embeddings = num_embeddings
+        self.embedding_dim = embedding_dim
+        self.embed_args = args
+        self.embed_kwargs = kwargs
+        self.padding_idx = padding_idx
+        if num_embeddings % make_vocab_size_divisible_by != 0:
+            self.num_embeddings = (
+                num_embeddings + make_vocab_size_divisible_by - (num_embeddings % make_vocab_size_divisible_by)
+            )
+        # create weight and bias
+        if weight is None:
+            factory_kwargs = {"device": device, "dtype": dtype}
+            weight = nn.Parameter(torch.empty((num_embeddings, self.embedding_dim), **factory_kwargs))
+        else:
+            weight.data = weight.data.to(device=device, dtype=dtype)
+
+        super().__init__(self.num_embeddings, num_embeddings, weight)
+
+        if weight is None:
+            self.reset_parameters()
+
+    def reset_parameters(self) -> None:
+        init.normal_(self.weight)
+        self._fill_padding_idx_with_zero()
+
+    def _fill_padding_idx_with_zero(self) -> None:
+        if self.padding_idx is not None:
+            with torch.no_grad():
+                self.weight[self.padding_idx].fill_(0)
+
+    def forward(self, input: Tensor) -> Tensor:
+        return F.embedding(input, self.weight, self.padding_idx, *self.embed_args, **self.embed_kwargs)
+
+    @staticmethod
+    def from_native_module(
+        module: nn.Embedding, process_group: Union[ProcessGroup, List[ProcessGroup]], *args, **kwargs
+    ) -> PaddingParallelModule:
+        r"""
+        Convert a native pytorch embedding module to a parallel module.
+        """
+        LazyInitContext.materialize(module)
+        # get the origin attributes
+        num_embeddings = module.num_embeddings
+        embedding_dim = module.embedding_dim
+        padding_idx = module.padding_idx
+        device = module.weight.device
+        # create the parallel module
+        padding_embedding = PaddingEmbedding(
+            num_embeddings=num_embeddings,
+            embedding_dim=embedding_dim,
+            padding_idx=padding_idx,
+            device=device,
+            weight=module.weight,
+            *args,
+            **kwargs,
+        )
+
+        return padding_embedding
+
+
+class VocabParallelEmbedding1D(PaddingParallelModule):
     r"""Embedding parallelized in the vocabulary dimension.
 
     Args:
         num_embeddings (int): number of embeddings.
         embedding_dim (int): dimension of embedding.
         padding_idx (int, optional): If specified, the entries at padding_idx do not contribute to the gradient;
             therefore, the embedding vector at padding_idx is not updated during training,
@@ -197,57 +270,65 @@
         embedding_dim: int,
         padding_idx: int = None,
         dtype: torch.dtype = None,
         device: torch.device = None,
         process_group: ProcessGroup = None,
         weight: Optional[nn.Parameter] = None,
         weight_initializer: Callable = init.normal_(),
+        make_vocab_size_divisible_by: int = 64,
         *args,
         **kwargs,
     ):
-        super().__init__()
         self.num_embeddings = num_embeddings
         self.embedding_dim = embedding_dim
         self.embed_args = args
         self.embed_kwargs = kwargs
         self.process_group = process_group
 
         tensor_parallel_size = dist.get_world_size(group=process_group)
         tensor_parallel_rank = dist.get_rank(group=process_group)
 
-        self.num_embeddings_per_partition = divide(num_embeddings, tensor_parallel_size)
-        self.num_embeddings = self.num_embeddings_per_partition
+        # generate weight and bias
+        if weight is None:
+            factory_kwargs = {"device": device, "dtype": dtype}
+            weight = nn.Parameter(torch.empty((num_embeddings, self.embedding_dim), **factory_kwargs))
+        else:
+            weight.data = weight.data.to(device=device, dtype=dtype)
+
+        # calculate new padding size
+        multiple = make_vocab_size_divisible_by * tensor_parallel_size
+        if num_embeddings % multiple != 0:
+            self.num_embeddings = num_embeddings + multiple - (num_embeddings % multiple)
+
+        # resize vocabulary size
+        super().__init__(self.num_embeddings, num_embeddings, weight)
+
+        # deal with tensor parallelism
+        self.num_embeddings_per_partition = divide(self.num_embeddings, tensor_parallel_size)
         self.vocab_start_index = tensor_parallel_rank * self.num_embeddings_per_partition
         self.vocab_end_index = self.vocab_start_index + self.num_embeddings_per_partition
 
         # padding index
         self.padding_idx = self._select_padding_idx(padding_idx)
 
         # offset the seed with randomizer index and rank
         seed = torch.random.initial_seed()
         self.randomizer = create_randomizer_with_offset(seed, process_group=self.process_group)
 
-        # parameter
-        if weight is None:
-            factory_kwargs = {"device": device, "dtype": dtype}
-            self.weight = nn.Parameter(torch.empty((num_embeddings, self.embedding_dim), **factory_kwargs))
-        else:
-            weight.data = weight.data.to(device=device, dtype=dtype)
-            self.weight = weight
         if not is_distributed_tensor(self.weight):
             sharded_weight = shard_rowwise(self.weight.data, process_group)
             sharded_tensor_to_existing_param(sharded_weight, self.weight)
 
         if weight is None:
             self.reset_parameters(weight_initializer)
 
     @staticmethod
     def from_native_module(
         module: nn.Embedding, process_group: Union[ProcessGroup, List[ProcessGroup]], *args, **kwargs
-    ) -> ParallelModule:
+    ) -> PaddingParallelModule:
         r"""
         Convert a native pytorch embedding module to a parallel module.
         """
         LazyInitContext.materialize(module)
         # get the origin attributes
         num_embeddings = module.num_embeddings
         embedding_dim = module.embedding_dim
@@ -299,18 +380,16 @@
 
     def forward(self, input_: Tensor) -> Tensor:
         # Build the mask.
         input_mask = (input_ < self.vocab_start_index) | (input_ >= self.vocab_end_index)
         # Mask the input.
         masked_input = input_.clone() - self.vocab_start_index
         masked_input[input_mask] = 0
-
         output_parallel = F.embedding(
             masked_input, self.weight, self.padding_idx, *self.embed_args, **self.embed_kwargs
         )
-
         # Mask the output embedding.
         embedding_output = output_parallel.clone()
         embedding_output[input_mask, :] = 0.0
         # Reduce across all the model parallel GPUs.
         output = reduce_forward(embedding_output, self.process_group)
         return output
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/layer/linear.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/linear.py`

 * *Files 26% similar despite different names*

```diff
@@ -19,22 +19,24 @@
     is_distributed_tensor,
     shard_colwise,
     shard_rowwise,
     sharded_tensor_to_existing_param,
 )
 
 from ._operation import (
+    gather_forward_reducescatter_backward,
     gather_forward_split_backward,
     linear_gather_forward_reducescatter_backward,
     linear_reducescatter_forward_gather_backward,
     linear_with_async_comm,
     reduce_forward,
+    reducescatter_forward_gather_backward,
     split_forward_gather_backward,
 )
-from .parallel_module import ParallelModule
+from .parallel_module import PaddingParallelModule, ParallelModule
 from .utils import create_randomizer_with_offset
 
 __all__ = ["Linear1D_Col", "Linear1D_Row"]
 
 
 class Linear1D_Col(ParallelModule):
     r"""Linear layer with column parallelism.
@@ -70,30 +72,31 @@
         in_features: int,
         out_features: int,
         bias: bool = True,
         dtype: torch.dtype = None,
         device: torch.device = None,
         process_group: ProcessGroup = None,
         gather_output: bool = False,
-        seq_parallel: bool = False,
+        seq_parallel_mode: str = None,
         seq_parallel_dim: int = 1,
         overlap: torch.cuda.Stream = None,
         skip_bias_add: bool = False,
         weight: Optional[Parameter] = None,
         bias_: Optional[Parameter] = None,
         weight_initializer: Callable = init.kaiming_uniform_(a=math.sqrt(5)),
         bias_initializer: Callable = init.xavier_uniform_(a=1, scale=1),
+        **kwargs,
     ):
-        super().__init__()
+        super().__init__(weight=weight, bias_=bias_, **kwargs)
 
         # Keep input parameters
         self.in_features = in_features
         self.out_features = out_features
         self.gather_output = gather_output
-        self.seq_parallel = seq_parallel
+        self.seq_parallel_mode = seq_parallel_mode
         self.seq_parallel_dim = seq_parallel_dim
         self.overlap = overlap
         self.skip_bias_add = skip_bias_add
         self.device = device
         self.process_group = process_group
 
         if skip_bias_add and not bias:
@@ -112,14 +115,15 @@
         # Parameters.
         if weight is None:
             factory_kwargs = {"device": device, "dtype": dtype}
             self.weight = Parameter(torch.empty(self.out_features, self.in_features, **factory_kwargs))
         else:
             weight.data = weight.data.to(device=device, dtype=dtype)
             self.weight = weight
+
         if not is_distributed_tensor(self.weight):
             sharded_weight = shard_rowwise(self.weight.data, self.process_group)
             sharded_tensor_to_existing_param(sharded_weight, self.weight)
 
         if bias:
             if bias_ is None:
                 self.bias = Parameter(torch.empty(self.out_features, **factory_kwargs))
@@ -134,15 +138,15 @@
 
         if weight is None:
             # init weights
             self.reset_parameters(weight_initializer, bias_initializer)
 
     @staticmethod
     def from_native_module(
-        module: nn.Linear, process_group: Union[ProcessGroup, List[ProcessGroup]], *args, **kwargs
+        module: nn.Linear, process_group: Union[ProcessGroup, List[ProcessGroup]], **kwargs
     ) -> ParallelModule:
         r"""
         Convert a native PyTorch linear layer to a parallelized linear layer.
         """
         LazyInitContext.materialize(module)
         # get the attributes
         in_features = module.in_features
@@ -167,15 +171,14 @@
             in_features=in_features,
             out_features=out_features,
             bias=bias,
             device=device,
             process_group=process_group,
             weight=module.weight,
             bias_=module.bias,
-            *args,
             **kwargs,
         )
 
         return linear_1d
 
     def reset_parameters(self, weight_initializer, bias_initializer) -> None:
         with self.randomizer.fork_rng(enable_cpu=True):
@@ -192,20 +195,26 @@
         )
 
         # Set up backprop all-reduce.
         input_parallel = input_
 
         # Matrix multiply.
         bias = self.bias if not self.skip_bias_add else None
-        if self.seq_parallel:
+
+        if self.seq_parallel_mode is None:
+            output_parallel = linear_with_async_comm(input_parallel, self.weight, bias, self.process_group, True)
+        elif self.seq_parallel_mode == "split_gather":
+            input_parallel = gather_forward_reducescatter_backward(
+                input_parallel, self.process_group, self.seq_parallel_dim
+            )
+            output_parallel = linear_with_async_comm(input_parallel, self.weight, bias, self.process_group, False)
+        elif self.seq_parallel_mode == "ring":
             output_parallel = linear_gather_forward_reducescatter_backward(
-                input_parallel, self.weight, bias, self.process_group, True, self.seq_parallel_dim, self.overlap
+                input_parallel, self.weight, bias, self.process_group, True, self.seq_parallel_dim, self.overlap, True
             )
-        else:
-            output_parallel = linear_with_async_comm(input_parallel, self.weight, bias, self.process_group, True)
 
         if self.gather_output:
             # All-gather across the partitions.
             output = gather_forward_split_backward(output_parallel, dim=-1, process_group=self.process_group)
         else:
             output = output_parallel
 
@@ -221,15 +230,16 @@
     Args:
         in_features (int): size of each input sample.
         out_features (int): size of each output sample.
         bias (bool, optional): If set to ``False``, the layer will not learn an additive bias, defaults to ``True``.
         dtype (`torch.dtype`): The dtype of parameters, defaults to None.
         parallel_input (bool): If set to ``True``, it's assumed that the input is split, defaults to False.
         process_group (`torch.distributed.ProcessGroup`): The process group to be used for weight sharding and communication, defaults to None.
-        seq_parallel (`bool`): If set to ``True``, it will use sequence parallel, defaults to False.
+        seq_parallel_mode (`str`): The type of sp mode, it will use sequence parallel when `seq_parallel_mode` is not None. Defaults to None.
+        seq_parallel_dim (`int`): Which dim will sequence parallelism split and gather the sequence.
         skip_bias_add (bool): If set to ``True``, it will skip bias add for linear layer,
             which is preserved for kernel fusion, defaults to False
         weight_initializer (:class:`typing.Callable`, optional):
             The initializer of weight, defaults to kaiming uniform initializer.
         bias_initializer (:class:`typing.Callable`, optional):
             The initializer of bias, defaults to xavier uniform initializer.
 
@@ -241,15 +251,15 @@
         self,
         in_features: int,
         out_features: int,
         bias: bool = True,
         dtype: torch.dtype = None,
         device: torch.device = None,
         process_group: ProcessGroup = None,
-        seq_parallel: bool = False,
+        seq_parallel_mode: str = None,
         seq_parallel_dim: int = 1,
         parallel_input: bool = True,
         skip_bias_add: bool = False,
         weight: Optional[Parameter] = None,
         bias_: Optional[Parameter] = None,
         weight_initializer: Callable = init.kaiming_uniform_(a=math.sqrt(5)),
         bias_initializer: Callable = init.xavier_uniform_(a=1, scale=1),
@@ -261,15 +271,15 @@
 
         # Keep input parameters
         self.in_features = in_features
         self.out_features = out_features
         self.parallel_input = parallel_input
         self.skip_bias_add = skip_bias_add
         self.process_group = process_group
-        self.seq_parallel = seq_parallel
+        self.seq_parallel_mode = seq_parallel_mode
         self.seq_parallel_dim = seq_parallel_dim
         self.num_partitions = dist.get_world_size(self.process_group)
 
         if skip_bias_add and not bias:
             raise ValueError("cannot skip bias addition if bias is None")
 
         # offset the seed with randomizer index and rank
@@ -309,15 +319,15 @@
 
         if weight is None:
             with self.randomizer.fork_rng(enable_cpu=True):
                 self.reset_parameters(weight_initializer, bias_initializer)
 
     @staticmethod
     def from_native_module(
-        module: nn.Linear, process_group: Union[ProcessGroup, List[ProcessGroup]], *args, **kwargs
+        module: nn.Linear, process_group: Union[ProcessGroup, List[ProcessGroup]], **kwargs
     ) -> ParallelModule:
         r"""
         Convert a native PyTorch linear layer to a parallelized linear layer.
         """
         LazyInitContext.materialize(module)
         # get the attributes
         in_features = module.in_features
@@ -343,15 +353,14 @@
             in_features=in_features,
             out_features=out_features,
             bias=bias,
             device=device,
             process_group=process_group,
             weight=module.weight,
             bias_=module.bias,
-            *args,
             **kwargs,
         )
 
         return linear_1d
 
     def chunk_weight(self):
         self.weight_list = torch.chunk(self.weight, self.stream_chunk_num, dim=0)
@@ -399,26 +408,242 @@
                 handle_list = []
                 for i in range(self.stream_chunk_num):
                     output_parallel_list[i] = F.linear(input_, self.weight_list[i])
                     handle = torch.distributed.all_reduce(
                         output_parallel_list[i], group=self.process_group, async_op=True
                     )
                     handle_list.append(handle)
-                    # output_parallel_list[i] = reduce_input(output_parallel_list[i], ParallelMode.PARALLEL_1D)
                 for handle in handle_list:
                     handle.wait()
                 output = torch.cat(output_parallel_list, dim=-1)
         else:
-            output_parallel = linear_with_async_comm(input_, self.weight, None, None, False)
-            if self.seq_parallel:
-                output = linear_reducescatter_forward_gather_backward(
+            if self.seq_parallel_mode is None:
+                output_parallel = linear_with_async_comm(input_, self.weight, None, self.process_group, False)
+                output = reduce_forward(output_parallel, self.process_group)
+            elif self.seq_parallel_mode == "split_gather":
+                output_parallel = linear_with_async_comm(input_, self.weight, None, self.process_group, False)
+                output = reducescatter_forward_gather_backward(
                     output_parallel, self.process_group, self.seq_parallel_dim
                 )
-            else:
-                output = reduce_forward(output_parallel, self.process_group)
+            elif self.seq_parallel_mode == "ring":
+                output = linear_reducescatter_forward_gather_backward(
+                    input_,
+                    self.weight,
+                    process_group=self.process_group,
+                    dim=self.seq_parallel_dim,
+                    ring=True,
+                )
 
         if not self.skip_bias_add:
             if self.bias is not None:
                 output = output + self.bias
             return output
         else:
             return output, self.bias
+
+
+class PaddingLMHead(PaddingParallelModule):
+    def __init__(
+        self,
+        in_features: int,
+        out_features: int,
+        bias: bool = True,
+        dtype: torch.dtype = None,
+        device: torch.device = None,
+        weight: Optional[Parameter] = None,
+        bias_: Optional[Parameter] = None,
+        make_vocab_size_divisible_by: int = 64,
+        weight_initializer: Callable = init.kaiming_uniform_(a=math.sqrt(5)),
+        bias_initializer: Callable = init.xavier_uniform_(a=1, scale=1),
+    ):
+        # Keep input parameters
+        self.in_features = in_features
+        self.out_features = out_features
+
+        if out_features % make_vocab_size_divisible_by != 0:
+            self.out_features = (
+                out_features + make_vocab_size_divisible_by - (out_features % make_vocab_size_divisible_by)
+            )
+        if weight is None:
+            factory_kwargs = {"device": device, "dtype": dtype}
+            weight = Parameter(torch.empty(out_features, self.in_features, **factory_kwargs))
+        else:
+            weight.data = weight.data.to(device=device, dtype=dtype)
+
+        if bias:
+            if bias_ is None:
+                self.bias = Parameter(torch.empty(out_features, **factory_kwargs))
+            else:
+                bias_.data = bias_.data.to(device=device, dtype=dtype)
+        else:
+            bias_ = None
+
+        # resize embeddings
+        super().__init__(self.out_features, out_features, weight, bias_)
+
+        if weight is None:
+            self.reset_parameters(weight_initializer, bias_initializer)
+
+    def reset_parameters(self, weight_initializer, bias_initializer) -> None:
+        fan_in, fan_out = self.in_features, self.out_features
+        weight_initializer(self.weight, fan_in=fan_in, fan_out=fan_out)
+        if self.bias is not None:
+            bias_initializer(self.bias, fan_in=fan_in)
+
+    @staticmethod
+    def from_native_module(
+        module: nn.Linear, process_group: Union[ProcessGroup, List[ProcessGroup]], **kwargs
+    ) -> PaddingParallelModule:
+        r"""
+        Convert a native PyTorch linear layer to a parallelized linear layer.
+        """
+        LazyInitContext.materialize(module)
+        # get the attributes
+        in_features = module.in_features
+        out_features = module.out_features
+        bias = module.bias is not None
+        device = module.weight.device
+        # ensure only one process group is passed
+
+        lm_head_linear = PaddingLMHead(
+            in_features=in_features,
+            out_features=out_features,
+            bias=bias,
+            device=device,
+            weight=module.weight,
+            bias_=module.bias,
+            **kwargs,
+        )
+
+        return lm_head_linear
+
+    def forward(self, input: Tensor) -> Tensor:
+        output = F.linear(input, self.weight, self.bias)
+        output = output[..., : self.old_num_embeddings]
+        return output
+
+
+class VocabParallelLMHead1D(Linear1D_Col, PaddingParallelModule):
+    r"""Linear layer with column parallelism.
+
+    The linear layer is defined as :math:`Y = XA + b`. A is parallelized along
+    its second dimension as :math:`A = [A_1, ..., A_p]`.
+
+    Args:
+        in_features (int): size of each input sample.
+        out_features (int): size of each output sample.
+        bias (bool, optional): If set to ``False``, the layer will not learn an additive bias, defaults to ``True``.
+        dtype (`torch.dtype`): The dtype of parameters, defaults to None.
+        device (`torch.device`): The device of parameters, defaults to None.
+        process_group (`torch.distributed.ProcessGroup`): The process group to be used for weight sharding and communication, defaults to None.
+        gather_output (bool, optional): If true, call all-gather on output and make Y available
+                    to all GPUs, otherwise, every GPU will have its output
+                    which is :math:`Y_i = XA_i`, defaults to False
+        seq_parallel (`bool`): If set to ``True``, it will use sequence parallel, defaults to False.
+        overlap (`bool`): If set to ``True``, it will overlap input all-gather with gradient computation during backward, defaults to False.
+        skip_bias_add (bool): If set to ``True``, it will skip bias add for linear layer,
+            which is preserved for kernel fusion, defaults to False
+        weight_initializer (`typing.Callable`):
+            The initializer of weight, defaults to kaiming uniform initializer.
+        bias_initializer (`typing.Callable`):
+            The initializer of bias, defaults to xavier uniform initializer.
+
+    More details about ``initializer`` please refer to
+    `init <https://github.com/hpcaitech/ColossalAI/blob/main/colossalai/nn/init.py>`_.
+    """
+
+    def __init__(
+        self,
+        in_features: int,
+        out_features: int,
+        bias: bool = True,
+        dtype: torch.dtype = None,
+        device: torch.device = None,
+        process_group: ProcessGroup = None,
+        weight: Optional[Parameter] = None,
+        bias_: Optional[Parameter] = None,
+        make_vocab_size_divisible_by: int = 64,
+        **kwargs,
+    ):
+        # create weight and bias
+        if weight is None:
+            factory_kwargs = {"device": device, "dtype": dtype}
+            weight = Parameter(torch.empty(out_features, self.in_features, **factory_kwargs))
+        if bias:
+            if bias_ is None:
+                bias_ = Parameter(torch.empty(out_features, **factory_kwargs))
+        else:
+            bias_ = None
+
+        # calculate new vocab size
+        self.tensor_parallel_size = dist.get_world_size(group=process_group)
+        new_out_features = out_features
+        multiple = make_vocab_size_divisible_by * self.tensor_parallel_size
+        if out_features % multiple != 0:
+            new_out_features = out_features + multiple - (out_features % multiple)
+
+        super().__init__(
+            in_features=in_features,
+            out_features=new_out_features,
+            bias=bias,
+            device=device,
+            process_group=process_group,
+            weight=weight,
+            bias_=bias_,
+            **kwargs,
+            new_num_embeddings=new_out_features,
+            old_num_embeddings=out_features,
+        )
+        # get the length of valid embeddings
+        tp_rank = dist.get_rank(process_group)
+        partition_size = self.new_num_embeddings // dist.get_world_size(process_group)
+        if self.old_num_embeddings >= (tp_rank + 1) * partition_size:
+            self.num_valid_embeddings_local = partition_size
+        elif self.old_num_embeddings >= tp_rank * partition_size:
+            self.num_valid_embeddings_local = self.old_num_embeddings - tp_rank * partition_size
+        else:
+            self.num_valid_embeddings_local = 0
+
+    @staticmethod
+    def from_native_module(
+        module: nn.Linear, process_group: Union[ProcessGroup, List[ProcessGroup]], **kwargs
+    ) -> PaddingParallelModule:
+        r"""
+        Convert a native PyTorch linear layer to a parallelized linear layer.
+        """
+        LazyInitContext.materialize(module)
+        # get the attributes
+        in_features = module.in_features
+        out_features = module.out_features
+        bias = module.bias is not None
+        device = module.weight.device
+
+        lm_head_linear = VocabParallelLMHead1D(
+            in_features=in_features,
+            out_features=out_features,
+            bias=bias,
+            device=device,
+            process_group=process_group,
+            weight=module.weight,
+            bias_=module.bias,
+            **kwargs,
+        )
+
+        return lm_head_linear
+
+    def forward(self, input_: Tensor) -> Tuple[Tensor, Tensor]:
+        # get forward output
+        if self.skip_bias_add:
+            output, bias = super().forward(input_)
+        else:
+            output = super().forward(input_)
+
+        # delete the padding of output
+        if self.gather_output:
+            output = output[..., : self.old_num_embeddings]
+        else:
+            output = output[..., : self.num_valid_embeddings_local]
+
+        # return
+        if self.skip_bias_add:
+            return output, bias
+        return output
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/layer/loss.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/loss.py`

 * *Files 14% similar despite different names*

```diff
@@ -11,15 +11,22 @@
     Overwrite the forward and backward function to calculate the cross entropy loss before gather
 
     Args:
         Function (:class:`torch.autograd.Function`): default
     """
 
     @staticmethod
-    def forward(ctx, vocab_logits: torch.Tensor, target: torch.Tensor, ignore_index: int, process_group: ProcessGroup):
+    def forward(
+        ctx,
+        vocab_logits: torch.Tensor,
+        target: torch.Tensor,
+        ignore_index: int,
+        process_group: ProcessGroup,
+        vocab_size: int,
+    ):
         r"""
         Calculate the cross entropy loss before gather, the origin loss function is as follows:
         loss = -log(exp(x[class])/sum(exp(x[i]))
         and can be rewrite as:
         loss = log(sum(exp(x[i])) - x[class]
 
         To avoid the `nan` of log(sum(exp(x[i]))), we minus the max of x[i]
@@ -37,31 +44,38 @@
         logits_max = torch.max(vocab_logits, dim=-1)[0]
         dist.all_reduce(logits_max, op=dist.ReduceOp.MAX, group=process_group)
 
         # minus the max to avoid the result of sum of exp is too large and the log is nan
         vocab_logits = vocab_logits - logits_max.unsqueeze(dim=-1)
 
         # mask the target in the local device
-        partition_vocab_size = vocab_logits.size()[-1]
         rank = dist.get_rank(group=process_group)
         world_size = dist.get_world_size(group=process_group)
-        global_vocab_size = partition_vocab_size * world_size
+        if vocab_size == None:
+            partition_vocab_size = vocab_logits.size()[-1]
+            global_vocab_size = partition_vocab_size * world_size
+        else:
+            global_vocab_size = vocab_size
+            partition_vocab_size = global_vocab_size // world_size
 
         # [down, up) => false, other device and -100 => true
         delta = (global_vocab_size + world_size - 1) // world_size
         down_threshold = rank * delta
         up_threshold = down_threshold + delta
+        if up_threshold > global_vocab_size:
+            up_threshold = global_vocab_size
         mask = (target < down_threshold) | (target >= up_threshold)
         masked_target = target.clone() - down_threshold
         masked_target[mask] = 0
 
         # reshape the logits and target
         # reshape the vocab_logits to [bath_size * seq_len, vocab_size]
         # reshape the labels to [bath_size * seq_len]
-        logits_2d = vocab_logits.view(-1, partition_vocab_size)
+        self_vocab_size = vocab_logits.size()[-1]
+        logits_2d = vocab_logits.view(-1, self_vocab_size)
         masked_target_1d = masked_target.view(-1)
 
         # extract the x[class] and set the x[other device] to zero
         pred_logits_1d = logits_2d[
             torch.arange(start=0, end=logits_2d.shape[0], device=logits_2d.device), masked_target_1d
         ]
         pred_logits_1d = pred_logits_1d.clone().contiguous()
@@ -100,14 +114,18 @@
         partion_vocab_size = grad_logits.shape[-1]
         grad_logits_2d = grad_logits.view(-1, partion_vocab_size)
 
         update = 1.0 - mask.view(-1).float()
         grad_logits_2d[torch.arange(0, grad_logits_2d.shape[0]), masked_target_1d] -= update
 
         grad_logits.mul_(grad_output.unsqueeze(dim=-1))
-        return grad_logits, None, None, None
+        return grad_logits, None, None, None, None
 
 
 def cross_entropy_1d(
-    vocab_logits: torch.Tensor, labels: torch.Tensor, ignore_index: int = -100, process_group: ProcessGroup = None
+    vocab_logits: torch.Tensor,
+    labels: torch.Tensor,
+    ignore_index: int = -100,
+    process_group: ProcessGroup = None,
+    vocab_size: int = None,
 ) -> torch.Tensor:
-    return DistCrossEntropy.apply(vocab_logits, labels, ignore_index, process_group)
+    return DistCrossEntropy.apply(vocab_logits, labels, ignore_index, process_group, vocab_size)
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/layer/normalization.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/normalization.py`

 * *Files 2% similar despite different names*

```diff
@@ -221,15 +221,21 @@
         if use_fast_ln:
             if EnableFastLayerNorm:
                 ApexFusedLayerNorm = FastLayerNormWithHook
             else:
                 # fall back to the normal fused layernorm is not built
                 ApexFusedLayerNorm = FusedLayerNormWithHook
         else:
-            ApexFusedLayerNorm = FusedLayerNormWithHook
+            try:
+                ApexFusedLayerNorm = FusedLayerNormWithHook
+            except NameError:
+                warnings.warn(
+                    "Please install Apex from source to use fused kernels, or set self.enable_fused_normalization = False. Using vanilla layernorm instead."
+                )
+                return module
 
         layernorm = (
             ApexFusedLayerNorm(normalized_shape, eps=eps, elementwise_affine=elementwise_affine).to(dtype).to(device)
         )
         layernorm.weight = module.weight
         layernorm.bias = module.bias
 
@@ -271,27 +277,24 @@
             pass
         except ImportError:
             raise ImportError(
                 "Please install apex from source (https://github.com/NVIDIA/apex) to use the fused RMS normalization kernel"
             )
 
         LazyInitContext.materialize(module)
-        # to check if it is huggingface LlamaRMSNorm or MistralRMSNorm
-        if module.__class__.__name__ in ["LlamaRMSNorm", "MistralRMSNorm"]:
-            normalized_shape = module.weight.shape[0]
-            eps = module.variance_epsilon
-            elementwise_affine = True
-        else:
-            # get the attributes of the module
-            normalized_shape = module.normalized_shape
-            eps = module.eps
-            elementwise_affine = module.elementwise_affine
+
+        # try to get normalized_shape, eps, elementwise_affine from the module
+        normalized_shape = getattr(module, "normalized_shape", module.weight.shape[0])
+        eps = module.variance_epsilon if hasattr(module, "variance_epsilon") else module.eps
+        elementwise_affine = getattr(module, "elementwise_affine", True)
 
         rmsnorm = FusedRMSNormWithHook(
-            normalized_shape=normalized_shape, eps=eps, elementwise_affine=elementwise_affine
+            normalized_shape=normalized_shape,
+            eps=eps,
+            elementwise_affine=elementwise_affine,
         )
 
         rmsnorm.weight = module.weight
 
         if sp_partial_derived:
             # Since gradients are computed using only a subset of the data,
             # aggregation of these gradients is necessary during backpropagation.
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/layer/qkv_fused_linear.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/qkv_fused_linear.py`

 * *Files 5% similar despite different names*

```diff
@@ -21,20 +21,20 @@
     is_distributed_tensor,
     shard_rowwise,
     sharded_tensor_to_existing_param,
 )
 
 from ._operation import (
     gather_forward_split_backward,
-    linear_reducescatter_forward_gather_backward,
     linear_with_async_comm,
     matmul_gather_forward_reducescatter_backward,
     matmul_with_async_comm,
     reduce_backward,
     reduce_forward,
+    reducescatter_forward_gather_backward,
     split_forward_gather_backward,
 )
 from .parallel_module import ParallelModule
 from .utils import create_randomizer_with_offset
 
 __all__ = ["FusedLinear1D_Col", "FusedLinear1D_Row", "GPT2FusedLinearConv1D_Col", "GPT2FusedLinearConv1D_Row"]
 
@@ -146,15 +146,15 @@
         in_features (int): size of each input sample.
         out_features (int): size of each output sample.
         bias (bool, optional): If set to ``False``, the layer will not learn an additive bias, defaults to ``True``.
         dtype (`torch.dtype`): The dtype of parameters, defaults to None.
         device (`torch.device`): The device of parameters, defaults to None.
         n_fused (int): The number items fused, defaults to 3 (QKV).
         process_group (`torch.distributed.ProcessGroup`): The process group to be used for weight sharding and communication, defaults to None.
-        seq_parallel (`bool`): If set to ``True``, it will use sequence parallel, defaults to False.
+        seq_parallel_mode (str): If set to ``None``, it will not use sequence parallel, otherwise will use corresponding mode of sequence parallel, defaults to None.
         gather_output (bool, optional): If true, call all-gather on output and make Y available
                     to all GPUs, otherwise, every GPU will have its output
                     which is :math:`Y_i = XA_i`, defaults to False
         skip_bias_add (bool): If set to ``True``, it will skip bias add for linear layer,
             which is preserved for kernel fusion, defaults to False
         weight_initializer (`typing.Callable`):
             The initializer of weight, defaults to kaiming uniform initializer.
@@ -171,30 +171,30 @@
         out_features: int,
         bias: bool = True,
         dtype: torch.dtype = None,
         device: torch.device = None,
         process_group: ProcessGroup = None,
         async_communication: bool = False,
         gather_output: bool = False,
-        seq_parallel: bool = False,
+        seq_parallel_mode: str = None,
         overlap: bool = False,
         skip_bias_add: bool = False,
         n_fused: int = 3,
         weight: Optional[Parameter] = None,
         bias_: Optional[Parameter] = None,
         weight_initializer: Callable = init.kaiming_uniform_(a=math.sqrt(5)),
         bias_initializer: Callable = init.xavier_uniform_(a=1, scale=1),
     ):
         super().__init__()
 
         # Keep input parameters
         self.in_features = in_features
         self.out_features = out_features
         self.gather_output = gather_output
-        self.seq_parallel = seq_parallel
+        self.seq_parallel_mode = seq_parallel_mode
         self.overlap = overlap
         self.skip_bias_add = skip_bias_add
         self.device = device
         self.n_fused = n_fused
         self.process_group = process_group
         self.async_communication = async_communication
 
@@ -308,25 +308,30 @@
         ), "Invalid shapes in Linear1D_Col forward: input={}, weight={}. Expected last dim of input {}.".format(
             input_.shape, self.weight.shape, self.weight.shape[-1]
         )
 
         # Matrix multiply.
         bias = self.bias if not self.skip_bias_add else None
 
-        if self.seq_parallel:
-            input_parallel = input_
-            output_parallel = matmul_gather_forward_reducescatter_backward(
-                input_parallel, self.weight, bias, self.process_group, True, 1, self.overlap
-            )
-        else:
+        if self.seq_parallel_mode is None:
             # Set up backprop all-reduce.
             input_parallel = reduce_backward(input_, self.process_group)
             output_parallel = matmul_with_async_comm(
                 input_parallel, self.weight, bias, self.process_group, self.async_communication
             )
+        elif self.seq_parallel_mode == "split_gather":
+            input_parallel = input_
+            output_parallel = matmul_gather_forward_reducescatter_backward(
+                input_parallel, self.weight, bias, self.process_group, True, 1, self.overlap
+            )
+        elif self.seq_parallel_mode == "ring":
+            input_parallel = input_
+            output_parallel = matmul_gather_forward_reducescatter_backward(
+                input_parallel, self.weight, bias, self.process_group, True, 1, self.overlap, True
+            )
 
         if self.gather_output:
             # All-gather across the partitions.
             output = gather_forward_split_backward(output_parallel, dim=-1, process_group=self.process_group)
         else:
             output = output_parallel
 
@@ -343,15 +348,15 @@
     Args:
         in_features (int): size of each input sample.
         out_features (int): size of each output sample.
         bias (bool, optional): If set to ``False``, the layer will not learn an additive bias, defaults to ``True``.
         dtype (`torch.dtype`): The dtype of parameters, defaults to None.
         parallel_input (bool): If set to ``True``, it's assumed that the input is split, defaults to False.
         skip_bias_add (bool): If set to ``True``, it will skip bias add for linear layer,
-        seq_parallel (`bool`): If set to ``True``, it will use sequence parallel, defaults to False.
+        seq_parallel_mode (str): If set to ``None``, it will not use sequence parallel, otherwise will use corresponding mode of sequence parallel, defaults to None.
             which is preserved for kernel fusion, defaults to False
         weight_initializer (:class:`typing.Callable`, optional):
             The initializer of weight, defaults to kaiming uniform initializer.
         bias_initializer (:class:`typing.Callable`, optional):
             The initializer of bias, defaults to xavier uniform initializer.
 
     More details about ``initializer`` please refer to
@@ -362,15 +367,15 @@
         self,
         in_features: int,
         out_features: int,
         bias: bool = True,
         dtype: torch.dtype = None,
         device: torch.device = None,
         process_group: ProcessGroup = None,
-        seq_parallel: bool = False,
+        seq_parallel_mode: str = None,
         parallel_input: bool = True,
         skip_bias_add: bool = False,
         weight: Optional[Parameter] = None,
         bias_: Optional[Parameter] = None,
         weight_initializer: Callable = init.kaiming_uniform_(a=math.sqrt(5)),
         bias_initializer: Callable = init.xavier_uniform_(a=1, scale=1),
         stream_chunk_num: int = 1,
@@ -381,15 +386,15 @@
 
         # Keep input parameters
         self.in_features = in_features
         self.out_features = out_features
         self.parallel_input = parallel_input
         self.skip_bias_add = skip_bias_add
         self.process_group = process_group
-        self.seq_parallel = seq_parallel
+        self.seq_parallel_mode = seq_parallel_mode
         self.num_partitions = dist.get_world_size(self.process_group)
 
         if skip_bias_add and not bias:
             raise ValueError("cannot skip bias addition if bias is None")
 
         # offset the seed with randomizer index and rank
         seed = torch.random.initial_seed()
@@ -524,19 +529,23 @@
                     )
                     handle_list.append(handle)
                     # output_parallel_list[i] = reduce_input(output_parallel_list[i], ParallelMode.PARALLEL_1D)
                 for handle in handle_list:
                     handle.wait()
                 output = torch.cat(output_parallel_list, dim=-1)
         else:
-            output_parallel = torch.matmul(input_, self.weight)
-            if self.seq_parallel:
-                output = linear_reducescatter_forward_gather_backward(output_parallel, self.process_group, 1)
-            else:
+            if self.seq_parallel_mode is None:
+                output_parallel = torch.matmul(input_, self.weight)
                 output = reduce_forward(output_parallel, self.process_group)
+            elif self.seq_parallel_mode == "split_gather":
+                output_parallel = torch.matmul(input_, self.weight)
+                output = reducescatter_forward_gather_backward(output_parallel, self.process_group, 1)
+            elif self.seq_parallel_mode == "ring":
+                output_parallel = torch.matmul(input_, self.weight)
+                output = reducescatter_forward_gather_backward(output_parallel, self.process_group, 1)
 
         if not self.skip_bias_add:
             if self.bias is not None:
                 output = output + self.bias
             return output
         else:
             return output, self.bias
@@ -698,15 +707,14 @@
 
         #     if bias:
         #         sharded_bias = split_fused_qkv_in_gpt2_style(module.bias.data,
         #                                                      n_fused=n_fused,
         #                                                      process_group=process_group,
         #                                                      is_transposed=False)
         #         linear_1d.bias.data.copy_(sharded_bias.data)
-        print(linear_1d.weight.shape)
         return linear_1d
 
     def reset_parameters(self, weight_initializer, bias_initializer) -> None:
         with self.randomizer.fork_rng(enable_cpu=True):
             fan_in, fan_out = self.in_features, self.out_features
             weight_initializer(self.weight, fan_in=fan_in, fan_out=fan_out)
             if self.bias is not None:
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/layer/utils.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/layer/utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -31,56 +31,64 @@
 
         Returns:
             bool: True if the parameter is marked as partially derived, False otherwise.
         """
         return getattr(param, "partial_derived", False)
 
     @staticmethod
-    def allreduce_partial_data_grad(tp_group: ProcessGroup, model: nn.Module = None, grads: List[torch.Tensor] = None):
+    def allreduce_partial_data_grad(
+        process_group: ProcessGroup,
+        model: nn.Module = None,
+        grads: List[torch.Tensor] = None,
+    ):
         """
         Allreduce partial derived gradients across the specified process group.
 
         This function performs gradient synchronization for parameters that are marked as partially derived in sequence parallelism.
 
         Args:
-            tp_group (ProcessGroup): The process group for gradient synchronization.
+            process_group (ProcessGroup): The process group for gradient synchronization.
             model (nn.Module): The model from which gradients will be synchronized.
             grads (List[torch.Tensor]): The list of gradients to be synchronized.
-
+            only_sp_partial (bool): Whether handle all the parameters or only parameters marked as partial derived.
         Raises:
             AssertionError: If both `model` and `grads` are provided or neither is provided.
         """
         # Ensure that exactly one of `model` and `grads` is provided for gradient synchronization.
         assert (model is not None) ^ (grads is not None), "Exactly one of model and grads must be not None."
 
         # Get the size of the process group, which determines whether synchronization is needed.
-        tp_size = get_world_size(tp_group) if tp_group is not None else 1
+        group_size = get_world_size(process_group) if process_group is not None else 1
 
-        if tp_size == 1:
+        if group_size == 1:
             # If the process group size is 1, no synchronization is required.
             return
 
         if model is not None:
             # If `model` is provided, extract partial derived gradients from the model's parameters.
             grads = []
+
             for p in model.parameters():
-                if p.grad is not None and SeqParallelUtils.is_sp_partial_derived_param(p):
-                    grads.append(p.grad.data)
+                if p.grad is not None:
+                    if SeqParallelUtils.is_sp_partial_derived_param(p):
+                        grads.append(p.grad.data)
 
             # Flatten and reduce the gradients using the specified process group.
+            if len(grads) == 0:
+                return
             coalesced = _flatten_dense_tensors(grads)
-            dist.all_reduce(coalesced, op=dist.ReduceOp.SUM, group=tp_group)
+            dist.all_reduce(coalesced, op=dist.ReduceOp.SUM, group=process_group)
 
             # Unflatten the synchronized gradients and update the model's gradients.
             for buf, synced in zip(grads, _unflatten_dense_tensors(coalesced, grads)):
                 buf.copy_(synced)
         else:
             # If `grads` are provided explicitly, synchronize those gradients directly.
             coalesced = _flatten_dense_tensors(grads)
-            dist.all_reduce(coalesced, op=dist.ReduceOp.SUM, group=tp_group)
+            dist.all_reduce(coalesced, op=dist.ReduceOp.SUM, group=process_group)
             for buf, synced in zip(grads, _unflatten_dense_tensors(coalesced, grads)):
                 buf.copy_(synced)
 
 
 class Randomizer:
     """
     Randomizer enables the program to be executed under a different seed within the context.
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/bert.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/bert.py`

 * *Files 0% similar despite different names*

```diff
@@ -182,21 +182,22 @@
         start_idx, end_idx = stage_index[0], stage_index[1]
         # layer_outputs
         layer_outputs = hidden_states if hidden_states is not None else None
 
         # split the input tensor along sequence dimension
         # [batch_size, seq_len, hidden_size] -> [batch_size, seq_len/TP_size, hidden_size]
         if shard_config is not None and shard_config.enable_sequence_parallelism:
-            hidden_states = split_forward_gather_backward(
-                hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
-            )
-            if encoder_hidden_states is not None:
-                encoder_hidden_states = split_forward_gather_backward(
-                    encoder_hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
+            if shard_config.sequence_parallelism_mode == "split_gather":
+                hidden_states = split_forward_gather_backward(
+                    hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
                 )
+                if encoder_hidden_states is not None:
+                    encoder_hidden_states = split_forward_gather_backward(
+                        encoder_hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
+                    )
 
         for idx, encoder_layer in enumerate(self.encoder.layer[start_idx:end_idx], start=start_idx):
             if stage_manager.is_first_stage() and idx == 0:
                 encoder_attention_mask = encoder_extended_attention_mask
 
             if output_hidden_states:
                 all_hidden_states = all_hidden_states + (hidden_states,)
@@ -236,17 +237,18 @@
             if output_attentions:
                 all_self_attentions = all_self_attentions + (layer_outputs[1],)
                 if self.config.add_cross_attention:
                     all_cross_attentions = all_cross_attentions + (layer_outputs[2],)
 
         # When sequence parallelism done, gather the output tensor in forward and split it in backward
         if shard_config is not None and shard_config.enable_sequence_parallelism:
-            hidden_states = gather_forward_split_backward(
-                hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
-            )
+            if shard_config.sequence_parallelism_mode == "split_gather":
+                hidden_states = gather_forward_split_backward(
+                    hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
+                )
 
         if output_hidden_states:
             all_hidden_states = all_hidden_states + (hidden_states,)
 
         # end of a stage loop
         sequence_output = hidden_states if hidden_states is not None else None
 
@@ -1281,7 +1283,20 @@
             past_key_values=encoder_outputs.past_key_values,
             hidden_states=encoder_outputs.hidden_states,
             attentions=encoder_outputs.attentions,
             cross_attentions=encoder_outputs.cross_attentions,
         )
 
     return forward
+
+
+def get_jit_fused_bert_intermediate_forward():
+    from transformers.models.bert.modeling_bert import BertIntermediate
+
+    from colossalai.kernel.jit.bias_gelu import GeLUFunction as JitGeLUFunction
+
+    def forward(self: BertIntermediate, hidden_states: torch.Tensor) -> torch.Tensor:
+        hidden_states, bias = self.dense(hidden_states)
+        hidden_states = JitGeLUFunction.apply(hidden_states, bias)
+        return hidden_states
+
+    return forward
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/blip2.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/blip2.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,12 +1,14 @@
 from typing import Optional, Tuple
 
 import torch
 import torch.nn as nn
 
+from colossalai.shardformer.layer import ColoAttention
+
 
 def forward_fn():
     def forward(
         self,
         hidden_states: torch.Tensor,
         head_mask: Optional[torch.Tensor] = None,
         output_attentions: Optional[bool] = False,
@@ -58,57 +60,86 @@
 
     return forward
 
 
 def get_blip2_flash_attention_forward():
     from transformers.models.blip_2.modeling_blip_2 import Blip2Attention
 
-    from colossalai.nn.layer.colo_attention import ColoAttention
-
     def forward(
         self: Blip2Attention,
         hidden_states: torch.Tensor,
         head_mask: Optional[torch.Tensor] = None,
         output_attentions: Optional[bool] = False,
     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
         """Input shape: Batch x Time x Channel"""
-
+        assert head_mask is None, "head_mask is not supported in FlashAttention"
         bsz, tgt_len, embed_dim = hidden_states.size()
         mixed_qkv = self.qkv(hidden_states)
-        mixed_qkv = mixed_qkv.reshape(bsz, tgt_len, 3, self.num_heads, -1).permute(2, 0, 1, 3, 4)
-        query_states, key_states, value_states = mixed_qkv[0], mixed_qkv[1], mixed_qkv[2]
+        mixed_qkv = mixed_qkv.reshape(bsz, tgt_len, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
+        query_states, key_states, value_states = (
+            mixed_qkv[0],
+            mixed_qkv[1],
+            mixed_qkv[2],
+        )
 
-        attention = ColoAttention(
-            embed_dim=self.embed_dim, num_heads=self.num_heads, dropout=self.dropout.p, scale=self.scale
+        dropout_p = self.dropout.p if self.training else 0.0
+        context_layer = ColoAttention.attention(
+            query_states,
+            key_states,
+            value_states,
+            dropout_p=dropout_p,
+            scale=self.scale,
         )
-        context_layer = attention(query_states, key_states, value_states)
+        context_layer = context_layer.permute(0, 2, 1, 3).reshape(bsz, tgt_len, self.embed_dim)
 
         output = self.projection(context_layer)
         outputs = (output, None)
 
         return outputs
 
     return forward
 
 
 def get_jit_fused_blip2_QFormer_self_output_forward():
     from transformers.models.blip_2.modeling_blip_2 import Blip2QFormerSelfOutput
 
-    def forward(self: Blip2QFormerSelfOutput, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
+    def forward(
+        self: Blip2QFormerSelfOutput,
+        hidden_states: torch.Tensor,
+        input_tensor: torch.Tensor,
+    ) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
         hidden_states = self.dropout_add(hidden_states, input_tensor, self.dropout.p, self.dropout.training)
         hidden_states = self.LayerNorm(hidden_states)
         return hidden_states
 
     return forward
 
 
 def get_jit_fused_blip2_QFormer_output_forward():
     from transformers.models.blip_2.modeling_blip_2 import Blip2QFormerOutput
 
-    def forward(self: Blip2QFormerOutput, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
+    def forward(
+        self: Blip2QFormerOutput,
+        hidden_states: torch.Tensor,
+        input_tensor: torch.Tensor,
+    ) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
         hidden_states = self.dropout_add(hidden_states, input_tensor, self.dropout.p, self.dropout.training)
         hidden_states = self.LayerNorm(hidden_states)
         return hidden_states
 
     return forward
+
+
+def get_jit_fused_blip2_mlp_forward():
+    from transformers.models.blip_2.modeling_blip_2 import Blip2MLP
+
+    from colossalai.kernel.jit.bias_gelu import GeLUFunction as JitGeLUFunction
+
+    def forward(self: Blip2MLP, hidden_states: torch.Tensor) -> torch.Tensor:
+        hidden_states, bias = self.fc1(hidden_states)
+        hidden_states = JitGeLUFunction.apply(hidden_states, bias)
+        hidden_states = self.fc2(hidden_states)
+        return hidden_states
+
+    return forward
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/bloom.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/bloom.py`

 * *Files 1% similar despite different names*

```diff
@@ -2,14 +2,15 @@
 from typing import List, Optional, Tuple, Union
 
 import torch
 import torch.distributed as dist
 from torch.distributed import ProcessGroup
 from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
 from torch.nn import functional as F
+from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask
 from transformers.modeling_outputs import (
     BaseModelOutputWithPastAndCrossAttentions,
     CausalLMOutputWithCrossAttentions,
     QuestionAnsweringModelOutput,
     SequenceClassifierOutputWithPast,
     TokenClassifierOutput,
 )
@@ -201,50 +202,46 @@
             attention_mask = torch.ones((batch_size, seq_length_with_past), device=hidden_states.device)
         else:
             attention_mask = attention_mask.to(hidden_states.device)
 
         alibi = self.build_alibi_tensor(attention_mask, self.num_heads, dtype=hidden_states.dtype)
 
         # causal_mask is constructed every stage and its input is passed through different stages
-        causal_mask = self._prepare_attn_mask(
+        causal_mask = _prepare_4d_causal_attention_mask(
             attention_mask,
             input_shape=(batch_size, seq_length),
+            inputs_embeds=hidden_states,
             past_key_values_length=past_key_values_length,
         )
-
+        causal_mask = causal_mask.bool()
         # split the input tensor along sequence dimension
         # [batch_size, seq_len, hidden_size] -> [batch_size, seq_len/TP_size, hidden_size]
-        if shard_config.enable_sequence_parallelism:
-            hidden_states = split_forward_gather_backward(
-                hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
-            )
+        if shard_config and shard_config.enable_sequence_parallelism:
+            if shard_config.sequence_parallelism_mode == "split_gather":
+                hidden_states = split_forward_gather_backward(
+                    hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
+                )
 
         start_idx, end_idx = stage_index[0], stage_index[1]
         for i, (block, layer_past) in enumerate(
             zip(self.h[start_idx:end_idx], past_key_values[start_idx:end_idx]), start=start_idx
         ):
             if output_hidden_states:
                 all_hidden_states = all_hidden_states + (hidden_states,)
 
             if self.gradient_checkpointing and self.training:
-
-                def create_custom_forward(module):
-                    def custom_forward(*inputs):
-                        # None for past_key_value
-                        return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)
-
-                    return custom_forward
-
-                outputs = torch.utils.checkpoint.checkpoint(
-                    create_custom_forward(block),
+                outputs = self._gradient_checkpointing_func(
+                    block.__call__,
                     hidden_states,
                     alibi,
                     causal_mask,
                     layer_past,
                     head_mask[i],
+                    use_cache,
+                    output_attentions,
                 )
             else:
                 outputs = block(
                     hidden_states,
                     layer_past=layer_past,
                     attention_mask=causal_mask,
                     head_mask=head_mask[i],
@@ -257,18 +254,19 @@
 
             if use_cache is True:
                 presents = presents + (outputs[1],)
             if output_attentions:
                 all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)
 
         # When sequence parallelism done, gather the output tensor in forward and split it in backward
-        if shard_config.enable_sequence_parallelism:
-            hidden_states = gather_forward_split_backward(
-                hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
-            )
+        if shard_config and shard_config.enable_sequence_parallelism:
+            if shard_config.sequence_parallelism_mode == "split_gather":
+                hidden_states = gather_forward_split_backward(
+                    hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
+                )
 
         if stage_manager.is_last_stage():
             # Add last hidden state
             hidden_states = self.ln_f(hidden_states)
 
         # TODO(jianghai): deal with all_hidden_states, all_self_attentions, presents
         if output_hidden_states:
@@ -996,45 +994,41 @@
         if attention_mask is None:
             attention_mask = torch.ones((batch_size, seq_length_with_past), device=hidden_states.device)
         else:
             attention_mask = attention_mask.to(hidden_states.device)
 
         alibi = self.build_alibi_tensor(attention_mask, self.num_heads, dtype=hidden_states.dtype)
 
-        causal_mask = self._prepare_attn_mask(
+        causal_mask = _prepare_4d_causal_attention_mask(
             attention_mask,
             input_shape=(batch_size, seq_length),
+            inputs_embeds=hidden_states,
             past_key_values_length=past_key_values_length,
         )
+        causal_mask = causal_mask.bool()
         # split the input tensor along sequence dimension
         # [batch_size, seq_len, hidden_size] -> [batch_size, seq_len/TP_size, hidden_size]
         hidden_states = split_forward_gather_backward(
             hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
         )
 
         for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
             if output_hidden_states:
                 all_hidden_states = all_hidden_states + (hidden_states,)
 
             if self.gradient_checkpointing and self.training:
-
-                def create_custom_forward(module):
-                    def custom_forward(*inputs):
-                        # None for past_key_value
-                        return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)
-
-                    return custom_forward
-
-                outputs = torch.utils.checkpoint.checkpoint(
-                    create_custom_forward(block),
+                outputs = self._gradient_checkpointing_func(
+                    block.__call__,
                     hidden_states,
                     alibi,
                     causal_mask,
                     layer_past,
                     head_mask[i],
+                    use_cache,
+                    output_attentions,
                 )
             else:
                 outputs = block(
                     hidden_states,
                     layer_past=layer_past,
                     attention_mask=causal_mask,
                     head_mask=head_mask[i],
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/chatglm2.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,75 +1,67 @@
 """ PyTorch ChatGLM model. """
+
 from typing import List, Optional, Tuple
 
 import torch
 import torch.utils.checkpoint
 from torch.nn import CrossEntropyLoss
 from transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast
 from transformers.utils import logging
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
 from colossalai.shardformer import ShardConfig
+from colossalai.shardformer.layer import AttnMaskType, ColoAttention
 from colossalai.shardformer.layer._operation import gather_forward_split_backward, split_forward_gather_backward
-from colossalai.shardformer.modeling.chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration, ChatGLMModel
 
 
 def get_flash_core_attention_forward():
-    from colossalai.nn.layer.colo_attention import AttnMaskType, ColoAttention
-
     from .chatglm2_6b.modeling_chatglm import CoreAttention
 
     def forward(self: CoreAttention, query_layer, key_layer, value_layer, attention_mask):
-        pytorch_major_version = int(torch.__version__.split(".")[0])
-        if pytorch_major_version >= 2:
-            query_layer, key_layer, value_layer = [k.permute(1, 2, 0, 3) for k in [query_layer, key_layer, value_layer]]
-            if attention_mask is None and query_layer.shape[2] == key_layer.shape[2]:
-                context_layer = torch.nn.functional.scaled_dot_product_attention(
-                    query_layer, key_layer, value_layer, is_causal=True
-                )
-            else:
-                if attention_mask is not None:
-                    attention_mask = ~attention_mask
-                context_layer = torch.nn.functional.scaled_dot_product_attention(
-                    query_layer, key_layer, value_layer, attention_mask
-                )
-            context_layer = context_layer.permute(2, 0, 1, 3)
-            new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)
-            context_layer = context_layer.reshape(*new_context_layer_shape)
-        else:
-            # Raw attention scores
-            query_layer = query_layer.permute(1, 0, 2, 3).contiguous()
-            key_layer = key_layer.permute(1, 0, 2, 3).contiguous()
-            value_layer = value_layer.permute(1, 0, 2, 3).contiguous()
-
-            scale = 1.0 / self.norm_factor
-            if self.coeff is not None:
-                scale = scale * self.coeff
-
-            flash_attention_mask = None
-            attn_mask_type = None
-            if attention_mask is None:
-                attn_mask_type = AttnMaskType.causal
-            else:
-                flash_attention_mask = ~(attention_mask[:, :, -1].squeeze(1).to(torch.bool)).contiguous()
-                if not torch.all(flash_attention_mask):
-                    attn_mask_type = AttnMaskType.paddedcausal
-
-            attention = ColoAttention(
-                embed_dim=self.hidden_size_per_partition,
-                num_heads=self.num_attention_heads_per_partition,
-                dropout=self.attention_dropout.p,
-                scale=scale,
+        query_layer, key_layer, value_layer = [k.permute(1, 2, 0, 3) for k in [query_layer, key_layer, value_layer]]
+        if attention_mask is None and query_layer.shape[2] == key_layer.shape[2]:
+            attention_mask_type = AttnMaskType.CAUSAL
+            attn_bias = torch.zeros(
+                query_layer.shape[0],
+                1,
+                query_layer.shape[2],
+                key_layer.shape[2],
+                dtype=query_layer.dtype,
+                device=query_layer.device,
             )
-            context_layer = attention(
-                query_layer, key_layer, value_layer, attn_mask=flash_attention_mask, attn_mask_type=attn_mask_type
+            temp_mask = (
+                torch.ones(
+                    query_layer.shape[2],
+                    key_layer.shape[2],
+                    dtype=torch.bool,
+                    device=query_layer.device,
+                )
+                .tril(diagonal=0)
+                .expand(query_layer.shape[0], 1, -1, -1)
             )
-
-            context_layer = context_layer.permute(1, 0, -1).contiguous()
-
+            attn_bias.masked_fill_(temp_mask.logical_not(), torch.finfo(query_layer.dtype).min)
+        else:
+            attention_mask_type = AttnMaskType.CUSTOM
+            if attention_mask is not None:
+                attn_bias = torch.zeros_like(attention_mask, dtype=query_layer.dtype)
+                attn_bias.masked_fill_(attention_mask, torch.finfo(query_layer.dtype).min)
+        dropout_p = self.attention_dropout.p if self.training else 0.0
+        context_layer = ColoAttention.attention(
+            query_layer,
+            key_layer,
+            value_layer,
+            attention_mask=attn_bias,
+            attention_mask_type=attention_mask_type,
+            dropout_p=dropout_p,
+            scale=1.0 / self.norm_factor,
+        )
+        context_layer = context_layer.permute(2, 0, 1, 3)
+        new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)
+        context_layer = context_layer.reshape(*new_context_layer_shape)
         return context_layer
 
     return forward
 
 
 def get_jit_fused_glm_block_forward():
     from .chatglm2_6b.modeling_chatglm import GLMBlock
@@ -124,15 +116,15 @@
 class ChatGLMPipelineForwards:
     """
     This class serves as a micro library for ChatGLM model forwards under pipeline parallelism.
     """
 
     @staticmethod
     def chatglm_model_forward(
-        self: ChatGLMModel,
+        self: "ChatGLMModel",
         input_ids,
         position_ids: Optional[torch.Tensor] = None,
         attention_mask: Optional[torch.BoolTensor] = None,
         full_attention_mask: Optional[torch.BoolTensor] = None,
         past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
         inputs_embeds: Optional[torch.Tensor] = None,
         use_cache: Optional[bool] = None,
@@ -165,19 +157,25 @@
                 inputs_embeds = self.embedding(input_ids)
             hidden_states = inputs_embeds
         else:
             seq_length, batch_size = hidden_states.shape[:2]
         if self.pre_seq_len is not None:
             if past_key_values is None:
                 past_key_values = self.get_prompt(
-                    batch_size=batch_size, device=input_ids.device, dtype=inputs_embeds.dtype
+                    batch_size=batch_size,
+                    device=input_ids.device,
+                    dtype=inputs_embeds.dtype,
                 )
             if attention_mask is not None:
                 attention_mask = torch.cat(
-                    [attention_mask.new_ones((batch_size, self.pre_seq_len)), attention_mask], dim=-1
+                    [
+                        attention_mask.new_ones((batch_size, self.pre_seq_len)),
+                        attention_mask,
+                    ],
+                    dim=-1,
                 )
         if full_attention_mask is None:
             if (attention_mask is not None and not attention_mask.all()) or (past_key_values and seq_length != 1):
                 full_attention_mask = self.get_masks(input_ids, past_key_values, padding_mask=attention_mask)
         # Rotary positional embeddings
         rotary_pos_emb = self.rotary_pos_emb(self.seq_length)
         if position_ids is not None:
@@ -194,64 +192,82 @@
                     "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                 )
                 use_cache = False
         all_self_attentions = None
         all_hidden_states = () if output_hidden_states else None
         start_idx, end_idx = stage_index[0], stage_index[1]
 
-        if shard_config.enable_sequence_parallelism:
-            hidden_states = split_forward_gather_backward(
-                hidden_states, dim=0, process_group=shard_config.tensor_parallel_process_group
-            )
+        if shard_config and shard_config.enable_sequence_parallelism:
+            if shard_config.sequence_parallelism_mode == "split_gather":
+                hidden_states = split_forward_gather_backward(
+                    hidden_states,
+                    dim=0,
+                    process_group=shard_config.tensor_parallel_process_group,
+                )
         for idx in range(start_idx, end_idx):
             layer = self.encoder._get_layer(idx)
             if output_hidden_states:
                 all_hidden_states = all_hidden_states + (hidden_states,)
             if self.encoder.gradient_checkpointing and self.encoder.training:
                 layer_ret = torch.utils.checkpoint.checkpoint(
-                    layer, hidden_states, attention_mask, rotary_pos_emb, past_key_values[idx], use_cache
+                    layer,
+                    hidden_states,
+                    attention_mask,
+                    rotary_pos_emb,
+                    past_key_values[idx],
+                    use_cache,
                 )
             else:
                 layer_ret = layer(
                     hidden_states,
                     full_attention_mask,
                     rotary_pos_emb,
                     kv_cache=past_key_values[idx],
                     use_cache=use_cache,
                 )
             hidden_states, kv_cache = layer_ret
             if use_cache:
                 presents = presents + (kv_cache,)
 
-        if shard_config.enable_sequence_parallelism:
-            hidden_states = gather_forward_split_backward(
-                hidden_states, dim=0, process_group=shard_config.tensor_parallel_process_group
-            )
+        if shard_config and shard_config.enable_sequence_parallelism:
+            if shard_config.sequence_parallelism_mode == "split_gather":
+                hidden_states = gather_forward_split_backward(
+                    hidden_states,
+                    dim=0,
+                    process_group=shard_config.tensor_parallel_process_group,
+                )
         if output_hidden_states:
             all_hidden_states = all_hidden_states + (hidden_states,)
         if stage_manager.is_last_stage():
             # final layer_norm
             if self.encoder.post_layer_norm:
                 hidden_states = self.encoder.final_layernorm(hidden_states)
             if not return_dict:
                 return tuple(
-                    v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None
+                    v
+                    for v in [
+                        hidden_states,
+                        presents,
+                        all_hidden_states,
+                        all_self_attentions,
+                    ]
+                    if v is not None
                 )
             return BaseModelOutputWithPast(
                 last_hidden_state=hidden_states,
                 past_key_values=presents,
                 hidden_states=all_hidden_states,
                 attentions=all_self_attentions,
             )
         else:
             return {"hidden_states": hidden_states}
 
     @staticmethod
     def chatglm_for_conditional_generation_forward(
-        self: ChatGLMForConditionalGeneration,
+        self: "ChatGLMForConditionalGeneration",
         input_ids: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.Tensor] = None,
         attention_mask: Optional[torch.Tensor] = None,
         past_key_values: Optional[Tuple[torch.FloatTensor]] = None,
         inputs_embeds: Optional[torch.Tensor] = None,
         labels: Optional[torch.Tensor] = None,
         use_cache: Optional[bool] = None,
@@ -364,27 +380,31 @@
         else:
             rotary_pos_emb = rotary_pos_emb[None, :seq_length]
         rotary_pos_emb = rotary_pos_emb.transpose(0, 1).contiguous()
 
         # Run encoder.
         # [seq_len, batch_size, hidden_size] -> [seq_len/TP_size, batch_size, hidden_size]
         inputs_embeds = split_forward_gather_backward(
-            inputs_embeds, dim=0, process_group=shard_config.tensor_parallel_process_group
+            inputs_embeds,
+            dim=0,
+            process_group=shard_config.tensor_parallel_process_group,
         )
         hidden_states, presents, all_hidden_states, all_self_attentions = self.encoder(
             inputs_embeds,
             full_attention_mask,
             rotary_pos_emb=rotary_pos_emb,
             kv_caches=past_key_values,
             use_cache=use_cache,
             output_hidden_states=output_hidden_states,
         )
 
         hidden_states = gather_forward_split_backward(
-            hidden_states, dim=0, process_group=shard_config.tensor_parallel_process_group
+            hidden_states,
+            dim=0,
+            process_group=shard_config.tensor_parallel_process_group,
         )
 
         if not return_dict:
             return tuple(
                 v
                 for v in [
                     hidden_states,
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/chatglm2_6b/configuration_chatglm.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/configuration_chatglm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/chatglm2_6b/modeling_chatglm.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/chatglm2_6b/modeling_chatglm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/falcon.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/falcon.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,13 +1,20 @@
+import math
+import warnings
 from typing import List, Optional, Tuple, Union
 
 import torch
 import torch.distributed as dist
 from torch.distributed import ProcessGroup
 from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
+from transformers.modeling_attn_mask_utils import (
+    AttentionMaskConverter,
+    _prepare_4d_causal_attention_mask,
+    _prepare_4d_causal_attention_mask_for_sdpa,
+)
 from transformers.modeling_outputs import (
     BaseModelOutputWithPastAndCrossAttentions,
     CausalLMOutputWithCrossAttentions,
     QuestionAnsweringModelOutput,
     SequenceClassifierOutputWithPast,
     TokenClassifierOutput,
 )
@@ -95,36 +102,44 @@
     from transformers.models.falcon.modeling_falcon import FalconDecoderLayer, dropout_add
 
     def forward(
         self: FalconDecoderLayer,
         hidden_states: torch.Tensor,
         alibi: Optional[torch.Tensor],
         attention_mask: torch.Tensor,
+        position_ids: Optional[torch.LongTensor] = None,
         layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
         head_mask: Optional[torch.Tensor] = None,
         use_cache: bool = False,
         output_attentions: bool = False,
+        **kwargs,
     ):
+        if "padding_mask" in kwargs:
+            warnings.warn(
+                "Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`"
+            )
         residual = hidden_states
 
         if self.config.new_decoder_architecture:
             attention_layernorm_out = self.ln_attn(hidden_states)
             mlp_layernorm_out = self.ln_mlp(hidden_states)
         else:
             attention_layernorm_out = self.input_layernorm(hidden_states)
 
         # Self attention.
         attn_outputs = self.self_attention(
             attention_layernorm_out,
             layer_past=layer_past,
             attention_mask=attention_mask,
+            position_ids=position_ids,
             alibi=alibi,
             head_mask=head_mask,
             use_cache=use_cache,
             output_attentions=output_attentions,
+            **kwargs,
         )
 
         attention_output = attn_outputs[0]
 
         if not self.config.new_decoder_architecture:
             if self.config.parallel_attn:
                 mlp_layernorm_out = attention_layernorm_out
@@ -150,106 +165,26 @@
             outputs = (output,) + outputs[1:]
 
         return outputs  # hidden_states, present, attentions
 
     return forward
 
 
-def get_falcon_flash_attention_forward():
-    try:
-        from xformers.ops import memory_efficient_attention as me_attention
-    except:
-        raise ImportError("Error: xformers module is not installed. Please install it to use flash attention.")
-    from transformers.models.falcon.modeling_falcon import FalconAttention
-
-    def forward(
-        self: FalconAttention,
-        hidden_states: torch.Tensor,
-        alibi: Optional[torch.Tensor],
-        attention_mask: torch.Tensor,
-        layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
-        head_mask: Optional[torch.Tensor] = None,
-        use_cache: bool = False,
-        output_attentions: bool = False,
-    ):
-        fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]
-        num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads
-        # 3 x [batch_size, seq_length, num_heads, head_dim]
-        (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)
-
-        batch_size, query_length, _, _ = query_layer.shape
-
-        query_layer = query_layer.transpose(1, 2).reshape(batch_size * self.num_heads, query_length, self.head_dim)
-        key_layer = key_layer.transpose(1, 2).reshape(
-            batch_size * num_kv_heads,
-            query_length,
-            self.head_dim,
-        )
-        value_layer = value_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)
-
-        past_kv_length = 0 if layer_past is None else layer_past[0].shape[1]
-        query_layer, key_layer = self.maybe_rotary(query_layer, key_layer, past_kv_length)
-
-        if layer_past is not None:
-            past_key, past_value = layer_past
-            # concatenate along seq_length dimension:
-            #  - key: [batch_size * self.num_heads, kv_length, head_dim]
-            #  - value: [batch_size * self.num_heads, kv_length, head_dim]
-            key_layer = torch.cat((past_key, key_layer), dim=1)
-            value_layer = torch.cat((past_value, value_layer), dim=1)
-
-        _, kv_length, _ = key_layer.shape
-        if use_cache:
-            present = (key_layer, value_layer)
-        else:
-            present = None
-
-        attention_mask_float = (attention_mask * 1.0).masked_fill(attention_mask, float("-1e9")).to(query_layer.dtype)
-
-        query_layer_ = query_layer.reshape(batch_size, self.num_heads, -1, self.head_dim).transpose(1, 2).contiguous()
-        key_layer_ = key_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim).transpose(1, 2).contiguous()
-        value_layer_ = value_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim).transpose(1, 2).contiguous()
-
-        if alibi is not None:
-            attention_mask_float = (
-                attention_mask_float + alibi.view(batch_size, self.num_heads, 1, kv_length) * self.beta
-            )
-
-        batch_size, src_len = query_layer_.size()[0], query_layer_.size()[1]
-        tgt_len = key_layer_.size()[1]
-        attention_mask_float = attention_mask_float.expand(batch_size, self.num_heads, src_len, tgt_len).contiguous()
-        context_layer = me_attention(
-            query_layer_,
-            key_layer_,
-            value_layer_,
-            attn_bias=attention_mask_float,
-            scale=self.inv_norm_factor,
-            p=self.attention_dropout.p,
-        )
-        batch_size, seq_length, _, _ = context_layer.shape
-        context_layer = context_layer.reshape(batch_size, seq_length, -1)
-
-        output_tensor = self.dense(context_layer)
-
-        return output_tensor, present
-
-    return forward
-
-
 class FalconPipelineForwards:
     """
     This class serves as a micro library for falcon pipeline forwards.
     """
 
     @staticmethod
     def falcon_model_forward(
         self: FalconModel,
         input_ids: Optional[torch.LongTensor] = None,
         past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,
         attention_mask: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
         head_mask: Optional[torch.LongTensor] = None,
         inputs_embeds: Optional[torch.LongTensor] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
@@ -270,102 +205,147 @@
 
         if past_key_values is not None:
             logger.warning_once("past_key_values is not supported for pipeline models at the moment.")
             past_key_values = None
 
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-        if past_key_values is None:
-            past_key_values = tuple([None] * len(self.h))
-        else:
-            past_key_values = self._convert_to_rw_cache(past_key_values)
-
-        # Prepare head mask if needed
-        # 1.0 in head_mask indicate we keep the head
-        # attention_probs has shape batch_size x num_heads x N x N
-        # head_mask has shape n_layer x batch x num_heads x N x N
-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)
-
         # case: First stage of training
         if stage_manager.is_first_stage():
             if input_ids is not None and inputs_embeds is not None:
                 raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
             elif input_ids is not None:
                 batch_size, seq_length = input_ids.shape
             elif inputs_embeds is not None:
                 batch_size, seq_length, _ = inputs_embeds.shape
             else:
                 raise ValueError("You have to specify either input_ids or inputs_embeds")
-
             if inputs_embeds is None:
                 inputs_embeds = self.word_embeddings(input_ids)
-
             hidden_states = inputs_embeds
-
         else:
             input_shape = hidden_states.shape[:-1]
             batch_size, seq_length = input_shape
 
+        if past_key_values is None:
+            past_key_values = tuple([None] * len(self.h))
+
+        if self.gradient_checkpointing and self.training:
+            if use_cache:
+                logger.warning(
+                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
+                )
+                use_cache = False
         presents = () if use_cache else None
         all_self_attentions = () if output_attentions else None
         all_hidden_states = () if output_hidden_states else None
 
         # Compute alibi tensor: check build_alibi_tensor documentation
         past_key_values_length = 0
         if past_key_values[0] is not None:
-            past_key_values_length = past_key_values[0][0].shape[1]  # 1 because RW-cache, not standard format
-        if attention_mask is None:
-            attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=hidden_states.device)
-        else:
-            attention_mask = attention_mask.to(hidden_states.device)
+            past_key_values_length = past_key_values[0][0].shape[-2]
 
         if self.use_alibi:
-            alibi = build_alibi_tensor(attention_mask, self.num_heads, dtype=hidden_states.dtype)
+            mask = (
+                torch.ones(
+                    (batch_size, seq_length + past_key_values_length), device=inputs_embeds.device, dtype=torch.long
+                )
+                if attention_mask is None
+                else attention_mask
+            )
+            alibi = build_alibi_tensor(mask, self.num_heads, dtype=hidden_states.dtype)
         else:
             alibi = None
+            if position_ids is None:
+                device = input_ids.device if input_ids is not None else inputs_embeds.device
+                position_ids = torch.arange(
+                    past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
+                )
+                position_ids = position_ids.unsqueeze(0)
 
-        causal_mask = self._prepare_attn_mask(
-            attention_mask,
-            input_shape=(batch_size, seq_length),
-            past_key_values_length=past_key_values_length,
-        )
+        if self._use_flash_attention_2:
+            # 2d mask is passed through the layers
+            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
+        elif self._use_sdpa and not output_attentions:
+            # output_attentions=True can not be supported when using SDPA, and we fall back on
+            # the manual implementation that requires a 4D causal mask in all cases.
+            if alibi is None:
+                attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
+                    attention_mask,
+                    (batch_size, seq_length),
+                    inputs_embeds,
+                    past_key_values_length,
+                )
+            elif head_mask is None:
+                alibi = alibi.reshape(batch_size, -1, *alibi.shape[1:])
+
+                attention_mask_2d = attention_mask
+                # We don't call _prepare_4d_causal_attention_mask_for_sdpa as we need to mask alibi using the 4D attention_mask untouched.
+                attention_mask = _prepare_4d_causal_attention_mask(
+                    attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
+                )
+
+                # We take care to integrate alibi bias in the attention_mask here.
+                if attention_mask_2d is None:
+                    attention_mask = alibi / math.sqrt(self.config.hidden_size // self.num_heads)
+                else:
+                    attention_mask = torch.masked_fill(
+                        alibi / math.sqrt(self.config.hidden_size // self.num_heads),
+                        attention_mask < -1,
+                        torch.finfo(alibi.dtype).min,
+                    )
+
+                    # From PyTorch 2.1 onwards, F.scaled_dot_product_attention with the memory-efficient attention backend
+                    # produces nans if sequences are completely unattended in the attention mask. Details: https://github.com/pytorch/pytorch/issues/110213
+                    if seq_length > 1:
+                        attention_mask = AttentionMaskConverter._unmask_unattended(
+                            attention_mask, attention_mask_2d, unmasked_value=0.0
+                        )
+            else:
+                # PyTorch SDPA does not support head_mask, we fall back on the eager implementation in this case.
+                attention_mask = _prepare_4d_causal_attention_mask(
+                    attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
+                )
+        else:
+            # 4d mask is passed through the layers
+            attention_mask = _prepare_4d_causal_attention_mask(
+                attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
+            )
+
+        # Prepare head mask if needed
+        # 1.0 in head_mask indicate we keep the head
+        # attention_probs has shape batch_size x num_heads x N x N
+        # head_mask has shape n_layer x batch x num_heads x N x N
+        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)
 
         start_idx, end_idx = stage_index[0], stage_index[1]
         for i, (block, layer_past) in enumerate(
             zip(self.h[start_idx:end_idx], past_key_values[start_idx:end_idx]), start=start_idx
         ):
             if output_hidden_states:
                 all_hidden_states = all_hidden_states + (hidden_states,)
 
             if self.gradient_checkpointing and self.training:
-                if use_cache:
-                    logger.warning(
-                        "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
-                    )
-                    use_cache = False
-
-                def create_custom_forward(module):
-                    def custom_forward(*inputs):
-                        # None for past_key_value
-                        return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)
-
-                    return custom_forward
-
-                outputs = torch.utils.checkpoint.checkpoint(
-                    create_custom_forward(block),
+                outputs = self._gradient_checkpointing_func(
+                    block.__call__,
                     hidden_states,
                     alibi,
-                    causal_mask,
+                    attention_mask,
+                    position_ids,
                     head_mask[i],
+                    layer_past,
+                    use_cache,
+                    output_attentions,
                 )
             else:
                 outputs = block(
                     hidden_states,
                     layer_past=layer_past,
-                    attention_mask=causal_mask,
+                    attention_mask=attention_mask,
+                    position_ids=position_ids,
                     head_mask=head_mask[i],
                     use_cache=use_cache,
                     output_attentions=output_attentions,
                     alibi=alibi,
                 )
 
             hidden_states = outputs[0]
@@ -378,17 +358,14 @@
         if stage_manager.is_last_stage():
             # Add last hidden state
             hidden_states = self.ln_f(hidden_states)
 
         if output_hidden_states:
             all_hidden_states = all_hidden_states + (hidden_states,)
 
-        if presents is not None:
-            presents = self._convert_cache_to_standard_format(presents, batch_size)
-
         if stage_manager.is_last_stage():
             if not return_dict:
                 return tuple(
                     v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None
                 )
             return BaseModelOutputWithPastAndCrossAttentions(
                 last_hidden_state=hidden_states,
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/gpt2.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/gpt2.py`

 * *Files 3% similar despite different names*

```diff
@@ -17,19 +17,89 @@
     GPT2ForTokenClassification,
     GPT2LMHeadModel,
     GPT2Model,
 )
 from transformers.utils import logging
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
+from colossalai.shardformer.layer import ColoAttention
 from colossalai.shardformer.layer._operation import gather_forward_split_backward, split_forward_gather_backward
 from colossalai.shardformer.shard import ShardConfig
 
 from ..layer import cross_entropy_1d
 
+logger = logging.get_logger(__name__)
+
+
+def _get_attention_mask(
+    self: GPT2Model,
+    shard_config: ShardConfig,
+    hidden_states: torch.Tensor,
+    past_key_values: Optional[Tuple[Tuple[torch.Tensor]]],
+    attention_mask: Optional[torch.FloatTensor],
+    encoder_hidden_states: Optional[torch.Tensor],
+    encoder_attention_mask: Optional[torch.FloatTensor],
+) -> Tuple[Optional[Union[torch.Tensor, dict]], Optional[Union[torch.Tensor, dict]]]:
+    batch_size, seq_len = hidden_states.shape[:2]
+    # If a 2D or 3D attention mask is provided for the cross-attention
+    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]
+    if self.config.add_cross_attention and encoder_hidden_states is not None:
+        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()
+        if shard_config.enable_flash_attention:
+            encoder_attention_mask = ColoAttention.prepare_attn_kwargs(
+                (encoder_batch_size, 1, seq_len, encoder_sequence_length),
+                dtype=hidden_states.dtype,
+                dtype2=encoder_hidden_states.dtype,
+                q_padding_mask=attention_mask,
+                kv_padding_mask=encoder_attention_mask,
+            )
+        else:
+            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)
+            if encoder_attention_mask is None:
+                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=encoder_hidden_states.device)
+            encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)
+    else:
+        if shard_config.enable_flash_attention:
+            encoder_attention_mask = {"attention_mask": None}
+        else:
+            encoder_attention_mask = None
+    # GPT2Attention mask.
+    past_key_values_length = 0
+    if past_key_values is not None and past_key_values[0] is not None:
+        past_key_values_length = past_key_values[0][0].shape[2]
+    if shard_config.enable_flash_attention:
+        if attention_mask is not None:
+            attention_mask = attention_mask.view(batch_size, -1)
+        attention_mask = ColoAttention.prepare_attn_kwargs(
+            (batch_size, 1, seq_len, seq_len + past_key_values_length),
+            hidden_states.dtype,
+            hidden_states.device,
+            attention_mask,
+            is_causal=True,
+        )
+    elif attention_mask is not None:
+        if batch_size <= 0:
+            raise ValueError("batch_size has to be defined and > 0")
+        attention_mask = attention_mask.view(batch_size, -1)
+        # We create a 3D attention mask from a 2D tensor mask.
+        # Sizes are [batch_size, 1, 1, to_seq_length]
+        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]
+        # this attention mask is more simple than the triangular masking of causal attention
+        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.
+        attention_mask = attention_mask[:, None, None, :]
+
+        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
+        # masked positions, this operation will create a tensor which is 0.0 for
+        # positions we want to attend and the dtype's smallest value for masked positions.
+        # Since we are adding it to the raw scores before the softmax, this is
+        # effectively the same as removing these entirely.
+        attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility
+        attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min
+    return attention_mask, encoder_attention_mask
+
 
 class GPT2PipelineForwards:
     """
     This class serves as a micro library for forward function substitution of GPT2 models
     under pipeline setting.
     """
 
@@ -78,135 +148,109 @@
 
         if stage_manager.is_first_stage():
             if input_ids is not None and inputs_embeds is not None:
                 raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
             elif input_ids is not None:
                 input_shape = input_ids.size()
                 input_ids = input_ids.view(-1, input_shape[-1])
-                batch_size = input_ids.shape[0]
+                input_ids.shape[0]
             elif inputs_embeds is not None:
                 input_shape = inputs_embeds.size()[:-1]
-                batch_size = inputs_embeds.shape[0]
+                inputs_embeds.shape[0]
             else:
                 raise ValueError("You have to specify either input_ids or inputs_embeds")
 
             device = input_ids.device if input_ids is not None else inputs_embeds.device
             if token_type_ids is not None:
                 token_type_ids = token_type_ids.view(-1, input_shape[-1])
         else:
             if hidden_states is None:
                 raise ValueError("hidden_states shouldn't be None for stages other than the first stage.")
             input_shape = hidden_states.size()[:-1]
             device = hidden_states.device
             hidden_states = hidden_states.view((-1,) + hidden_states.shape[-2:])
-            batch_size = hidden_states.shape[0]
-
-        # GPT2Attention mask.
-        if attention_mask is not None:
-            if batch_size <= 0:
-                raise ValueError("batch_size has to be defined and > 0")
-            attention_mask = attention_mask.view(batch_size, -1)
-            # We create a 3D attention mask from a 2D tensor mask.
-            # Sizes are [batch_size, 1, 1, to_seq_length]
-            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]
-            # this attention mask is more simple than the triangular masking of causal attention
-            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.
-            attention_mask = attention_mask[:, None, None, :]
-
-            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
-            # masked positions, this operation will create a tensor which is 0.0 for
-            # positions we want to attend and the dtype's smallest value for masked positions.
-            # Since we are adding it to the raw scores before the softmax, this is
-            # effectively the same as removing these entirely.
-            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility
-            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min
-
-        # If a 2D or 3D attention mask is provided for the cross-attention
-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]
-        if self.config.add_cross_attention and encoder_hidden_states is not None:
-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()
-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)
-            if encoder_attention_mask is None:
-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)
-            encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)
-        else:
-            encoder_attention_mask = None
+            hidden_states.shape[0]
 
         # Prepare head mask if needed
         # 1.0 in head_mask indicate we keep the head
         # attention_probs has shape bsz x n_heads x N x N
         # head_mask has shape n_layer x batch x n_heads x N x N
         head_mask = self.get_head_mask(head_mask, self.config.n_layer)
 
         if stage_manager.is_first_stage():
-            if position_ids is not None:
-                position_ids = position_ids.view(-1, input_shape[-1])
-            else:
+            if position_ids is None:
                 position_ids = torch.arange(0, input_shape[-1], dtype=torch.long, device=device)
-                position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
+                position_ids = position_ids.unsqueeze(0)
 
             if inputs_embeds is None:
                 inputs_embeds = self.wte(input_ids)
             position_embeds = self.wpe(position_ids)
             hidden_states = inputs_embeds + position_embeds
             if token_type_ids is not None:
                 token_type_embeds = self.wte(token_type_ids)
                 hidden_states = hidden_states + token_type_embeds
             hidden_states = self.drop(hidden_states)
 
         output_shape = input_shape + (hidden_states.size(-1),)
 
+        attention_mask, encoder_attention_mask = _get_attention_mask(
+            self,
+            shard_config,
+            hidden_states,
+            past_key_values,
+            attention_mask,
+            encoder_hidden_states,
+            encoder_attention_mask,
+        )
+
         if self.gradient_checkpointing and self.training:
             if use_cache:
                 logger.warning_once(
                     "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                 )
                 use_cache = False
         presents = () if use_cache else None
         all_self_attentions = () if output_attentions else None
         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None
         all_hidden_states = () if output_hidden_states else None
 
         # split the input tensor along sequence dimension
         # [batch_size, seq_len, hidden_size] -> [batch_size, seq_len/TP_size, hidden_size]
-        if shard_config.enable_sequence_parallelism:
-            hidden_states = split_forward_gather_backward(
-                hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
-            )
+        if shard_config and shard_config.enable_sequence_parallelism:
+            if shard_config.sequence_parallelism_mode == "split_gather":
+                hidden_states = split_forward_gather_backward(
+                    hidden_states,
+                    dim=1,
+                    process_group=shard_config.tensor_parallel_process_group,
+                )
 
         # Going through held blocks.
         start_idx, end_idx = stage_index[0], stage_index[1]
         for i in range(start_idx, end_idx):
             block = self.h[i]
             torch.cuda.set_device(hidden_states.device)
             # Ensure that attention_mask is always on the same device as hidden_states
-            if attention_mask is not None:
+            if torch.is_tensor(attention_mask):
                 attention_mask = attention_mask.to(hidden_states.device)
             if isinstance(head_mask, torch.Tensor):
                 head_mask = head_mask.to(hidden_states.device)
             if output_hidden_states:
                 all_hidden_states = all_hidden_states + (hidden_states,)
 
             if self.gradient_checkpointing and self.training:
-
-                def create_custom_forward(module):
-                    def custom_forward(*inputs):
-                        # None for past_key_value
-                        return module(*inputs, use_cache, output_attentions)
-
-                    return custom_forward
-
-                outputs = torch.utils.checkpoint.checkpoint(
-                    create_custom_forward(block),
+                outputs = self._gradient_checkpointing_func(
+                    block.__call__,
                     hidden_states,
                     None,
                     attention_mask,
                     head_mask[i],
                     encoder_hidden_states,
                     encoder_attention_mask,
+                    use_cache,
+                    output_attentions,
                 )
             else:
                 outputs = block(
                     hidden_states,
                     layer_past=None,
                     attention_mask=attention_mask,
                     head_mask=head_mask[i],
@@ -222,33 +266,42 @@
 
             if output_attentions:
                 all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)
                 if self.config.add_cross_attention:
                     all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)
 
         # When sequence parallelism done, gather the output tensor in forward and split it in backward
-        if shard_config.enable_sequence_parallelism:
-            hidden_states = gather_forward_split_backward(
-                hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
-            )
+        if shard_config and shard_config.enable_sequence_parallelism:
+            if shard_config.sequence_parallelism_mode == "split_gather":
+                hidden_states = gather_forward_split_backward(
+                    hidden_states,
+                    dim=1,
+                    process_group=shard_config.tensor_parallel_process_group,
+                )
 
         if stage_manager.is_last_stage():
             hidden_states = self.ln_f(hidden_states)
 
         hidden_states = hidden_states.view(output_shape)
 
         # Add last hidden state
         if output_hidden_states:
             all_hidden_states = all_hidden_states + (hidden_states,)
 
         if stage_manager.is_last_stage():
             if not return_dict:
                 return tuple(
                     v
-                    for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]
+                    for v in [
+                        hidden_states,
+                        presents,
+                        all_hidden_states,
+                        all_self_attentions,
+                        all_cross_attentions,
+                    ]
                     if v is not None
                 )
 
             return BaseModelOutputWithPastAndCrossAttentions(
                 last_hidden_state=hidden_states,
                 past_key_values=presents,
                 hidden_states=all_hidden_states,
@@ -326,17 +379,20 @@
             # Shift so that tokens < n predict n
             shift_logits = lm_logits[..., :-1, :].contiguous()
             shift_labels = labels[..., 1:].contiguous()
             # Flatten the tokens
             loss_fct = CrossEntropyLoss()
             shift_logits = shift_logits.view(-1, shift_logits.size(-1))
             shift_labels = shift_labels.view(-1)
-            if shard_config.enable_tensor_parallelism:
+            if shard_config.enable_tensor_parallelism and shard_config.parallel_output:
                 loss = cross_entropy_1d(
-                    shift_logits, shift_labels, process_group=shard_config.tensor_parallel_process_group
+                    shift_logits,
+                    shift_labels,
+                    process_group=shard_config.tensor_parallel_process_group,
+                    vocab_size=self.lm_head.out_features,
                 )
             else:
                 loss = loss_fct(shift_logits, shift_labels)
 
         if not return_dict:
             output = (lm_logits,) + outputs[1:]
             return ((loss,) + output) if loss is not None else output
@@ -725,96 +781,72 @@
             attentions=outputs.attentions,
         )
 
 
 def get_gpt2_flash_attention_forward():
     from transformers.models.gpt2.modeling_gpt2 import GPT2Attention
 
-    from colossalai.nn.layer.colo_attention import AttnMaskType, ColoAttention
-
-    def split_heads(tensor, num_heads, attn_head_size):
-        """
-        Splits hidden_size dim into attn_head_size and num_heads
-        """
-        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)
-        tensor = tensor.view(new_shape)
-        return tensor
-
     def forward(
         self: GPT2Attention,
         hidden_states: Optional[Tuple[torch.FloatTensor]],
         layer_past: Optional[Tuple[torch.Tensor]] = None,
-        attention_mask: Optional[torch.FloatTensor] = None,
+        attention_mask: Optional[dict] = None,
         head_mask: Optional[torch.FloatTensor] = None,
         encoder_hidden_states: Optional[torch.Tensor] = None,
-        encoder_attention_mask: Optional[torch.FloatTensor] = None,
+        encoder_attention_mask: Optional[dict] = None,
         use_cache: Optional[bool] = False,
         output_attentions: Optional[bool] = False,
     ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:
+        assert head_mask is None, "FlashAttention does not support head_mask"
         if encoder_hidden_states is not None:
             if not hasattr(self, "q_attn"):
                 raise ValueError(
                     "If class is used as cross attention, the weights `q_attn` have to be defined. "
                     "Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`."
                 )
 
             query = self.q_attn(hidden_states)
             key, value = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)
             attention_mask = encoder_attention_mask
         else:
             query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
-
-        query = split_heads(query, self.num_heads, self.head_dim)
-        key = split_heads(key, self.num_heads, self.head_dim)
-        value = split_heads(value, self.num_heads, self.head_dim)
+        query = self._split_heads(query, self.num_heads, self.head_dim)
+        key = self._split_heads(key, self.num_heads, self.head_dim)
+        value = self._split_heads(value, self.num_heads, self.head_dim)
 
         if layer_past is not None:
             past_key, past_value = layer_past
             key = torch.cat((past_key, key), dim=1)
             value = torch.cat((past_value, value), dim=1)
 
         if use_cache is True:
             present = (key, value)
         else:
             present = None
 
-        if not self.is_cross_attention:
-            attn_mask_type = AttnMaskType.causal
-            flash_attention_mask = None
-        if attention_mask != None:
-            flash_attention_mask = ~(attention_mask[:, :, -1].squeeze(1).to(torch.bool)).contiguous()
-            if not torch.all(flash_attention_mask):
-                if attn_mask_type == AttnMaskType.causal:
-                    attn_mask_type == AttnMaskType.paddedcausal
-                else:
-                    attn_mask_type = AttnMaskType.padding
-
-        scale = value.size(-1) ** -0.5
+        scale = 1.0
+        if self.scale_attn_weights:
+            scale /= value.size(-1) ** 0.5
         if self.scale_attn_by_inverse_layer_idx:
-            scale = scale * (1 / float(self.layer_idx + 1))
-
-        # use coloattention
-        attention = ColoAttention(
-            embed_dim=self.embed_dim, num_heads=self.num_heads, dropout=self.attn_dropout.p, scale=scale
-        )
-
-        attn_output = attention(query, key, value, attn_mask=flash_attention_mask, attn_mask_type=attn_mask_type)
-
+            scale /= float(self.layer_idx + 1)
+        dropout_p = self.attn_dropout.p if self.training else 0.0
+        attn_output = ColoAttention.attention(query, key, value, **attention_mask, dropout_p=dropout_p, scale=scale)
+        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)
         attn_output = self.c_proj(attn_output)
         attn_output = self.resid_dropout(attn_output)
         outputs = (attn_output, present, None)
 
         return outputs
 
     return forward
 
 
-def gpt2_sequence_parallel_forward_fn(shard_config: ShardConfig):
+def get_gpt_model_forward_for_flash_attn(shard_config: ShardConfig):
     def forward(
-        self,
+        self: GPT2Model,
         input_ids: Optional[torch.LongTensor] = None,
         past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
         attention_mask: Optional[torch.FloatTensor] = None,
         token_type_ids: Optional[torch.LongTensor] = None,
         position_ids: Optional[torch.LongTensor] = None,
         head_mask: Optional[torch.FloatTensor] = None,
         inputs_embeds: Optional[torch.FloatTensor] = None,
@@ -831,20 +863,21 @@
         )
         use_cache = use_cache if use_cache is not None else self.config.use_cache
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
         if input_ids is not None and inputs_embeds is not None:
             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
         elif input_ids is not None:
+            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)
             input_shape = input_ids.size()
             input_ids = input_ids.view(-1, input_shape[-1])
-            batch_size = input_ids.shape[0]
+            input_ids.shape[0]
         elif inputs_embeds is not None:
             input_shape = inputs_embeds.size()[:-1]
-            batch_size = inputs_embeds.shape[0]
+            inputs_embeds.shape[0]
         else:
             raise ValueError("You have to specify either input_ids or inputs_embeds")
 
         device = input_ids.device if input_ids is not None else inputs_embeds.device
 
         if token_type_ids is not None:
             token_type_ids = token_type_ids.view(-1, input_shape[-1])
@@ -853,47 +886,209 @@
 
         if past_key_values is None:
             past_length = 0
             past_key_values = tuple([None] * len(self.h))
         else:
             past_length = past_key_values[0][0].size(-2)
         if position_ids is None:
-            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)
+            position_ids = torch.arange(
+                past_length,
+                input_shape[-1] + past_length,
+                dtype=torch.long,
+                device=device,
+            )
             position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
 
-        # GPT2Attention mask.
-        if attention_mask is not None:
-            if batch_size <= 0:
-                raise ValueError("batch_size has to be defined and > 0")
-            attention_mask = attention_mask.view(batch_size, -1)
-            # We create a 3D attention mask from a 2D tensor mask.
-            # Sizes are [batch_size, 1, 1, to_seq_length]
-            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]
-            # this attention mask is more simple than the triangular masking of causal attention
-            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.
-            attention_mask = attention_mask[:, None, None, :]
-
-            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
-            # masked positions, this operation will create a tensor which is 0.0 for
-            # positions we want to attend and the dtype's smallest value for masked positions.
-            # Since we are adding it to the raw scores before the softmax, this is
-            # effectively the same as removing these entirely.
-            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility
-            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min
-
-        # If a 2D or 3D attention mask is provided for the cross-attention
-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]
-        if self.config.add_cross_attention and encoder_hidden_states is not None:
-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()
-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)
-            if encoder_attention_mask is None:
-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)
-            encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)
+        # Prepare head mask if needed
+        # 1.0 in head_mask indicate we keep the head
+        # attention_probs has shape bsz x n_heads x N x N
+        # head_mask has shape n_layer x batch x n_heads x N x N
+        head_mask = self.get_head_mask(head_mask, self.config.n_layer)
+
+        if inputs_embeds is None:
+            inputs_embeds = self.wte(input_ids)
+        position_embeds = self.wpe(position_ids)
+        hidden_states = inputs_embeds + position_embeds
+
+        if token_type_ids is not None:
+            token_type_embeds = self.wte(token_type_ids)
+            hidden_states = hidden_states + token_type_embeds
+
+        hidden_states = self.drop(hidden_states)
+
+        output_shape = (-1,) + input_shape[1:] + (hidden_states.size(-1),)
+
+        attention_mask, encoder_attention_mask = _get_attention_mask(
+            self,
+            shard_config,
+            hidden_states,
+            past_key_values,
+            attention_mask,
+            encoder_hidden_states,
+            encoder_attention_mask,
+        )
+
+        if self.gradient_checkpointing and self.training:
+            if use_cache:
+                logger.warning_once(
+                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
+                )
+                use_cache = False
+
+        presents = () if use_cache else None
+        all_self_attentions = () if output_attentions else None
+        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None
+        all_hidden_states = () if output_hidden_states else None
+        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
+            # Model parallel
+            if self.model_parallel:
+                torch.cuda.set_device(hidden_states.device)
+                # Ensure layer_past is on same device as hidden_states (might not be correct)
+                if layer_past is not None:
+                    layer_past = tuple(past_state.to(hidden_states.device) for past_state in layer_past)
+                # Ensure that attention_mask is always on the same device as hidden_states
+                if torch.is_tensor(attention_mask):
+                    attention_mask = attention_mask.to(hidden_states.device)
+                if isinstance(head_mask, torch.Tensor):
+                    head_mask = head_mask.to(hidden_states.device)
+            if output_hidden_states:
+                all_hidden_states = all_hidden_states + (hidden_states,)
+
+            if self.gradient_checkpointing and self.training:
+
+                def create_custom_forward(module):
+                    def custom_forward(*inputs):
+                        # None for past_key_value
+                        return module(*inputs, use_cache, output_attentions)
+
+                    return custom_forward
+
+                outputs = torch.utils.checkpoint.checkpoint(
+                    create_custom_forward(block),
+                    hidden_states,
+                    None,
+                    attention_mask,
+                    head_mask[i],
+                    encoder_hidden_states,
+                    encoder_attention_mask,
+                )
+            else:
+                outputs = block(
+                    hidden_states,
+                    layer_past=layer_past,
+                    attention_mask=attention_mask,
+                    head_mask=head_mask[i],
+                    encoder_hidden_states=encoder_hidden_states,
+                    encoder_attention_mask=encoder_attention_mask,
+                    use_cache=use_cache,
+                    output_attentions=output_attentions,
+                )
+
+            hidden_states = outputs[0]
+            if use_cache is True:
+                presents = presents + (outputs[1],)
+
+            if output_attentions:
+                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)
+                if self.config.add_cross_attention:
+                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)
+
+            # Model Parallel: If it's the last layer for that device, put things on the next device
+            if self.model_parallel:
+                for k, v in self.device_map.items():
+                    if i == v[-1] and "cuda:" + str(k) != self.last_device:
+                        hidden_states = hidden_states.to("cuda:" + str(k + 1))
+
+        hidden_states = self.ln_f(hidden_states)
+
+        hidden_states = hidden_states.view(output_shape)
+        # Add last hidden state
+        if output_hidden_states:
+            all_hidden_states = all_hidden_states + (hidden_states,)
+
+        if not return_dict:
+            return tuple(
+                v
+                for v in [
+                    hidden_states,
+                    presents,
+                    all_hidden_states,
+                    all_self_attentions,
+                    all_cross_attentions,
+                ]
+                if v is not None
+            )
+
+        return BaseModelOutputWithPastAndCrossAttentions(
+            last_hidden_state=hidden_states,
+            past_key_values=presents,
+            hidden_states=all_hidden_states,
+            attentions=all_self_attentions,
+            cross_attentions=all_cross_attentions,
+        )
+
+    return forward
+
+
+def gpt2_sequence_parallel_forward_fn(shard_config: ShardConfig):
+    def forward(
+        self,
+        input_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
+        attention_mask: Optional[torch.FloatTensor] = None,
+        token_type_ids: Optional[torch.LongTensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        head_mask: Optional[torch.FloatTensor] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        encoder_hidden_states: Optional[torch.Tensor] = None,
+        encoder_attention_mask: Optional[torch.FloatTensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        if input_ids is not None and inputs_embeds is not None:
+            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
+        elif input_ids is not None:
+            input_shape = input_ids.size()
+            input_ids = input_ids.view(-1, input_shape[-1])
+            input_ids.shape[0]
+        elif inputs_embeds is not None:
+            input_shape = inputs_embeds.size()[:-1]
+            inputs_embeds.shape[0]
         else:
-            encoder_attention_mask = None
+            raise ValueError("You have to specify either input_ids or inputs_embeds")
+
+        device = input_ids.device if input_ids is not None else inputs_embeds.device
+
+        if token_type_ids is not None:
+            token_type_ids = token_type_ids.view(-1, input_shape[-1])
+        if position_ids is not None:
+            position_ids = position_ids.view(-1, input_shape[-1])
+
+        if past_key_values is None:
+            past_length = 0
+            past_key_values = tuple([None] * len(self.h))
+        else:
+            past_length = past_key_values[0][0].size(-2)
+        if position_ids is None:
+            position_ids = torch.arange(
+                past_length,
+                input_shape[-1] + past_length,
+                dtype=torch.long,
+                device=device,
+            )
+            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
 
         # Prepare head mask if needed
         # 1.0 in head_mask indicate we keep the head
         # attention_probs has shape bsz x n_heads x N x N
         # head_mask has shape n_layer x batch x n_heads x N x N
         head_mask = self.get_head_mask(head_mask, self.config.n_layer)
 
@@ -905,14 +1100,23 @@
         if token_type_ids is not None:
             token_type_embeds = self.wte(token_type_ids)
             hidden_states = hidden_states + token_type_embeds
 
         hidden_states = self.drop(hidden_states)
 
         output_shape = input_shape + (hidden_states.size(-1),)
+        attention_mask, encoder_attention_mask = _get_attention_mask(
+            self,
+            shard_config,
+            hidden_states,
+            past_key_values,
+            attention_mask,
+            encoder_hidden_states,
+            encoder_attention_mask,
+        )
 
         if self.gradient_checkpointing and self.training:
             if use_cache:
                 logger = logging.get_logger(__name__)
                 logger.warning_once(
                     "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                 )
@@ -922,26 +1126,28 @@
         all_self_attentions = () if output_attentions else None
         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None
         all_hidden_states = () if output_hidden_states else None
 
         # split the input tensor along sequence dimension
         # [batch_size, seq_len, hidden_size] -> [batch_size, seq_len/TP_size, hidden_size]
         hidden_states = split_forward_gather_backward(
-            hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
+            hidden_states,
+            dim=1,
+            process_group=shard_config.sequence_parallel_process_group,
         )
 
         for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
             # Model parallel
             if self.model_parallel:
                 torch.cuda.set_device(hidden_states.device)
                 # Ensure layer_past is on same device as hidden_states (might not be correct)
                 if layer_past is not None:
                     layer_past = tuple(past_state.to(hidden_states.device) for past_state in layer_past)
                 # Ensure that attention_mask is always on the same device as hidden_states
-                if attention_mask is not None:
+                if torch.is_tensor(attention_mask):
                     attention_mask = attention_mask.to(hidden_states.device)
                 if isinstance(head_mask, torch.Tensor):
                     head_mask = head_mask.to(hidden_states.device)
             if output_hidden_states:
                 all_hidden_states = all_hidden_states + (hidden_states,)
 
             if self.gradient_checkpointing and self.training:
@@ -987,27 +1193,35 @@
             if self.model_parallel:
                 for k, v in self.device_map.items():
                     if i == v[-1] and "cuda:" + str(k) != self.last_device:
                         hidden_states = hidden_states.to("cuda:" + str(k + 1))
 
         # When sequence parallelism done, gather the output tensor in forward and split it in backward
         hidden_states = gather_forward_split_backward(
-            hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
+            hidden_states,
+            dim=1,
+            process_group=shard_config.sequence_parallel_process_group,
         )
 
         hidden_states = self.ln_f(hidden_states)
         hidden_states = hidden_states.view(output_shape)
         # Add last hidden state
         if output_hidden_states:
             all_hidden_states = all_hidden_states + (hidden_states,)
 
         if not return_dict:
             return tuple(
                 v
-                for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]
+                for v in [
+                    hidden_states,
+                    presents,
+                    all_hidden_states,
+                    all_self_attentions,
+                    all_cross_attentions,
+                ]
                 if v is not None
             )
 
         return BaseModelOutputWithPastAndCrossAttentions(
             last_hidden_state=hidden_states,
             past_key_values=presents,
             hidden_states=all_hidden_states,
@@ -1069,23 +1283,22 @@
         if labels is not None:
             # move labels to correct device to enable model parallelism
             labels = labels.to(lm_logits.device)
             # Shift so that tokens < n predict n
             shift_logits = lm_logits[..., :-1, :].contiguous()
             shift_labels = labels[..., 1:].contiguous()
             # Flatten the tokens
-            loss_fct = CrossEntropyLoss()
             shift_logits = shift_logits.view(-1, shift_logits.size(-1))
             shift_labels = shift_labels.view(-1)
-            if shard_config.enable_tensor_parallelism:
-                loss = cross_entropy_1d(
-                    shift_logits, shift_labels, process_group=shard_config.tensor_parallel_process_group
-                )
-            else:
-                loss = loss_fct(shift_logits, shift_labels)
+            loss = cross_entropy_1d(
+                shift_logits,
+                shift_labels,
+                process_group=shard_config.tensor_parallel_process_group,
+                vocab_size=self.lm_head.out_features,
+            )
 
         if not return_dict:
             output = (lm_logits,) + transformer_outputs[1:]
             return ((loss,) + output) if loss is not None else output
 
         return CausalLMOutputWithCrossAttentions(
             loss=loss,
@@ -1093,7 +1306,22 @@
             past_key_values=transformer_outputs.past_key_values,
             hidden_states=transformer_outputs.hidden_states,
             attentions=transformer_outputs.attentions,
             cross_attentions=transformer_outputs.cross_attentions,
         )
 
     return forward
+
+
+def get_jit_fused_gpt2_mlp_forward():
+    from transformers.models.gpt2.modeling_gpt2 import GPT2MLP
+
+    from colossalai.kernel.jit.bias_gelu import GeLUFunction as JitGeLUFunction
+
+    def forward(self: GPT2MLP, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:
+        hidden_states, bias = self.c_fc(hidden_states)
+        hidden_states = JitGeLUFunction.apply(hidden_states, bias)
+        hidden_states = self.c_proj(hidden_states)
+        hidden_states = self.dropout(hidden_states)
+        return hidden_states
+
+    return forward
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/gptj.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/gptj.py`

 * *Files 12% similar despite different names*

```diff
@@ -15,17 +15,62 @@
     GPTJModel,
     apply_rotary_pos_emb,
     get_embed_positions,
 )
 from transformers.utils import is_torch_fx_proxy, logging
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
+from colossalai.shardformer.layer import ColoAttention
 from colossalai.shardformer.layer._operation import gather_forward_split_backward, split_forward_gather_backward
 from colossalai.shardformer.shard import ShardConfig
 
+logger = logging.get_logger(__name__)
+
+
+def _get_attention_mask(
+    self: GPTJModel,
+    shard_config: ShardConfig,
+    hidden_states: torch.Tensor,
+    past_key_values: Optional[Tuple[Tuple[torch.Tensor]]],
+    attention_mask: Optional[torch.FloatTensor],
+) -> Optional[Union[torch.Tensor, dict]]:
+    batch_size, seq_len = hidden_states.shape[:2]
+    past_key_values_length = 0
+    if past_key_values is not None and past_key_values[0] is not None:
+        past_key_values_length = past_key_values[0][0].shape[2]
+    if shard_config.enable_flash_attention:
+        if attention_mask is not None:
+            attention_mask = attention_mask.view(batch_size, -1)
+        attention_mask = ColoAttention.prepare_attn_kwargs(
+            (batch_size, 1, seq_len, seq_len + past_key_values_length),
+            hidden_states.dtype,
+            hidden_states.device,
+            attention_mask,
+            is_causal=True,
+        )
+    elif attention_mask is not None:
+        if batch_size <= 0:
+            raise ValueError("batch_size has to be defined and > 0")
+        attention_mask = attention_mask.view(batch_size, -1)
+        # We create a 3D attention mask from a 2D tensor mask.
+        # Sizes are [batch_size, 1, 1, to_seq_length]
+        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]
+        # this attention mask is more simple than the triangular masking of causal attention
+        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.
+        attention_mask = attention_mask[:, None, None, :]
+
+        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
+        # masked positions, this operation will create a tensor which is 0.0 for
+        # positions we want to attend and the dtype's smallest value for masked positions.
+        # Since we are adding it to the raw scores before the softmax, this is
+        # effectively the same as removing these entirely.
+        attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility
+        attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min
+    return attention_mask
+
 
 class GPTJPipelineForwards:
     """
     This class serves as a micro library for forward function substitution of GPTJ models
     under pipeline setting.
     """
 
@@ -92,57 +137,37 @@
         else:
             if hidden_states is None:
                 raise ValueError("hidden_states shouldn't be None for stages other than the first stage.")
             input_shape = hidden_states.size()[:-1]
             batch_size, seq_length = input_shape[0], input_shape[1]
             device = hidden_states.device
 
-        # Attention mask.
-        if attention_mask is not None:
-            if batch_size <= 0:
-                raise ValueError("batch_size has to be defined and > 0")
-            attention_mask = attention_mask.view(batch_size, -1)
-            # We create a 3D attention mask from a 2D tensor mask.
-            # Sizes are [batch_size, 1, 1, to_seq_length]
-            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]
-            # this attention mask is more simple than the triangular masking of causal attention
-            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.
-            attention_mask = attention_mask[:, None, None, :]
-
-            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
-            # masked positions, this operation will create a tensor which is 0.0 for
-            # positions we want to attend and the dtype's smallest value for masked positions.
-            # Since we are adding it to the raw scores before the softmax, this is
-            # effectively the same as removing these entirely.
-            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility
-            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min
-
         # Prepare head mask if needed
         # 1.0 in head_mask indicate we keep the head
         # attention_probs has shape bsz x num_attention_heads x N x N
         # head_mask has shape n_layer x batch x num_attention_heads x N x N
         head_mask = self.get_head_mask(head_mask, self.config.n_layer)
 
         # position id to be assigned not just for the first stage for attn input
-        if position_ids is not None:
-            position_ids = position_ids.view(-1, seq_length)
-        else:
+        if position_ids is None:
             position_ids = torch.arange(0, seq_length, dtype=torch.long, device=device)
-            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
+            position_ids = position_ids.unsqueeze(0)
         if stage_manager.is_first_stage():
             if inputs_embeds is None:
                 inputs_embeds = self.wte(input_ids)
             hidden_states = inputs_embeds
             if token_type_ids is not None:
                 token_type_embeds = self.wte(token_type_ids)
                 hidden_states = hidden_states + token_type_embeds
             hidden_states = self.drop(hidden_states)
 
         output_shape = input_shape + (hidden_states.size(-1),)
 
+        attention_mask = _get_attention_mask(self, shard_config, hidden_states, past_key_values, attention_mask)
+
         if self.gradient_checkpointing and self.training:
             if use_cache:
                 logger.warning_once(
                     "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                 )
                 use_cache = False
 
@@ -150,15 +175,17 @@
         all_self_attentions = () if output_attentions else None
         all_hidden_states = () if output_hidden_states else None
 
         # split the input tensor along sequence dimension
         # [batch_size, seq_len, hidden_size] -> [batch_size, seq_len/TP_size, hidden_size]
         if shard_config.enable_sequence_parallelism:
             hidden_states = split_forward_gather_backward(
-                hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
+                hidden_states,
+                dim=1,
+                process_group=shard_config.tensor_parallel_process_group,
             )
 
         # Going through held blocks.
         start_idx, end_idx = stage_index[0], stage_index[1]
         for i in range(start_idx, end_idx):
             block = self.h[i]
             torch.cuda.set_device(hidden_states.device)
@@ -168,29 +195,23 @@
                 attention_mask = attention_mask.to(hidden_states.device)
             if isinstance(head_mask, torch.Tensor):
                 head_mask = head_mask.to(hidden_states.device)
             if output_hidden_states:
                 all_hidden_states = all_hidden_states + (hidden_states,)
 
             if self.gradient_checkpointing and self.training:
-
-                def create_custom_forward(module):
-                    def custom_forward(*inputs):
-                        # None for past_key_value
-                        return module(*inputs, use_cache, output_attentions)
-
-                    return custom_forward
-
-                outputs = torch.utils.checkpoint.checkpoint(
-                    create_custom_forward(block),
+                outputs = self._gradient_checkpointing_func(
+                    block.__call__,
                     hidden_states,
                     None,
                     attention_mask,
                     position_ids,
                     head_mask[i],
+                    use_cache,
+                    output_attentions,
                 )
             else:
                 outputs = block(
                     hidden_states=hidden_states,
                     layer_past=None,
                     attention_mask=attention_mask,
                     position_ids=position_ids,
@@ -205,29 +226,38 @@
 
             if output_attentions:
                 all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)
 
         # When sequence parallelism done, gather the output tensor in forward and split it in backward
         if shard_config.enable_sequence_parallelism:
             hidden_states = gather_forward_split_backward(
-                hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
+                hidden_states,
+                dim=1,
+                process_group=shard_config.tensor_parallel_process_group,
             )
 
         if stage_manager.is_last_stage():
             hidden_states = self.ln_f(hidden_states)
 
         hidden_states = hidden_states.view(output_shape)
         # Add last hidden state
         if output_hidden_states:
             all_hidden_states = all_hidden_states + (hidden_states,)
 
         if stage_manager.is_last_stage():
             if not return_dict:
                 return tuple(
-                    v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None
+                    v
+                    for v in [
+                        hidden_states,
+                        presents,
+                        all_hidden_states,
+                        all_self_attentions,
+                    ]
+                    if v is not None
                 )
 
             return BaseModelOutputWithPast(
                 last_hidden_state=hidden_states,
                 past_key_values=presents,
                 hidden_states=all_hidden_states,
                 attentions=all_self_attentions,
@@ -526,47 +556,35 @@
             attentions=outputs.attentions,
         )
 
 
 def get_gptj_flash_attention_forward():
     from transformers.models.gptj.modeling_gptj import GPTJAttention
 
-    from colossalai.nn.layer.colo_attention import AttnMaskType, ColoAttention
-
-    def split_heads(tensor, num_attention_heads, attn_head_size, rotary):
-        """
-        Splits hidden dim into attn_head_size and num_attention_heads
-        """
-        new_shape = tensor.size()[:-1] + (num_attention_heads, attn_head_size)
-        tensor = tensor.view(new_shape)
-        if rotary or len(tensor.shape) in [4, 5]:
-            return tensor
-        else:
-            raise ValueError(f"Input tensor rank should be one of [4, 5], but is: {len(tensor.shape)}")
-
     def forward(
         self: GPTJAttention,
         hidden_states: torch.FloatTensor,
         layer_past: Optional[Tuple[torch.Tensor]] = None,
-        attention_mask: Optional[torch.FloatTensor] = None,
+        attention_mask: Optional[dict] = None,
         position_ids: Optional[torch.LongTensor] = None,
         head_mask: Optional[torch.FloatTensor] = None,
         use_cache: Optional[bool] = False,
         output_attentions: Optional[bool] = False,
     ) -> Union[
         Tuple[torch.Tensor, Tuple[torch.Tensor]],
         Optional[Tuple[torch.Tensor, Tuple[torch.Tensor], Tuple[torch.Tensor, ...]]],
     ]:
+        assert head_mask is None, "head_mask is not supported for FlashAttention"
         query = self.q_proj(hidden_states)
         key = self.k_proj(hidden_states)
         value = self.v_proj(hidden_states)
 
-        query = split_heads(query, self.num_attention_heads, self.head_dim, True)
-        key = split_heads(key, self.num_attention_heads, self.head_dim, True)
-        value = split_heads(value, self.num_attention_heads, self.head_dim, False)
+        query = self._split_heads(query, self.num_attention_heads, self.head_dim, True)
+        key = self._split_heads(key, self.num_attention_heads, self.head_dim, True)
+        value = self._split_heads(value, self.num_attention_heads, self.head_dim, False)
 
         if is_torch_fx_proxy(position_ids) or torch.jit.is_tracing():
             # The logic to conditionally copy to GPU could not be traced, so we do this
             # every time in the torch.fx case
             embed_positions = get_embed_positions(self.embed_positions, position_ids)
         else:
             embed_positions = self._get_embed_positions(position_ids)
@@ -587,54 +605,212 @@
 
             key = torch.cat([k_rot, k_pass], dim=-1)
             query = torch.cat([q_rot, q_pass], dim=-1)
         else:
             key = apply_rotary_pos_emb(key, sin, cos)
             query = apply_rotary_pos_emb(query, sin, cos)
 
-        # key = key.permute(0, 2, 1, 3)
-        # query = query.permute(0, 2, 1, 3)
-        key = key.to(dtype=value.dtype)  # fp16 compatibility
-        query = query.to(dtype=value.dtype)
+        key = key.permute(0, 2, 1, 3)
+        query = query.permute(0, 2, 1, 3)
 
         if layer_past is not None:
             past_key = layer_past[0]
             past_value = layer_past[1]
-            key = torch.cat((past_key, key), dim=1)
-            value = torch.cat((past_value, value), dim=1)
+            key = torch.cat((past_key, key), dim=-2)
+            value = torch.cat((past_value, value), dim=-2)
 
         if use_cache is True:
-            present = (key, value)
+            # Note that this cast is quite ugly, but is not implemented before ROPE as the original codebase keeps the key in float32 all along the computation.
+            # Reference: https://github.com/kingoflolz/mesh-transformer-jax/blob/f8315e3003033b23f21d78361b288953064e0e76/mesh_transformer/layers.py#L128
+            present = (key.to(hidden_states.dtype), value)
         else:
             present = None
 
-        # use AttnMaskType and ColoAttention
-        attn_mask_type = AttnMaskType.causal
-        flash_attention_mask = None
-        if attention_mask != None:
-            if attn_mask_type == AttnMaskType.causal:
-                attn_mask_type == AttnMaskType.paddedcausal
-            else:
-                attn_mask_type = AttnMaskType.padding
-            flash_attention_mask = ~(attention_mask[:, :, -1].squeeze(1).to(torch.bool)).contiguous()
+        dropout_p = self.attn_dropout.p if self.training else 0.0
+        attn_output = ColoAttention.attention(query, key, value, **attention_mask, dropout_p=dropout_p)
+        attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_dim)
+        attn_output = self.out_proj(attn_output)
+        attn_output = self.resid_dropout(attn_output)
+        outputs = (attn_output, present, None)
+
+        return outputs  # a, present, (attentions)
+
+    return forward
 
-        # use coloattention
-        scale = value.size(-1) ** -0.5
 
-        attention = ColoAttention(
-            embed_dim=self.embed_dim, num_heads=self.num_attention_heads, dropout=self.attn_dropout.p, scale=scale
+def gptj_model_forward_for_flash_attention(shard_config: ShardConfig):
+    def forward(
+        self,
+        input_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
+        attention_mask: Optional[torch.FloatTensor] = None,
+        token_type_ids: Optional[torch.LongTensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        head_mask: Optional[torch.FloatTensor] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, BaseModelOutputWithPast]:
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
         )
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        if input_ids is not None and inputs_embeds is not None:
+            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
+        elif input_ids is not None:
+            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)
+            input_shape = input_ids.size()
+            input_ids = input_ids.view(-1, input_shape[-1])
+            input_ids.shape[0]
+        elif inputs_embeds is not None:
+            input_shape = inputs_embeds.size()[:-1]
+            inputs_embeds.shape[0]
+        else:
+            raise ValueError("You have to specify either input_ids or inputs_embeds")
 
-        attn_output = attention(query, key, value, attn_mask=flash_attention_mask, attn_mask_type=attn_mask_type)
+        device = input_ids.device if input_ids is not None else inputs_embeds.device
 
-        attn_output = self.out_proj(attn_output)
-        attn_output = self.resid_dropout(attn_output)
-        outputs = (attn_output, present, None)
+        if token_type_ids is not None:
+            token_type_ids = token_type_ids.view(-1, input_shape[-1])
 
-        return outputs  # a, present, (attentions)
+        if position_ids is not None:
+            position_ids = position_ids.view(-1, input_shape[-1]).long()
+
+        if past_key_values is None:
+            past_length = 0
+            past_key_values = tuple([None] * len(self.h))
+        else:
+            past_length = past_key_values[0][0].size(-2)
+
+        if position_ids is None:
+            position_ids = torch.arange(
+                past_length,
+                input_shape[-1] + past_length,
+                dtype=torch.long,
+                device=device,
+            )
+            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
+
+        # Prepare head mask if needed
+        # 1.0 in head_mask indicate we keep the head
+        # attention_probs has shape bsz x num_attention_heads x N x N
+        # head_mask has shape n_layer x batch x num_attention_heads x N x N
+        head_mask = self.get_head_mask(head_mask, self.config.n_layer)
+
+        if inputs_embeds is None:
+            inputs_embeds = self.wte(input_ids)
+
+        hidden_states = inputs_embeds
+
+        if token_type_ids is not None:
+            token_type_embeds = self.wte(token_type_ids)
+            hidden_states = hidden_states + token_type_embeds
+
+        hidden_states = self.drop(hidden_states)
+
+        attention_mask = _get_attention_mask(self, shard_config, hidden_states, past_key_values, attention_mask)
+
+        output_shape = (-1,) + input_shape[1:] + (hidden_states.size(-1),)
+
+        if self.gradient_checkpointing and self.training:
+            if use_cache:
+                logger.warning_once(
+                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
+                )
+                use_cache = False
+
+        presents = () if use_cache else None
+        all_self_attentions = () if output_attentions else None
+        all_hidden_states = () if output_hidden_states else None
+        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
+            # Model parallel
+            if self.model_parallel:
+                torch.cuda.set_device(hidden_states.device)
+                # Ensure layer_past is on same device as hidden_states (might not be correct)
+                if layer_past is not None:
+                    layer_past = tuple(past_state.to(hidden_states.device) for past_state in layer_past)
+                # Ensure that attention_mask is always on the same device as hidden_states
+                if attention_mask is not None:
+                    attention_mask = attention_mask.to(hidden_states.device)
+                if isinstance(head_mask, torch.Tensor):
+                    head_mask = head_mask.to(hidden_states.device)
+            if output_hidden_states:
+                all_hidden_states = all_hidden_states + (hidden_states,)
+
+            if self.gradient_checkpointing and self.training:
+
+                def create_custom_forward(module):
+                    def custom_forward(*inputs):
+                        # None for past_key_value
+                        return module(*inputs, use_cache, output_attentions)
+
+                    return custom_forward
+
+                outputs = torch.utils.checkpoint.checkpoint(
+                    create_custom_forward(block),
+                    hidden_states,
+                    None,
+                    attention_mask,
+                    position_ids,
+                    head_mask[i],
+                )
+            else:
+                outputs = block(
+                    hidden_states=hidden_states,
+                    layer_past=layer_past,
+                    attention_mask=attention_mask,
+                    position_ids=position_ids,
+                    head_mask=head_mask[i],
+                    use_cache=use_cache,
+                    output_attentions=output_attentions,
+                )
+
+            hidden_states = outputs[0]
+            if use_cache is True:
+                presents = presents + (outputs[1],)
+
+            if output_attentions:
+                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)
+
+            # Model Parallel: If it's the last layer for that device, put things on the next device
+            if self.model_parallel:
+                for k, v in self.device_map.items():
+                    if i == v[-1] and "cuda:" + str(k) != self.last_device:
+                        hidden_states = hidden_states.to("cuda:" + str(k + 1))
+
+        hidden_states = self.ln_f(hidden_states)
+
+        hidden_states = hidden_states.view(output_shape)
+        # Add last hidden state
+        if output_hidden_states:
+            all_hidden_states = all_hidden_states + (hidden_states,)
+
+        if not return_dict:
+            return tuple(
+                v
+                for v in [
+                    hidden_states,
+                    presents,
+                    all_hidden_states,
+                    all_self_attentions,
+                ]
+                if v is not None
+            )
+
+        return BaseModelOutputWithPast(
+            last_hidden_state=hidden_states,
+            past_key_values=presents,
+            hidden_states=all_hidden_states,
+            attentions=all_self_attentions,
+        )
 
     return forward
 
 
 def gptj_sequence_parallel_forward_fn(shard_config: ShardConfig):
     def forward(
         self,
@@ -658,18 +834,18 @@
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
         if input_ids is not None and inputs_embeds is not None:
             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
         elif input_ids is not None:
             input_shape = input_ids.size()
             input_ids = input_ids.view(-1, input_shape[-1])
-            batch_size = input_ids.shape[0]
+            input_ids.shape[0]
         elif inputs_embeds is not None:
             input_shape = inputs_embeds.size()[:-1]
-            batch_size = inputs_embeds.shape[0]
+            inputs_embeds.shape[0]
         else:
             raise ValueError("You have to specify either input_ids or inputs_embeds")
 
         device = input_ids.device if input_ids is not None else inputs_embeds.device
 
         if token_type_ids is not None:
             token_type_ids = token_type_ids.view(-1, input_shape[-1])
@@ -680,37 +856,22 @@
         if past_key_values is None:
             past_length = 0
             past_key_values = tuple([None] * len(self.h))
         else:
             past_length = past_key_values[0][0].size(-2)
 
         if position_ids is None:
-            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)
+            position_ids = torch.arange(
+                past_length,
+                input_shape[-1] + past_length,
+                dtype=torch.long,
+                device=device,
+            )
             position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
 
-        # Attention mask.
-        if attention_mask is not None:
-            if batch_size <= 0:
-                raise ValueError("batch_size has to be defined and > 0")
-            attention_mask = attention_mask.view(batch_size, -1)
-            # We create a 3D attention mask from a 2D tensor mask.
-            # Sizes are [batch_size, 1, 1, to_seq_length]
-            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]
-            # this attention mask is more simple than the triangular masking of causal attention
-            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.
-            attention_mask = attention_mask[:, None, None, :]
-
-            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
-            # masked positions, this operation will create a tensor which is 0.0 for
-            # positions we want to attend and the dtype's smallest value for masked positions.
-            # Since we are adding it to the raw scores before the softmax, this is
-            # effectively the same as removing these entirely.
-            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility
-            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min
-
         # Prepare head mask if needed
         # 1.0 in head_mask indicate we keep the head
         # attention_probs has shape bsz x num_attention_heads x N x N
         # head_mask has shape n_layer x batch x num_attention_heads x N x N
         head_mask = self.get_head_mask(head_mask, self.config.n_layer)
 
         if inputs_embeds is None:
@@ -721,14 +882,15 @@
         if token_type_ids is not None:
             token_type_embeds = self.wte(token_type_ids)
             hidden_states = hidden_states + token_type_embeds
 
         hidden_states = self.drop(hidden_states)
 
         output_shape = input_shape + (hidden_states.size(-1),)
+        attention_mask = _get_attention_mask(self, shard_config, hidden_states, past_key_values, attention_mask)
 
         if self.gradient_checkpointing and self.training:
             if use_cache:
                 logger.warning_once(
                     "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                 )
                 use_cache = False
@@ -736,15 +898,17 @@
         presents = () if use_cache else None
         all_self_attentions = () if output_attentions else None
         all_hidden_states = () if output_hidden_states else None
 
         # split the input tensor along sequence dimension
         # [batch_size, seq_len, hidden_size] -> [batch_size, seq_len/TP_size, hidden_size]
         hidden_states = split_forward_gather_backward(
-            hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
+            hidden_states,
+            dim=1,
+            process_group=shard_config.tensor_parallel_process_group,
         )
 
         for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
             # Model parallel
             if self.model_parallel:
                 torch.cuda.set_device(hidden_states.device)
                 # Ensure layer_past is on same device as hidden_states (might not be correct)
@@ -797,26 +961,37 @@
             if self.model_parallel:
                 for k, v in self.device_map.items():
                     if i == v[-1] and "cuda:" + str(k) != self.last_device:
                         hidden_states = hidden_states.to("cuda:" + str(k + 1))
 
         # When sequence parallelism done, gather the output tensor in forward and split it in backward
         hidden_states = gather_forward_split_backward(
-            hidden_states, dim=1, process_group=shard_config.tensor_parallel_process_group
+            hidden_states,
+            dim=1,
+            process_group=shard_config.tensor_parallel_process_group,
         )
 
         hidden_states = self.ln_f(hidden_states)
 
         hidden_states = hidden_states.view(output_shape)
         # Add last hidden state
         if output_hidden_states:
             all_hidden_states = all_hidden_states + (hidden_states,)
 
         if not return_dict:
-            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)
+            return tuple(
+                v
+                for v in [
+                    hidden_states,
+                    presents,
+                    all_hidden_states,
+                    all_self_attentions,
+                ]
+                if v is not None
+            )
 
         return BaseModelOutputWithPast(
             last_hidden_state=hidden_states,
             past_key_values=presents,
             hidden_states=all_hidden_states,
             attentions=all_self_attentions,
         )
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/jit.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/jit.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/llama.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/mistral.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,300 +1,284 @@
 import warnings
 from typing import List, Optional, Tuple, Union
 
 import torch
-import torch.nn.functional as F
 from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
+from transformers.cache_utils import Cache, DynamicCache
+from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask
 from transformers.modeling_outputs import (
     BaseModelOutputWithPast,
     CausalLMOutputWithPast,
     SequenceClassifierOutputWithPast,
 )
-from transformers.models.llama.modeling_llama import LlamaForCausalLM, LlamaForSequenceClassification, LlamaModel
+from transformers.models.mistral.modeling_mistral import MistralForCausalLM, MistralModel
 from transformers.utils import logging
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
 from colossalai.shardformer.shard import ShardConfig
 
-from ..layer import cross_entropy_1d
-from ..layer._operation import _gather
+from ..layer import ColoAttention
 
-try:
-    from transformers.models.llama.modeling_llama import _prepare_4d_causal_attention_mask
+logger = logging.get_logger(__name__)
 
-    LATEST_VERSION = True
-except ImportError:
-    LATEST_VERSION = False
-
-
-class LlamaPipelineForwards:
-    """
-    This class serves as a micro library for forward function substitution of Llama models
-    under pipeline setting.
-    """
 
+class MistralForwards:
     @staticmethod
-    def llama_model_forward(
-        self: LlamaModel,
+    def mistral_model_forward(
+        self: MistralModel,
         input_ids: torch.LongTensor = None,
         attention_mask: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.LongTensor] = None,
         past_key_values: Optional[List[torch.FloatTensor]] = None,
         inputs_embeds: Optional[torch.FloatTensor] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         stage_index: Optional[List[int]] = None,
         shard_config: ShardConfig = None,
-    ):
-        logger = logging.get_logger(__name__)
-
+    ) -> Union[Tuple, BaseModelOutputWithPast]:
+        if use_cache:
+            logger.warning_once("use_cache=True is not supported for Mistral models at the moment.")
+            use_cache = False
         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
         output_hidden_states = (
             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
         )
-        use_cache = use_cache if use_cache is not None else self.config.use_cache
 
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
         # retrieve input_ids and inputs_embeds
         if stage_manager.is_first_stage():
             if input_ids is not None and inputs_embeds is not None:
                 raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
             elif input_ids is not None:
                 batch_size, seq_length = input_ids.shape
             elif inputs_embeds is not None:
                 batch_size, seq_length, _ = inputs_embeds.shape
             else:
-                raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
-            device = input_ids.device if input_ids is not None else inputs_embeds.device
-            if inputs_embeds is None:
-                inputs_embeds = self.embed_tokens(input_ids)
+                raise ValueError("You have to specify either input_ids or inputs_embeds")
+            inputs_embeds = self.embed_tokens(input_ids)
             hidden_states = inputs_embeds
         else:
             input_shape = hidden_states.shape[:-1]
             batch_size, seq_length = input_shape
             device = hidden_states.device
 
-        seq_length_with_past = seq_length
         past_key_values_length = 0
 
-        # TODO(jianghai): left the recording kv-value tensors as () or None type, this feature may be added in the future.
-        if output_attentions:
-            logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
-            output_attentions = False
-        if output_hidden_states:
-            logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
-            output_hidden_states = False
-        if use_cache:
-            logger.warning_once("use_cache=True is not supported for pipeline models at the moment.")
-            use_cache = False
-
-        if past_key_values is not None:
-            past_key_values_length = past_key_values[0][0].shape[2]
-            seq_length_with_past = seq_length_with_past + past_key_values_length
-
         if position_ids is None:
+            device = input_ids.device if input_ids is not None else inputs_embeds.device
             position_ids = torch.arange(
                 past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
             )
             position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
         else:
             position_ids = position_ids.view(-1, seq_length).long()
 
-        # embed positions, for the first stage, hidden_states is the input embeddings,
-        # for the other stages, hidden_states is the output of the previous stage
-        if attention_mask is None:
-            attention_mask = torch.ones(
-                (batch_size, seq_length_with_past), dtype=torch.bool, device=hidden_states.device
-            )
-        if LATEST_VERSION:
-            attention_mask = _prepare_4d_causal_attention_mask(
-                attention_mask, (batch_size, seq_length), hidden_states, past_key_values_length
+        if attention_mask is not None and self._use_flash_attention_2 and use_cache:
+            is_padding_right = attention_mask[:, -1].sum().item() != batch_size
+            if is_padding_right:
+                raise ValueError(
+                    "You are attempting to perform batched generation with padding_side='right'"
+                    " this may lead to unexpected behaviour for Flash Attention version of Mistral. Make sure to "
+                    " call `tokenizer.padding_side  = 'left'` before tokenizing the input. "
+                )
+
+        if shard_config.enable_flash_attention:
+            # in this case, attention_mask is a dict rather than a tensor
+            mask_shape = (batch_size, 1, seq_length, seq_length)
+            attention_mask = ColoAttention.prepare_attn_kwargs(
+                mask_shape,
+                hidden_states.dtype,
+                hidden_states.device,
+                q_padding_mask=attention_mask,
+                is_causal=True,
             )
         else:
-            attention_mask = self._prepare_decoder_attention_mask(
-                attention_mask, (batch_size, seq_length), hidden_states, past_key_values_length
-            )
+            if self._use_flash_attention_2:
+                # 2d mask is passed through the layers
+                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
+            else:
+                # 4d mask is passed through the layers
+                attention_mask = _prepare_4d_causal_attention_mask(
+                    attention_mask,
+                    (batch_size, seq_length),
+                    hidden_states,
+                    past_key_values_length,
+                    sliding_window=self.config.sliding_window,
+                )
 
         if self.gradient_checkpointing and self.training:
             if use_cache:
                 logger.warning_once(
                     "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                 )
                 use_cache = False
 
         # decoder layers
         all_hidden_states = () if output_hidden_states else None
         all_self_attns = () if output_attentions else None
-        next_decoder_cache = () if use_cache else None
 
         start_idx, end_idx = stage_index[0], stage_index[1]
+        num_ckpt_layers = 0
+        if self.gradient_checkpointing and self.training:
+            num_ckpt_layers = end_idx - start_idx
+            # TODO: We can replace `gradient_checkpointing_enable` fn and initialize a gradient_checkpointing (List[bool]) for each layer
+            if shard_config.gradient_checkpoint_config is not None:
+                num_ckpt_layers = shard_config.gradient_checkpoint_config.get_num_ckpt_layers(
+                    stage=stage_manager.stage,
+                    num_stages=stage_manager.num_stages,
+                    num_layers=end_idx - start_idx,
+                    model_chunk_id=(stage_manager.model_chunk_id if stage_manager.is_interleave else 0),
+                    num_model_chunks=stage_manager.num_model_chunks,
+                )
+            assert num_ckpt_layers <= end_idx - start_idx
+
         for idx, decoder_layer in enumerate(self.layers[start_idx:end_idx], start=start_idx):
             if output_hidden_states:
                 all_hidden_states += (hidden_states,)
 
-            past_key_value = past_key_values[idx] if past_key_values is not None else None
-
-            if self.gradient_checkpointing and self.training:
-
-                def create_custom_forward(module):
-                    def custom_forward(*inputs):
-                        # None for past_key_value
-                        return module(*inputs, output_attentions, None)
-
-                    return custom_forward
-
-                layer_outputs = torch.utils.checkpoint.checkpoint(
-                    create_custom_forward(decoder_layer),
+            if idx - start_idx < num_ckpt_layers:
+                layer_outputs = self._gradient_checkpointing_func(
+                    decoder_layer.__call__,
                     hidden_states,
                     attention_mask,
                     position_ids,
-                    None,
+                    past_key_values,
+                    output_attentions,
+                    use_cache,
                 )
             else:
                 layer_outputs = decoder_layer(
                     hidden_states,
                     attention_mask=attention_mask,
                     position_ids=position_ids,
-                    past_key_value=past_key_value,
+                    past_key_value=past_key_values,
                     output_attentions=output_attentions,
                     use_cache=use_cache,
                 )
 
             hidden_states = layer_outputs[0]
 
             if use_cache:
-                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
+                layer_outputs[2 if output_attentions else 1]
+
             if output_attentions:
                 all_self_attns += (layer_outputs[1],)
 
         if stage_manager.is_last_stage():
             hidden_states = self.norm(hidden_states)
 
         # add hidden states from the last decoder layer
         if output_hidden_states:
             all_hidden_states += (hidden_states,)
-        next_cache = next_decoder_cache if use_cache else None
+
+        next_cache = None
         if stage_manager.is_last_stage():
             if not return_dict:
                 return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
             return BaseModelOutputWithPast(
                 last_hidden_state=hidden_states,
                 past_key_values=next_cache,
                 hidden_states=all_hidden_states,
                 attentions=all_self_attns,
             )
-        # always return dict for imediate stage
-        return {"hidden_states": hidden_states}
+        else:
+            return {"hidden_states": hidden_states}
 
     @staticmethod
-    def llama_for_causal_lm_forward(
-        self: LlamaForCausalLM,
+    def mistral_for_causal_lm_forward(
+        self: MistralForCausalLM,
         input_ids: torch.LongTensor = None,
         attention_mask: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.LongTensor] = None,
         past_key_values: Optional[List[torch.FloatTensor]] = None,
         inputs_embeds: Optional[torch.FloatTensor] = None,
         labels: Optional[torch.LongTensor] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         stage_index: Optional[List[int]] = None,
         shard_config: ShardConfig = None,
-    ):
+    ) -> Union[Tuple, CausalLMOutputWithPast]:
         r"""
         Args:
             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
 
         Returns:
 
         Example:
 
         ```python
-        >>> from transformers import AutoTokenizer, LlamaForCausalLM
+        >>> from transformers import AutoTokenizer, MistralForCausalLM
 
-        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)
+        >>> model = MistralForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)
         >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)
 
         >>> prompt = "Hey, are you conscious? Can you talk to me?"
         >>> inputs = tokenizer(prompt, return_tensors="pt")
 
         >>> # Generate
         >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
         "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
         ```"""
-        logger = logging.get_logger(__name__)
+
         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
         output_hidden_states = (
             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
         )
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-        # TODO(jianghai): left the recording kv-value tensors as () or None type, this feature may be added in the future.
-        if output_attentions:
-            logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
-            output_attentions = False
-        if output_hidden_states:
-            logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
-            output_hidden_states = False
-
         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-        outputs = LlamaPipelineForwards.llama_model_forward(
+        outputs = MistralForwards.mistral_model_forward(
             self.model,
             input_ids=input_ids,
             attention_mask=attention_mask,
             position_ids=position_ids,
             past_key_values=past_key_values,
             inputs_embeds=inputs_embeds,
             use_cache=use_cache,
             output_attentions=output_attentions,
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             stage_manager=stage_manager,
             hidden_states=hidden_states,
             stage_index=stage_index,
+            shard_config=shard_config,
         )
+
         past_key_values = None
 
         if stage_manager.is_last_stage():
             hidden_states = outputs[0]
             logits = self.lm_head(hidden_states)
+            logits = logits.float()
+
             loss = None
             if labels is not None:
                 # Shift so that tokens < n predict n
                 shift_logits = logits[..., :-1, :].contiguous()
                 shift_labels = labels[..., 1:].contiguous()
                 # Flatten the tokens
                 loss_fct = CrossEntropyLoss()
+                shift_logits = shift_logits.view(-1, self.config.vocab_size)
                 shift_labels = shift_labels.view(-1)
                 # Enable model parallelism
                 shift_labels = shift_labels.to(shift_logits.device)
-                if shard_config.enable_tensor_parallelism:
-                    new_vocab_size = logits.shape[-1]
-                    shift_logits = shift_logits.view(-1, new_vocab_size)
-                    loss = cross_entropy_1d(
-                        shift_logits, shift_labels, process_group=shard_config.tensor_parallel_process_group
-                    )
-                else:
-                    shift_logits = shift_logits.view(-1, self.config.vocab_size)
-                    loss = loss_fct(shift_logits, shift_labels)
-
-            if not shard_config.parallel_output:
-                logits = _gather(logits, -1, shard_config.tensor_parallel_process_group)
+                loss = loss_fct(shift_logits, shift_labels)
 
             if not return_dict:
                 output = (logits,) + outputs[1:]
                 return (loss,) + output if loss is not None else output
 
             return CausalLMOutputWithPast(
                 loss=loss,
@@ -304,82 +288,75 @@
                 attentions=outputs.attentions,
             )
         else:
             hidden_states = outputs.get("hidden_states")
             return {"hidden_states": hidden_states}
 
     @staticmethod
-    def llama_for_sequence_classification_forward(
-        self: LlamaForSequenceClassification,
+    def mistral_for_sequence_classification_forward(
+        self,
         input_ids: torch.LongTensor = None,
         attention_mask: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.LongTensor] = None,
         past_key_values: Optional[List[torch.FloatTensor]] = None,
         inputs_embeds: Optional[torch.FloatTensor] = None,
         labels: Optional[torch.LongTensor] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         stage_index: Optional[List[int]] = None,
         shard_config: ShardConfig = None,
-    ):
+    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:
         r"""
         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
         """
-        logger = logging.get_logger(__name__)
-
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-        # TODO(jianghai): left the recording kv-value tensors as () or None type, this feature may be added in the future.
-        if output_attentions:
-            logger.warning_once("output_attentions=True is not supported for pipeline models at the moment.")
-            output_attentions = False
-        if output_hidden_states:
-            logger.warning_once("output_hidden_states=True is not supported for pipeline models at the moment.")
-            output_hidden_states = False
 
-        transformer_outputs = LlamaPipelineForwards.llama_model_forward(
+        transformer_outputs = MistralForwards.mistral_model_forward(
             self.model,
             input_ids,
             attention_mask=attention_mask,
             position_ids=position_ids,
             past_key_values=past_key_values,
             inputs_embeds=inputs_embeds,
             use_cache=use_cache,
             output_attentions=output_attentions,
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             stage_manager=stage_manager,
             hidden_states=hidden_states,
             stage_index=stage_index,
+            shard_config=shard_config,
         )
 
         if input_ids is not None:
             batch_size = input_ids.shape[0]
         elif inputs_embeds is not None:
             batch_size = inputs_embeds.shape[0]
         else:
             batch_size = hidden_states.shape[0]
 
         if stage_manager.is_last_stage():
             hidden_states = transformer_outputs[0]
             logits = self.score(hidden_states)
-
             if self.config.pad_token_id is None and batch_size != 1:
                 raise ValueError("Cannot handle batch sizes > 1 if no padding token is defined.")
             if self.config.pad_token_id is None:
                 sequence_lengths = -1
             else:
                 if input_ids is not None:
-                    sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)
+                    sequence_lengths = (torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1).to(
+                        logits.device
+                    )
                 else:
                     sequence_lengths = -1
 
             pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]
 
             loss = None
             if labels is not None:
@@ -403,208 +380,232 @@
                     loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))
                 elif self.config.problem_type == "multi_label_classification":
                     loss_fct = BCEWithLogitsLoss()
                     loss = loss_fct(pooled_logits, labels)
             if not return_dict:
                 output = (pooled_logits,) + transformer_outputs[1:]
                 return ((loss,) + output) if loss is not None else output
-
-            return SequenceClassifierOutputWithPast(
-                loss=loss,
-                logits=pooled_logits,
-                past_key_values=transformer_outputs.past_key_values,
-                hidden_states=transformer_outputs.hidden_states,
-                attentions=transformer_outputs.attentions,
-            )
-
         else:
             hidden_states = transformer_outputs.get("hidden_states")
             return {"hidden_states": hidden_states}
 
+        return SequenceClassifierOutputWithPast(
+            loss=loss,
+            logits=pooled_logits,
+            past_key_values=transformer_outputs.past_key_values,
+            hidden_states=transformer_outputs.hidden_states,
+            attentions=transformer_outputs.attentions,
+        )
 
-def get_llama_flash_attention_forward(shard_config: ShardConfig):
-    from transformers.models.llama.modeling_llama import LlamaAttention, apply_rotary_pos_emb
-
-    from colossalai.nn.layer.colo_attention import AttnMaskType, ColoAttention
 
-    llama_version = 2
-    try:
-        from transformers.models.llama.modeling_llama import repeat_kv
-    except:
-        warnings.warn("using llamav1, llamav1 hasn't repeat_kv function")
-        llama_version = 1
+def get_mistral_model_forward_for_flash_attn(shard_config: ShardConfig):
+    logger = logging.get_logger(__name__)
+    assert shard_config.enable_flash_attention, "Flash Attention is not enabled."
 
     def forward(
-        self: LlamaAttention,
-        hidden_states: torch.Tensor,
+        self: MistralModel,
+        input_ids: torch.LongTensor = None,
         attention_mask: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.LongTensor] = None,
-        past_key_value: Optional[Tuple[torch.Tensor]] = None,
-        output_attentions: bool = False,
-        use_cache: bool = False,
-        **kwargs,
-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
-        bsz, q_len, _ = hidden_states.size()
-        assert q_len % 4 == 0, "Flash Attention Error: The sequence length should be a multiple of 4."
+        past_key_values: Optional[List[torch.FloatTensor]] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, BaseModelOutputWithPast]:
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
 
-        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
-        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
-        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-        kv_seq_len = key_states.shape[-2]
-        if past_key_value is not None:
-            kv_seq_len += past_key_value[0].shape[-2]
+        # retrieve input_ids and inputs_embeds
+        if input_ids is not None and inputs_embeds is not None:
+            raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
+        elif input_ids is not None:
+            batch_size, seq_length = input_ids.shape
+        elif inputs_embeds is not None:
+            batch_size, seq_length, _ = inputs_embeds.shape
+        else:
+            raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
 
-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
+        past_key_values_length = 0
 
-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
+        if use_cache:
+            use_legacy_cache = not isinstance(past_key_values, Cache)
+            if use_legacy_cache:
+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)
+            past_key_values_length = past_key_values.get_usable_length(seq_length)
 
-        if past_key_value is not None:
-            # reuse k, v, self_attention
-            key_states = torch.cat([past_key_value[0], key_states], dim=2)
-            value_states = torch.cat([past_key_value[1], value_states], dim=2)
+        if position_ids is None:
+            device = input_ids.device if input_ids is not None else inputs_embeds.device
+            position_ids = torch.arange(
+                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
+            )
+            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
+        else:
+            position_ids = position_ids.view(-1, seq_length).long()
 
-        past_key_value = (key_states, value_states) if use_cache else None
+        if inputs_embeds is None:
+            inputs_embeds = self.embed_tokens(input_ids)
 
-        # repeat k/v heads if n_kv_heads < n_heads
-        if llama_version == 2:
-            key_states = repeat_kv(key_states, self.num_key_value_groups)
-            value_states = repeat_kv(value_states, self.num_key_value_groups)
-
-        me_input_shape = (bsz, q_len, self.num_heads, self.head_dim)
-        query_states = query_states.transpose(1, 2).contiguous().view(*me_input_shape)
-        key_states = key_states.transpose(1, 2).contiguous().view(*me_input_shape)
-        value_states = value_states.transpose(1, 2).contiguous().view(*me_input_shape)
-
-        flash_attention_mask = None
-        attn_mask_type = AttnMaskType.causal
-        if not getattr(shard_config, "causal_lm", False) and attention_mask != None:
-            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
+        if attention_mask is not None and self._use_flash_attention_2 and use_cache:
+            is_padding_right = attention_mask[:, -1].sum().item() != batch_size
+            if is_padding_right:
                 raise ValueError(
-                    f"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"
+                    "You are attempting to perform batched generation with padding_side='right'"
+                    " this may lead to unexpected behaviour for Flash Attention version of Mistral. Make sure to "
+                    " call `tokenizer.padding_side  = 'left'` before tokenizing the input. "
+                )
+        if shard_config.enable_flash_attention:
+            # in this case, attention_mask is a dict rather than a tensor
+            mask_shape = (batch_size, 1, seq_length, seq_length)
+            attention_mask = ColoAttention.prepare_attn_kwargs(
+                mask_shape,
+                inputs_embeds.dtype,
+                inputs_embeds.device,
+                q_padding_mask=attention_mask,
+                is_causal=True,
+            )
+        else:
+            if self._use_flash_attention_2:
+                # 2d mask is passed through the layers
+                attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
+            else:
+                # 4d mask is passed through the layers
+                attention_mask = _prepare_4d_causal_attention_mask(
+                    attention_mask,
+                    (batch_size, seq_length),
+                    inputs_embeds,
+                    past_key_values_length,
+                    sliding_window=self.config.sliding_window,
                 )
-            flash_attention_mask = ~(attention_mask[:, :, -1].squeeze(1).to(torch.bool)).contiguous()
-            attn_mask_type = AttnMaskType.paddedcausal
 
-        attention = ColoAttention(embed_dim=self.hidden_size, num_heads=self.num_heads)
-        attn_output = attention(
-            query_states,
-            key_states,
-            value_states,
-            attn_mask=flash_attention_mask,
-            attn_mask_type=attn_mask_type,
-            origin_attn_mask=attention_mask,
-        )
+        hidden_states = inputs_embeds
 
-        attn_output = self.o_proj(attn_output)
+        if self.gradient_checkpointing and self.training:
+            if use_cache:
+                logger.warning_once(
+                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
+                )
+                use_cache = False
 
-        return attn_output, None, past_key_value
+        # decoder layers
+        all_hidden_states = () if output_hidden_states else None
+        all_self_attns = () if output_attentions else None
+        next_decoder_cache = None
 
-    return forward
+        for decoder_layer in self.layers:
+            if output_hidden_states:
+                all_hidden_states += (hidden_states,)
 
+            if self.gradient_checkpointing and self.training:
+                layer_outputs = self._gradient_checkpointing_func(
+                    decoder_layer.__call__,
+                    hidden_states,
+                    attention_mask,
+                    position_ids,
+                    past_key_values,
+                    output_attentions,
+                    use_cache,
+                )
+            else:
+                layer_outputs = decoder_layer(
+                    hidden_states,
+                    attention_mask=attention_mask,
+                    position_ids=position_ids,
+                    past_key_value=past_key_values,
+                    output_attentions=output_attentions,
+                    use_cache=use_cache,
+                )
 
-def get_lm_forward_with_dist_cross_entropy(shard_config: ShardConfig):
-    from transformers import LlamaForCausalLM
+            hidden_states = layer_outputs[0]
 
-    def forward(
-        self: LlamaForCausalLM,
-        input_ids: torch.LongTensor = None,
-        attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
-        past_key_values: Optional[List[torch.FloatTensor]] = None,
-        inputs_embeds: Optional[torch.FloatTensor] = None,
-        labels: Optional[torch.LongTensor] = None,
-        use_cache: Optional[bool] = None,
-        output_attentions: Optional[bool] = None,
-        output_hidden_states: Optional[bool] = None,
-        return_dict: Optional[bool] = None,
-    ) -> Union[Tuple, CausalLMOutputWithPast]:
-        r"""
-        Args:
-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,
-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored
-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
+            if use_cache:
+                next_decoder_cache = layer_outputs[2 if output_attentions else 1]
 
-        Returns:
+            if output_attentions:
+                all_self_attns += (layer_outputs[1],)
 
-        Example:
+        hidden_states = self.norm(hidden_states)
 
-        ```python
-        >>> from transformers import AutoTokenizer, LlamaForCausalLM
+        # add hidden states from the last decoder layer
+        if output_hidden_states:
+            all_hidden_states += (hidden_states,)
 
-        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)
-        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)
+        next_cache = None
+        if use_cache:
+            next_cache = next_decoder_cache.to_legacy_cache() if use_legacy_cache else next_decoder_cache
 
-        >>> prompt = "Hey, are you conscious? Can you talk to me?"
-        >>> inputs = tokenizer(prompt, return_tensors="pt")
+        if not return_dict:
+            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
+        return BaseModelOutputWithPast(
+            last_hidden_state=hidden_states,
+            past_key_values=next_cache,
+            hidden_states=all_hidden_states,
+            attentions=all_self_attns,
+        )
 
-        >>> # Generate
-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)
-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
-        "Hey, are you conscious? Can you talk to me?\nI'm not conscious, but I can talk to you."
-        ```"""
+    return forward
 
-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
-        output_hidden_states = (
-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
-        )
-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)
-        outputs = self.model(
-            input_ids=input_ids,
-            attention_mask=attention_mask,
-            position_ids=position_ids,
-            past_key_values=past_key_values,
-            inputs_embeds=inputs_embeds,
-            use_cache=use_cache,
-            output_attentions=output_attentions,
-            output_hidden_states=output_hidden_states,
-            return_dict=return_dict,
-        )
+def get_mistral_flash_attention_forward(shard_config: ShardConfig):
+    from transformers.models.mistral.modeling_mistral import MistralAttention, apply_rotary_pos_emb, repeat_kv
 
-        hidden_states = outputs[0]
-        if self.config.pretraining_tp > 1:
-            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)
-            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]
-            logits = torch.cat(logits, dim=-1)
-        else:
-            logits = self.lm_head(hidden_states)
-        logits = logits.float()
+    def forward(
+        self: MistralAttention,
+        hidden_states: torch.Tensor,
+        attention_mask: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_value: Optional[Cache] = None,
+        output_attentions: bool = False,
+        use_cache: bool = False,
+        **kwargs,
+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
+        if "padding_mask" in kwargs:
+            warnings.warn(
+                "Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`"
+            )
+        bsz, q_len, _ = hidden_states.size()
 
-        loss = None
-        if labels is not None:
-            # Shift so that tokens < n predict n
-            shift_logits = logits[..., :-1, :].contiguous()
-            shift_labels = labels[..., 1:].contiguous()
-            # Flatten the tokens
-            loss_fct = CrossEntropyLoss()
-            shift_labels = shift_labels.view(-1)
-            # Enable model parallelism
-            shift_labels = shift_labels.to(shift_logits.device)
-            if shard_config.enable_tensor_parallelism:
-                new_vocab_size = logits.shape[-1]
-                shift_logits = shift_logits.view(-1, new_vocab_size)
-                loss = cross_entropy_1d(
-                    shift_logits, shift_labels, process_group=shard_config.tensor_parallel_process_group
+        query_states = self.q_proj(hidden_states)
+        key_states = self.k_proj(hidden_states)
+        value_states = self.v_proj(hidden_states)
+
+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
+
+        kv_seq_len = key_states.shape[-2]
+        if past_key_value is not None:
+            if self.layer_idx is None:
+                raise ValueError(
+                    f"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} "
+                    "for auto-regressive decoding with k/v caching, please make sure to initialize the attention class "
+                    "with a layer index."
                 )
-            else:
-                shift_logits = shift_logits.view(-1, self.config.vocab_size)
-                loss = loss_fct(shift_logits, shift_labels)
+            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
+        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
 
-        if not shard_config.parallel_output:
-            logits = _gather(logits, -1, shard_config.tensor_parallel_process_group)
+        if past_key_value is not None:
+            cache_kwargs = {"sin": sin, "cos": cos}  # Specific to RoPE models
+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
 
-        if not return_dict:
-            output = (logits,) + outputs[1:]
-            return (loss,) + output if loss is not None else output
+        # repeat k/v heads if n_kv_heads < n_heads
+        key_states = repeat_kv(key_states, self.num_key_value_groups)
+        value_states = repeat_kv(value_states, self.num_key_value_groups)
 
-        return CausalLMOutputWithPast(
-            loss=loss,
-            logits=logits,
-            past_key_values=outputs.past_key_values,
-            hidden_states=outputs.hidden_states,
-            attentions=outputs.attentions,
-        )
+        assert isinstance(attention_mask, dict), "Flash Attention Error: attention_mask should be a dict."
+        attn_output = ColoAttention.attention(query_states, key_states, value_states, **attention_mask)
+
+        attn_output = attn_output.transpose(1, 2).contiguous()
+        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
+
+        attn_output = self.o_proj(attn_output)
+
+        return attn_output, None, past_key_value
 
     return forward
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/mistral.py` & `colossalai-nightly-2024.5.4/colossalai/kernel/extensions/flash_attention/flash_attention_dao_cuda.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,73 +1,96 @@
-from typing import Optional, Tuple
+from ..base_extension import _Extension
 
-import torch
 
+class FlashAttentionDaoCudaExtension(_Extension):
+    def __init__(self):
+        super().__init__(name="flash_attention_dao_cuda", support_aot=False, support_jit=False, priority=10)
 
-def get_mistral_flash_attention_forward():
-    from transformers.models.mistral.modeling_mistral import MistralAttention, apply_rotary_pos_emb, repeat_kv
+    def is_available(self) -> bool:
+        # cuda extension can only be built if cuda is available
+        try:
+            import torch
 
-    from colossalai.nn.layer.colo_attention import AttnMaskType, ColoAttention
-
-    def forward(
-        self: MistralAttention,
-        hidden_states: torch.Tensor,
-        attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
-        past_key_value: Optional[Tuple[torch.Tensor]] = None,
-        output_attentions: bool = False,
-        use_cache: bool = False,
-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
-        bsz, q_len, _ = hidden_states.size()
-        assert q_len % 4 == 0, "Flash Attention Error: The sequence length should be a multiple of 4."
-
-        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
-        key_states = (
-            self.k_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
-        )
-        value_states = (
-            self.v_proj(hidden_states).view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
-        )
+            from flash_attn import flash_attn_func, flash_attn_varlen_kvpacked_func  # noqa
+            from flash_attn.bert_padding import index_first_axis, pad_input  # noqa
 
-        kv_seq_len = key_states.shape[-2]
-        if past_key_value is not None:
-            kv_seq_len += past_key_value[0].shape[-2]
-
-        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
-
-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
-
-        if past_key_value is not None:
-            # reuse k, v, self_attention
-            key_states = torch.cat([past_key_value[0], key_states], dim=2)
-            value_states = torch.cat([past_key_value[1], value_states], dim=2)
-
-        past_key_value = (key_states, value_states) if use_cache else None
-
-        key_states = repeat_kv(key_states, self.num_key_value_groups)
-        value_states = repeat_kv(value_states, self.num_key_value_groups)
-
-        me_input_shape = (bsz, q_len, self.num_heads, self.head_dim)
-        query_states = query_states.transpose(1, 2).contiguous().view(*me_input_shape)
-        key_states = key_states.transpose(1, 2).contiguous().view(*me_input_shape)
-        value_states = value_states.transpose(1, 2).contiguous().view(*me_input_shape)
-
-        flash_attention_mask = None
-        attn_mask_type = AttnMaskType.causal
-        if attention_mask != None:
-            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
-                raise ValueError(
-                    f"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"
-                )
-            flash_attention_mask = ~(attention_mask[:, :, -1].squeeze(1).to(torch.bool)).contiguous()
-            attn_mask_type = AttnMaskType.paddedcausal
+            cuda_available = torch.cuda.is_available()
+        except:
+            cuda_available = False
+        return cuda_available
+
+    def assert_compatible(self) -> bool:
+        pass
 
-        attention = ColoAttention(embed_dim=self.hidden_size, num_heads=self.num_heads)
-        attn_output = attention(
-            query_states, key_states, value_states, attn_mask=flash_attention_mask, attn_mask_type=attn_mask_type
+    def build_aot(self) -> None:
+        raise NotImplementedError(
+            "We rely on the third-party flash-attn library for flash attention (https://github.com/Dao-AILab/flash-attention). Please install flash-attn via 'pip install flash-attn --no-build-isolation'."
         )
 
-        attn_output = self.o_proj(attn_output)
+    def build_jit(self) -> None:
+        raise NotImplementedError(
+            "We rely on the third-party flash-attn library for flash attention (https://github.com/Dao-AILab/flash-attention). Please install flash-attn via 'pip install flash-attn --no-build-isolation'"
+        )
+
+    def load(self):
+        from typing import Optional
 
-        return attn_output, None, past_key_value
+        import torch
+        from einops import rearrange
+        from flash_attn import flash_attn_func, flash_attn_varlen_kvpacked_func
+        from flash_attn.bert_padding import index_first_axis, pad_input
+
+        def _unpad_input(hidden_states: torch.Tensor, indices: torch.Tensor):
+            return index_first_axis(rearrange(hidden_states, "b s ... -> (b s) ..."), indices)
+
+        def flash_attention(
+            q: torch.Tensor,
+            k: torch.Tensor,
+            v: torch.Tensor,
+            dropout_p: float = 0.0,
+            scale: Optional[float] = None,
+            attention_mask: Optional[torch.Tensor] = None,
+            is_causal: bool = False,
+            cu_seqlens_q: Optional[torch.Tensor] = None,
+            cu_seqlens_kv: Optional[torch.Tensor] = None,
+            max_seqlen_q: Optional[int] = None,
+            max_seqlen_kv: Optional[int] = None,
+            q_indices: Optional[torch.Tensor] = None,
+            kv_indices: Optional[torch.Tensor] = None,
+        ):
+            # [B, N, S, D] -> [B, S, N, D]
+            q = q.transpose(1, 2)
+            k = k.transpose(1, 2)
+            v = v.transpose(1, 2)
+            b, s_q = q.shape[:2]
+            if cu_seqlens_q is not None:
+                # padded / padded causal
+                # unpad input: [B, S, N, D] -> [T, N, D]
+                q = _unpad_input(q, q_indices)
+                kv = _unpad_input(torch.stack(tensors=(k, v), dim=2), kv_indices)
+                attn_output = flash_attn_varlen_kvpacked_func(
+                    q,
+                    kv,
+                    cu_seqlens_q,
+                    cu_seqlens_kv,
+                    max_seqlen_q,
+                    max_seqlen_kv,
+                    dropout_p=dropout_p,
+                    softmax_scale=scale,
+                    causal=is_causal,
+                )
+                # pad output: [T, N, D] -> [B, S, N, D]
+                attn_output = pad_input(attn_output, q_indices, b, s_q)
+            else:
+                # causal / no attn mask
+                attn_output = flash_attn_func(
+                    q,
+                    k,
+                    v,
+                    dropout_p=dropout_p,
+                    softmax_scale=scale,
+                    causal=is_causal,
+                )
+            # [B, S, N, D] -> [B, N, S, D]
+            return attn_output.transpose(1, 2)
 
-    return forward
+        return flash_attention
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/opt.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/opt.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 import random
 from typing import List, Optional, Tuple, Union
 
 import torch
 from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
+from transformers.modeling_attn_mask_utils import _prepare_4d_causal_attention_mask
 from transformers.modeling_outputs import (
     BaseModelOutputWithPast,
     CausalLMOutputWithPast,
     QuestionAnsweringModelOutput,
     SequenceClassifierOutputWithPast,
 )
 from transformers.models.opt.modeling_opt import (
@@ -14,77 +15,69 @@
     OPTForQuestionAnswering,
     OPTForSequenceClassification,
     OPTModel,
 )
 from transformers.utils import logging
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
+from colossalai.shardformer.layer import ColoAttention
+from colossalai.shardformer.shard import ShardConfig
+
+logger = logging.get_logger(__name__)
+
+
+def _get_attention_mask(
+    self: OPTModel,
+    shard_config: ShardConfig,
+    hidden_states: torch.Tensor,
+    past_key_values_length: int,
+    attention_mask: Optional[torch.FloatTensor],
+):
+    batch_size, seq_length = hidden_states.shape[:2]
+    mask_seq_length = past_key_values_length + seq_length
+    if shard_config.enable_flash_attention:
+        attention_mask = ColoAttention.prepare_attn_kwargs(
+            (batch_size, 1, seq_length, mask_seq_length),
+            hidden_states.dtype,
+            hidden_states.device,
+            attention_mask,
+            is_causal=True,
+        )
+    else:
+        attention_mask = _prepare_4d_causal_attention_mask(
+            attention_mask,
+            (batch_size, seq_length),
+            hidden_states,
+            past_key_values_length,
+        )
+    return attention_mask
 
 
 class OPTPipelineForwards:
     """
     This class serves as a micro library for forward function substitution of OPT models
     under pipeline setting.
     """
 
     @staticmethod
-    def _prepare_decoder_attention_mask(attention_mask, input_shape, _dtype, device, past_key_values_length):
-        # create causal mask
-        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
-        from transformers.models.opt.modeling_opt import _make_causal_mask
-
-        combined_attention_mask = None
-        if input_shape[-1] > 1:
-            combined_attention_mask = _make_causal_mask(
-                input_shape,
-                _dtype,
-                device,
-                past_key_values_length=past_key_values_length,
-            )
-
-        if attention_mask is not None:
-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
-            expanded_attn_mask = OPTPipelineForwards._expand_mask(attention_mask, _dtype, tgt_len=input_shape[-1]).to(
-                device
-            )
-            combined_attention_mask = (
-                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask
-            )
-
-        return combined_attention_mask
-
-    @staticmethod
-    def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):
-        """
-        Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.
-        """
-        bsz, src_len = mask.size()
-        tgt_len = tgt_len if tgt_len is not None else src_len
-
-        expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)
-
-        inverted_mask = 1.0 - expanded_mask
-
-        return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)
-
-    @staticmethod
     def opt_model_forward(
         self: OPTModel,
         input_ids: torch.LongTensor = None,
         attention_mask: Optional[torch.Tensor] = None,
         head_mask: Optional[torch.Tensor] = None,
         past_key_values: Optional[List[torch.FloatTensor]] = None,
         inputs_embeds: Optional[torch.FloatTensor] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         stage_index: Optional[List[int]] = None,
+        shard_config: Optional[ShardConfig] = None,
     ) -> Union[Tuple, BaseModelOutputWithPast]:
         """
         This forward method is modified based on transformers.models.opt.modeling_opt.OPTModel.forward
         """
 
         from transformers.modeling_outputs import BaseModelOutputWithPast
         from transformers.utils import logging
@@ -115,43 +108,67 @@
 
             if inputs_embeds is None:
                 inputs_embeds = decoder.embed_tokens(input_ids)
 
             if decoder.project_in is not None:
                 inputs_embeds = decoder.project_in(inputs_embeds)
             device = input_ids.device if input_ids is not None else inputs_embeds.device
-            _dtype = inputs_embeds.dtype
-
+            inputs_embeds.dtype
+            hidden_states = inputs_embeds
         else:
             if hidden_states is None:
                 raise ValueError("hidden_states shouldn't be None for intermediate stages.")
             input_shape = hidden_states.size()[:-1]
             batch_size, seq_length = input_shape[0], input_shape[1]
             device = hidden_states.device
-            _dtype = hidden_states.dtype
+            hidden_states.dtype
 
         past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0
         # required mask seq length can be calculated via length of past
         mask_seq_length = past_key_values_length + seq_length
         # embed positions
-        if attention_mask is None:
-            attention_mask = torch.ones(batch_size, mask_seq_length, device=device)
-        elif attention_mask.shape[1] != mask_seq_length:
-            raise ValueError(
-                f"The provided attention mask has length {attention_mask.shape[1]}, but its length should be "
-                f"{mask_seq_length} (sum of the lengths of current and past inputs)"
+        if self.decoder._use_flash_attention_2:
+            # 2d mask is passed through the layers
+            causal_attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
+            attention_mask = (
+                torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)
+                if attention_mask is None
+                else attention_mask
+            )
+        else:
+            # 4d mask is passed through the layers
+            if attention_mask is None:
+                attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)
+            elif attention_mask.shape[1] != mask_seq_length:
+                raise ValueError(
+                    f"The provided attention mask has length {attention_mask.shape[1]}, but its length should be "
+                    f"{mask_seq_length} (sum of the lengths of current and past inputs)"
+                )
+            causal_attention_mask = _prepare_4d_causal_attention_mask(
+                attention_mask, input_shape, hidden_states, past_key_values_length
             )
-
-        causal_attention_mask = OPTPipelineForwards._prepare_decoder_attention_mask(
-            attention_mask, input_shape, _dtype, device, past_key_values_length
-        )
 
         if stage_manager.is_first_stage():
+            causal_attention_mask = _get_attention_mask(
+                self,
+                shard_config,
+                inputs_embeds,
+                past_key_values_length,
+                attention_mask,
+            )
             pos_embeds = decoder.embed_positions(attention_mask, past_key_values_length)
             hidden_states = inputs_embeds + pos_embeds
+        else:
+            causal_attention_mask = _get_attention_mask(
+                self,
+                shard_config,
+                hidden_states,
+                past_key_values_length,
+                attention_mask,
+            )
 
         if decoder.gradient_checkpointing and decoder.training:
             if use_cache:
                 logger.warning_once(
                     "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
                 )
                 use_cache = False
@@ -198,28 +215,22 @@
             dropout_probability = random.uniform(0, 1)
             if decoder.training and (dropout_probability < decoder.layerdrop):
                 continue
 
             past_key_value = past_key_values[idx] if past_key_values is not None else None
 
             if decoder.gradient_checkpointing and decoder.training:
-
-                def create_custom_forward(module):
-                    def custom_forward(*inputs):
-                        # None for past_key_value
-                        return module(*inputs, output_attentions, None)
-
-                    return custom_forward
-
-                layer_outputs = torch.utils.checkpoint.checkpoint(
-                    create_custom_forward(decoder_layer),
+                layer_outputs = self._gradient_checkpointing_func(
+                    decoder_layer.__call__,
                     hidden_states,
                     causal_attention_mask,
                     head_mask[idx] if head_mask is not None else None,
                     None,
+                    output_attentions,
+                    use_cache,
                 )
             else:
                 layer_outputs = decoder_layer(
                     hidden_states,
                     attention_mask=causal_attention_mask,
                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),
                     past_key_value=past_key_value,
@@ -245,15 +256,24 @@
         if output_hidden_states:
             all_hidden_states += (hidden_states,)
 
         next_cache = next_decoder_cache if use_cache else None
 
         if stage_manager.is_last_stage():
             if not return_dict:
-                return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
+                return tuple(
+                    v
+                    for v in [
+                        hidden_states,
+                        next_cache,
+                        all_hidden_states,
+                        all_self_attns,
+                    ]
+                    if v is not None
+                )
 
             return BaseModelOutputWithPast(
                 last_hidden_state=hidden_states,
                 past_key_values=next_cache,
                 hidden_states=all_hidden_states,
                 attentions=all_self_attns,
             )
@@ -272,14 +292,15 @@
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         stage_index: Optional[List[int]] = None,
+        shard_config: Optional[ShardConfig] = None,
     ) -> Union[Tuple, CausalLMOutputWithPast]:
         r"""
         This function is modified on the basis of transformers.models.opt.modeling_opt.OPTForCausalLM.forward.
         Please refer to original code of transformers for more details.
         """
 
         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
@@ -299,14 +320,15 @@
             use_cache=use_cache,
             output_attentions=output_attentions,
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             stage_manager=stage_manager,
             hidden_states=hidden_states,
             stage_index=stage_index,
+            shard_config=shard_config,
         )
         if stage_manager.is_last_stage():
             logits = self.lm_head(outputs[0]).contiguous()
             loss = None
             if labels is not None:
                 # move labels to correct device to enable model parallelism
                 labels = labels.to(logits.device)
@@ -343,14 +365,15 @@
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         stage_index: Optional[List[int]] = None,
+        shard_config: Optional[ShardConfig] = None,
     ) -> Union[Tuple, SequenceClassifierOutputWithPast]:
         r"""
         This function is modified on the basis of transformers.models.opt.modeling_opt.OPTForSequenceClassification.forward.
         Please refer to original code of transformers for more details.
         """
 
         logger = logging.get_logger(__name__)
@@ -367,14 +390,15 @@
             use_cache=use_cache,
             output_attentions=output_attentions,
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             stage_manager=stage_manager,
             hidden_states=hidden_states,
             stage_index=stage_index,
+            shard_config=shard_config,
         )
 
         if stage_manager.is_last_stage():
             hidden_states = transformer_outputs[0]
             logits = self.score(hidden_states)
 
             batch_size = input_ids.shape[0] if input_ids is not None else hidden_states.shape[0]
@@ -444,14 +468,15 @@
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         stage_index: Optional[List[int]] = None,
+        shard_config: Optional[ShardConfig] = None,
     ) -> Union[Tuple, QuestionAnsweringModelOutput]:
         r"""
         This function is modified on the basis of transformers.models.opt.modeling_opt.OPTForQuestionAnswering.forward.
         Please refer to original code of transformers for more details.
         """
         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
@@ -465,14 +490,15 @@
             use_cache=use_cache,
             output_attentions=output_attentions,
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             stage_manager=stage_manager,
             hidden_states=hidden_states,
             stage_index=stage_index,
+            shard_config=shard_config,
         )
         if stage_manager.is_last_stage():
             hidden_states = transformer_outputs[0]
 
             logits = self.qa_outputs(hidden_states)
             start_logits, end_logits = logits.split(1, dim=-1)
             start_logits = start_logits.squeeze(-1).contiguous()
@@ -507,100 +533,241 @@
                 attentions=transformer_outputs.attentions,
             )
         else:
             hidden_states = transformer_outputs.get("hidden_states")
             return {"hidden_states": hidden_states}
 
 
-def get_opt_flash_attention_forward():
+def get_opt_flash_attention_forward(shard_config: ShardConfig):
     from transformers.models.opt.modeling_opt import OPTAttention
 
-    from colossalai.nn.layer.colo_attention import AttnMaskType, ColoAttention
-
     def forward(
         self: OPTAttention,
         hidden_states: torch.Tensor,
         key_value_states: Optional[torch.Tensor] = None,
         past_key_value: Optional[Tuple[torch.Tensor]] = None,
-        attention_mask: Optional[torch.Tensor] = None,
+        attention_mask: Optional[dict] = None,
         layer_head_mask: Optional[torch.Tensor] = None,
         output_attentions: bool = False,
     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
         """Input shape: Batch x Time x Channel"""
-
+        assert layer_head_mask is None, "layer_head_mask is not supported for FlashAttention"
         # if key_value_states are provided this layer is used as a cross-attention layer
         # for the decoder
         is_cross_attention = key_value_states is not None
+
         bsz, tgt_len, _ = hidden_states.size()
 
-        attention_input_shape = (bsz, -1, self.num_heads, self.head_dim)
         # get query proj
-        query_states = self.q_proj(hidden_states).view(*attention_input_shape)
+        query_states = self.q_proj(hidden_states)
         # get key, value proj
         if is_cross_attention and past_key_value is not None:
-            # reuse k, v, cross_attentions
-            key_states = past_key_value[0].transpose(1, 2).contiguous().view(*attention_input_shape)
-            value_states = past_key_value[1].transpose(1, 2).contiguous().view(*attention_input_shape)
+            # reuse k,v, cross_attentions
+            key_states = past_key_value[0]
+            value_states = past_key_value[1]
         elif is_cross_attention:
             # cross_attentions
-            key_states = self.k_proj(key_value_states).view(*attention_input_shape)
-            value_states = self.v_proj(key_value_states).view(*attention_input_shape)
+            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)
+            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)
         elif past_key_value is not None:
             # reuse k, v, self_attention
-            key_states = self.k_proj(hidden_states).view(*attention_input_shape)
-            value_states = self.v_proj(hidden_states).view(*attention_input_shape)
-            key_states = torch.cat([past_key_value[0], key_states], dim=1)
-            value_states = torch.cat([past_key_value[1], value_states], dim=1)
+            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
+            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
+            key_states = torch.cat([past_key_value[0], key_states], dim=2)
+            value_states = torch.cat([past_key_value[1], value_states], dim=2)
         else:
             # self_attention
-            key_states = self.k_proj(hidden_states).view(*attention_input_shape)
-            value_states = self.v_proj(hidden_states).view(*attention_input_shape)
+            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
+            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
 
         if self.is_decoder:
             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.
             # Further calls to cross_attention layer can then reuse all cross-attention
             # key/value_states (first "if" case)
             # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of
             # all previous decoder key/value_states. Further calls to uni-directional self-attention
             # can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)
             # if encoder bi-directional self-attention `past_key_value` is always `None`
             past_key_value = (key_states, value_states)
 
-        src_len = key_states.size(1)
-        if layer_head_mask != None:
-            if layer_head_mask.size() != (self.num_heads,):
-                raise ValueError(
-                    f"Head mask for a single layer should be of size {(self.num_heads,)}, but is"
-                    f" {layer_head_mask.size()}"
-                )
+        query_states = self._shape(query_states, tgt_len, bsz)
 
-        flash_attention_mask = None
-        attn_mask_type = AttnMaskType.causal
-        if attention_mask != None:
-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):
-                raise ValueError(
-                    f"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}"
-                )
-            flash_attention_mask = ~(attention_mask[:, :, -1].squeeze(1).to(torch.bool)).contiguous()
-            if not torch.all(flash_attention_mask):
-                attn_mask_type = AttnMaskType.paddedcausal
-
-        attention = ColoAttention(
-            embed_dim=self.embed_dim, num_heads=self.num_heads, dropout=self.dropout, scale=self.scaling
-        )
-        attn_output = attention(
-            query_states, key_states, value_states, attn_mask=flash_attention_mask, attn_mask_type=attn_mask_type
+        dropout_p = self.dropout if self.training else 0.0
+        attn_output = ColoAttention.attention(
+            query_states,
+            key_states,
+            value_states,
+            **attention_mask,
+            dropout_p=dropout_p,
+            scale=self.scaling,
         )
 
+        attn_output = attn_output.transpose(1, 2)
+
+        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be
+        # partitioned aross GPUs when using tensor-parallelism.
+        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)
+
         attn_output = self.out_proj(attn_output)
+
         return attn_output, None, past_key_value
 
     return forward
 
 
+def get_opt_decoder_forward_for_flash_attention(shard_config: ShardConfig):
+    from transformers.models.opt.modeling_opt import OPTDecoder
+
+    def forward(
+        self: OPTDecoder,
+        input_ids: torch.LongTensor = None,
+        attention_mask: Optional[torch.Tensor] = None,
+        head_mask: Optional[torch.Tensor] = None,
+        past_key_values: Optional[List[torch.FloatTensor]] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, BaseModelOutputWithPast]:
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
+        )
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
+
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
+
+        # retrieve input_ids and inputs_embeds
+        if input_ids is not None and inputs_embeds is not None:
+            raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
+        elif input_ids is not None:
+            input_shape = input_ids.size()
+            input_ids = input_ids.view(-1, input_shape[-1])
+        elif inputs_embeds is not None:
+            input_shape = inputs_embeds.size()[:-1]
+        else:
+            raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
+
+        if inputs_embeds is None:
+            inputs_embeds = self.embed_tokens(input_ids)
+
+        batch_size, seq_length = input_shape
+        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0
+        # required mask seq length can be calculated via length of past
+        mask_seq_length = past_key_values_length + seq_length
+
+        # embed positions
+        if attention_mask is None:
+            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)
+        elif attention_mask.shape[1] != mask_seq_length:
+            raise ValueError(
+                f"The provided attention mask has length {attention_mask.shape[1]}, but its length should be "
+                f"{mask_seq_length} (sum of the lengths of current and past inputs)"
+            )
+        causal_attention_mask = _get_attention_mask(
+            self, shard_config, inputs_embeds, past_key_values_length, attention_mask
+        )
+        pos_embeds = self.embed_positions(attention_mask, past_key_values_length)
+
+        if self.project_in is not None:
+            inputs_embeds = self.project_in(inputs_embeds)
+
+        hidden_states = inputs_embeds + pos_embeds
+
+        if self.gradient_checkpointing and self.training:
+            if use_cache:
+                logger.warning_once(
+                    "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
+                )
+                use_cache = False
+
+        # decoder layers
+        all_hidden_states = () if output_hidden_states else None
+        all_self_attns = () if output_attentions else None
+        next_decoder_cache = () if use_cache else None
+
+        # check if head_mask has a correct number of layers specified if desired
+        for attn_mask, mask_name in zip([head_mask], ["head_mask"]):
+            if attn_mask is not None:
+                if attn_mask.size()[0] != (len(self.layers)):
+                    raise ValueError(
+                        f"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for"
+                        f" {head_mask.size()[0]}."
+                    )
+
+        for idx, decoder_layer in enumerate(self.layers):
+            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
+            if output_hidden_states:
+                all_hidden_states += (hidden_states,)
+
+            if self.training:
+                dropout_probability = torch.rand([])
+                if dropout_probability < self.layerdrop:
+                    continue
+
+            past_key_value = past_key_values[idx] if past_key_values is not None else None
+
+            if self.gradient_checkpointing and self.training:
+
+                def create_custom_forward(module):
+                    def custom_forward(*inputs):
+                        # None for past_key_value
+                        return module(*inputs, output_attentions, None)
+
+                    return custom_forward
+
+                layer_outputs = torch.utils.checkpoint.checkpoint(
+                    create_custom_forward(decoder_layer),
+                    hidden_states,
+                    causal_attention_mask,
+                    head_mask[idx] if head_mask is not None else None,
+                    None,
+                )
+            else:
+                layer_outputs = decoder_layer(
+                    hidden_states,
+                    attention_mask=causal_attention_mask,
+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),
+                    past_key_value=past_key_value,
+                    output_attentions=output_attentions,
+                    use_cache=use_cache,
+                )
+
+            hidden_states = layer_outputs[0]
+
+            if use_cache:
+                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)
+
+            if output_attentions:
+                all_self_attns += (layer_outputs[1],)
+
+        if self.final_layer_norm is not None:
+            hidden_states = self.final_layer_norm(hidden_states)
+
+        if self.project_out is not None:
+            hidden_states = self.project_out(hidden_states)
+
+        # add hidden states from the last decoder layer
+        if output_hidden_states:
+            all_hidden_states += (hidden_states,)
+
+        next_cache = next_decoder_cache if use_cache else None
+        if not return_dict:
+            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)
+        return BaseModelOutputWithPast(
+            last_hidden_state=hidden_states,
+            past_key_values=next_cache,
+            hidden_states=all_hidden_states,
+            attentions=all_self_attns,
+        )
+
+    return forward
+
+
 def get_jit_fused_opt_decoder_layer_forward():
     from transformers.models.opt.modeling_opt import OPTDecoderLayer
 
     def forward(
         self: OPTDecoderLayer,
         hidden_states: torch.Tensor,
         attention_mask: Optional[torch.Tensor] = None,
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/sam.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/sam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/t5.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/t5.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 import warnings
 from typing import Dict, List, Optional, Tuple, Union
 
 import torch
 from torch.nn import CrossEntropyLoss
-from torch.utils.checkpoint import checkpoint
 from transformers.modeling_outputs import (
     BaseModelOutput,
     BaseModelOutputWithPastAndCrossAttentions,
     Seq2SeqLMOutput,
     Seq2SeqModelOutput,
 )
 from transformers.models.t5.modeling_t5 import T5EncoderModel, T5ForConditionalGeneration, T5Model, T5Stack
@@ -114,35 +113,32 @@
             input_shape = hidden_states.size()[:-1]
             batch_size, seq_length = input_shape[0], input_shape[1]
             device = hidden_states.device
 
         # required mask seq length can be calculated via length of past
         mask_seq_length = past_key_values[0][0].shape[2] + seq_length if past_key_values is not None else seq_length
 
-        if attention_mask is None:
-            attention_mask = torch.ones(batch_size, mask_seq_length, device=device)
-        if in_decoder and encoder_attention_mask is None and encoder_hidden_states is not None:
-            encoder_seq_length = encoder_hidden_states.shape[1]
-            encoder_attention_mask = torch.ones(batch_size, encoder_seq_length, device=device, dtype=torch.long)
-
         # initialize past_key_values with `None` if past does not exist
         if past_key_values is None:
             past_key_values = [None] * len(self.block)
 
+        if attention_mask is None:
+            attention_mask = torch.ones(batch_size, mask_seq_length, device=device)
+
         # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]
         # ourselves in which case we just need to make it broadcastable to all heads.
         extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)
 
         # If a 2D or 3D attention mask is provided for the cross-attention
         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]
         if self.is_decoder and encoder_hidden_states is not None:
             encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()
             encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)
             if encoder_attention_mask is None:
-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=inputs_embeds.device)
+                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=inputs_embeds.device, dtype=torch.long)
             encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)
         else:
             encoder_extended_attention_mask = None
 
         # Prepare head mask if needed
         head_mask = self.get_head_mask(head_mask, self.config.num_layers)
         cross_attn_head_mask = self.get_head_mask(cross_attn_head_mask, self.config.num_layers)
@@ -158,32 +154,27 @@
             past_key_value = past_key_values[i]
             layer_module = self.block[i]
             layer_head_mask = head_mask[i]
             cross_attn_layer_head_mask = cross_attn_head_mask[i]
             torch.cuda.set_device(hidden_states.device)
 
             if self.gradient_checkpointing and self.training:
-
-                def create_custom_forward(module):
-                    def custom_forward(*inputs):
-                        return tuple(module(*inputs, use_cache, output_attentions))
-
-                    return custom_forward
-
-                layer_outputs = checkpoint(
-                    create_custom_forward(layer_module),
+                layer_outputs = self._gradient_checkpointing_func(
+                    layer_module.forward,
                     hidden_states,
                     extended_attention_mask,
                     position_bias,
                     encoder_hidden_states,
                     encoder_extended_attention_mask,
                     encoder_decoder_position_bias,
                     layer_head_mask,
                     cross_attn_layer_head_mask,
                     None,  # past_key_value is always None with gradient checkpointing
+                    use_cache,
+                    output_attentions,
                 )
             else:
                 layer_outputs = layer_module(
                     hidden_states,
                     attention_mask=extended_attention_mask,
                     position_bias=position_bias,
                     encoder_hidden_states=encoder_hidden_states,
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/vit.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/vit.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,46 +1,42 @@
-import math
 from typing import List, Optional, Tuple, Union
 
 import torch
 from transformers.models.vit.modeling_vit import BaseModelOutput, ViTEncoder
 from transformers.utils import logging
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
+from colossalai.shardformer.layer import ColoAttention
 
 
 def _encoder_forward(
     encoder: ViTEncoder,
     start_idx: int,
     end_idx: int,
     hidden_states: torch.Tensor,
     head_mask: Optional[torch.Tensor] = None,
+    output_attentions: bool = False,
+    output_hidden_states: bool = False,
     return_dict: bool = True,
     stage_manager: PipelineStageManager = None,
 ) -> Union[tuple, BaseModelOutput]:
     for i in range(start_idx, end_idx):
         layer_module = encoder.layer[i]
 
         layer_head_mask = head_mask[i] if head_mask is not None else None
 
         if encoder.gradient_checkpointing and encoder.training:
-
-            def create_custom_forward(module):
-                def custom_forward(*inputs):
-                    return module(*inputs, False)
-
-                return custom_forward
-
-            layer_outputs = torch.utils.checkpoint.checkpoint(
-                create_custom_forward(layer_module),
+            layer_outputs = encoder._gradient_checkpointing_func(
+                layer_module.__call__,
                 hidden_states,
                 layer_head_mask,
+                output_attentions,
             )
         else:
-            layer_outputs = layer_module(hidden_states, layer_head_mask, False)
+            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)
 
         hidden_states = layer_outputs[0]
     if not stage_manager.is_last_stage():
         return hidden_states
     else:
         if not return_dict:
             return tuple(hidden_states)
@@ -94,28 +90,32 @@
 
             # TODO(FoolPlayer): maybe have a cleaner way to cast the input (from `ImageProcessor` side?)
             expected_dtype = self.embeddings.patch_embeddings.projection.weight.dtype
             if pixel_values.dtype != expected_dtype:
                 pixel_values = pixel_values.to(expected_dtype)
 
             embedding_output = self.embeddings(
-                pixel_values, bool_masked_pos=bool_masked_pos, interpolate_pos_encoding=interpolate_pos_encoding
+                pixel_values,
+                bool_masked_pos=bool_masked_pos,
+                interpolate_pos_encoding=interpolate_pos_encoding,
             )
             hidden_states = embedding_output
         else:
             assert (
                 hidden_states is not None
             ), f"Current stage is {stage_manager.stage}, hidden_states should not be None"
 
         encoder_outputs = _encoder_forward(
             encoder=self.encoder,
             start_idx=stage_index[0],
             end_idx=stage_index[1],
             hidden_states=hidden_states,
             head_mask=head_mask,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             stage_manager=stage_manager,
         )
         if not stage_manager.is_last_stage():
             return {"hidden_states": encoder_outputs}
 
         sequence_output = encoder_outputs[0]
@@ -332,42 +332,35 @@
 
     return pp_forward
 
 
 def get_vit_flash_self_attention_forward():
     from transformers.models.vit.modeling_vit import ViTSelfAttention
 
-    from colossalai.nn.layer.colo_attention import ColoAttention
-
-    def transpose_for_scores(x: torch.Tensor, num_attention_heads, attention_head_size) -> torch.Tensor:
-        new_x_shape = x.size()[:-1] + (num_attention_heads, attention_head_size)
-        x = x.view(new_x_shape)
-        return x
-
     def forward(
         self: ViTSelfAttention,
         hidden_states: torch.Tensor,
         head_mask: Optional[torch.Tensor] = None,
         output_attentions: bool = False,
     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:
+        assert head_mask is None, "head_mask is not supported for FlashAttention"
         mixed_query_layer = self.query(hidden_states)
 
-        key_layer = transpose_for_scores(self.key(hidden_states), self.num_attention_heads, self.attention_head_size)
-        value_layer = transpose_for_scores(
-            self.value(hidden_states), self.num_attention_heads, self.attention_head_size
-        )
-        query_layer = transpose_for_scores(mixed_query_layer, self.num_attention_heads, self.attention_head_size)
+        key_layer = self.transpose_for_scores(self.key(hidden_states))
+        value_layer = self.transpose_for_scores(self.value(hidden_states))
+        query_layer = self.transpose_for_scores(mixed_query_layer)
+
+        dropout_p = self.dropout.p if self.training else 0.0
+        context_layer = ColoAttention.attention(query_layer, key_layer, value_layer, dropout_p=dropout_p)
+
+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
+        context_layer = context_layer.view(new_context_layer_shape)
 
-        scale = 1.0 / math.sqrt(self.attention_head_size)
-        attention = ColoAttention(
-            embed_dim=self.all_head_size, num_heads=self.num_attention_heads, dropout=self.dropout.p, scale=scale
-        )
-        context_layer = attention(query_layer, key_layer, value_layer)
-
-        outputs = (context_layer,)
+        outputs = (context_layer, None) if output_attentions else (context_layer,)
 
         return outputs
 
     return forward
 
 
 def get_jit_fused_vit_output_forward():
@@ -375,7 +368,19 @@
 
     def forward(self: ViTOutput, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
         hidden_states = self.dropout_add(hidden_states, input_tensor, self.dropout.p, self.dropout.training)
         return hidden_states
 
     return forward
+
+
+def get_jit_fused_vit_intermediate_forward():
+    from colossalai.kernel.jit.bias_gelu import GeLUFunction as JitGeLUFunction
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        hidden_states, bias = self.dense(hidden_states)
+        hidden_states = JitGeLUFunction.apply(hidden_states, bias)
+
+        return hidden_states
+
+    return forward
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/modeling/whisper.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/modeling/whisper.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,130 +1,313 @@
 import logging
 import random
 from typing import List, Optional, Tuple, Union
 
 import torch
 from torch import nn
 from torch.nn import CrossEntropyLoss
+from transformers.modeling_attn_mask_utils import (
+    _prepare_4d_causal_attention_mask,
+    _prepare_4d_causal_attention_mask_for_sdpa,
+)
 from transformers.modeling_outputs import (
     BaseModelOutput,
     BaseModelOutputWithPastAndCrossAttentions,
     Seq2SeqLMOutput,
     Seq2SeqModelOutput,
     SequenceClassifierOutput,
 )
 from transformers.models.whisper.modeling_whisper import (
+    WhisperDecoder,
     WhisperEncoder,
     WhisperForAudioClassification,
     WhisperForConditionalGeneration,
     WhisperModel,
+    shift_tokens_right,
 )
 from transformers.utils import logging
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
+from colossalai.shardformer.layer import ColoAttention
+from colossalai.shardformer.shard import ShardConfig
 
+logger = logging.get_logger(__name__)
 
-def get_whisper_flash_attention_forward():
-    from transformers.models.whisper.modeling_whisper import WhisperAttention
 
-    from colossalai.nn.layer.colo_attention import AttnMaskType, ColoAttention
+def _get_attention_mask(
+    self: WhisperDecoder,
+    shard_config: ShardConfig,
+    hidden_states: torch.Tensor,
+    past_key_values_length: int,
+    attention_mask: Optional[torch.FloatTensor],
+    head_mask: Optional[torch.Tensor] = None,
+    output_attentions: bool = False,
+):
+    batch_size, seq_length = hidden_states.shape[:2]
+    mask_seq_length = past_key_values_length + seq_length
+    if shard_config.enable_flash_attention:
+        attention_mask = ColoAttention.prepare_attn_kwargs(
+            (batch_size, 1, seq_length, mask_seq_length),
+            hidden_states.dtype,
+            hidden_states.device,
+            attention_mask,
+            is_causal=True,
+        )
+    else:
+        input_shape = (batch_size, seq_length)
+        if self._use_flash_attention_2:
+            # 2d mask is passed through the layers
+            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
+        elif self._use_sdpa and head_mask is None and not output_attentions:
+            # output_attentions=True & head_mask can not be supported when using SDPA.
+            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(
+                attention_mask, input_shape, hidden_states, past_key_values_length
+            )
+        else:
+            # 4d mask is passed through the layers
+            attention_mask = _prepare_4d_causal_attention_mask(
+                attention_mask, input_shape, hidden_states, past_key_values_length
+            )
+    return attention_mask
 
-    def shape(tensor: torch.Tensor, seq_len: int, bsz: int, num_heads: int, head_dim: int):
-        return tensor.view(bsz, seq_len, num_heads, head_dim).contiguous()
+
+def get_whisper_flash_attention_forward():
+    from transformers.models.whisper.modeling_whisper import WhisperAttention
 
     def forward(
         self: WhisperAttention,
         hidden_states: torch.Tensor,
         key_value_states: Optional[torch.Tensor] = None,
         past_key_value: Optional[Tuple[torch.Tensor]] = None,
-        attention_mask: Optional[torch.Tensor] = None,
+        attention_mask: Optional[dict] = None,
         layer_head_mask: Optional[torch.Tensor] = None,
         output_attentions: bool = False,
     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
         """Input shape: Batch x Time x Channel"""
-
+        assert layer_head_mask is None, "layer_head_mask is not supported for FlashAttention"
+        # for encoder, attention_mask is None
+        if attention_mask is None:
+            attention_mask = {}
         # if key_value_states are provided this layer is used as a cross-attention layer
         # for the decoder
         is_cross_attention = key_value_states is not None
 
         bsz, tgt_len, _ = hidden_states.size()
 
+        # get query proj
+        query_states = self.q_proj(hidden_states)
         # get key, value proj
         # `past_key_value[0].shape[2] == key_value_states.shape[1]`
         # is checking that the `sequence_length` of the `past_key_value` is the same as
         # the provided `key_value_states` to support prefix tuning
         if (
             is_cross_attention
             and past_key_value is not None
-            and past_key_value[0].shape[1] == key_value_states.shape[1]
+            and past_key_value[0].shape[2] == key_value_states.shape[1]
         ):
             # reuse k,v, cross_attentions
             key_states = past_key_value[0]
             value_states = past_key_value[1]
         elif is_cross_attention:
             # cross_attentions
-            key_states = shape(self.k_proj(key_value_states), -1, bsz, self.num_heads, self.head_dim)
-            value_states = shape(self.v_proj(key_value_states), -1, bsz, self.num_heads, self.head_dim)
+            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)
+            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)
         elif past_key_value is not None:
             # reuse k, v, self_attention
-            key_states = shape(self.k_proj(hidden_states), -1, bsz, self.num_heads, self.head_dim)
-            value_states = shape(self.v_proj(hidden_states), -1, bsz, self.num_heads, self.head_dim)
-            key_states = torch.cat([past_key_value[0], key_states], dim=1)
-            value_states = torch.cat([past_key_value[1], value_states], dim=1)
+            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
+            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
+            key_states = torch.cat([past_key_value[0], key_states], dim=2)
+            value_states = torch.cat([past_key_value[1], value_states], dim=2)
         else:
             # self_attention
-            key_states = shape(self.k_proj(hidden_states), -1, bsz, self.num_heads, self.head_dim)
-            value_states = shape(self.v_proj(hidden_states), -1, bsz, self.num_heads, self.head_dim)
+            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
+            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
 
         if self.is_decoder:
             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.
             # Further calls to cross_attention layer can then reuse all cross-attention
             # key/value_states (first "if" case)
             # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of
             # all previous decoder key/value_states. Further calls to uni-directional self-attention
             # can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)
             # if encoder bi-directional self-attention `past_key_value` is always `None`
             past_key_value = (key_states, value_states)
 
-        # get query proj
-        query_states = shape(self.q_proj(hidden_states), tgt_len, bsz, self.num_heads, self.head_dim)
+        query_states = self._shape(query_states, tgt_len, bsz)
 
-        src_len = key_states.size(1)
-        if layer_head_mask is not None:
-            if layer_head_mask.size() != (self.num_heads,):
-                raise ValueError(
-                    f"Head mask for a single layer should be of size {(self.num_heads,)}, but is"
-                    f" {layer_head_mask.size()}"
-                )
+        dropout_p = self.dropout if self.training else 0.0
+        attn_output = ColoAttention.attention(
+            query_states,
+            key_states,
+            value_states,
+            **attention_mask,
+            dropout_p=dropout_p,
+            scale=self.scaling,
+        )
+        attn_output = attn_output.transpose(1, 2)
 
-        attn_type = None
-        flash_attention_mask = None
+        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be
+        # partitioned across GPUs when using tensor-parallelism.
+        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)
 
-        if self.is_decoder:
-            if attention_mask is not None:
-                if attention_mask.size() != (bsz, 1, tgt_len, src_len):
-                    raise ValueError(
-                        f"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}"
-                    )
-                flash_attention_mask = ~(attention_mask[:, :, -1].squeeze(1).to(torch.bool).contiguous())
-                if not torch.all(flash_attention_mask):
-                    attn_type = AttnMaskType.paddedcausal
-                else:
-                    attn_type = AttnMaskType.causal
+        attn_output = self.out_proj(attn_output)
 
-        attention = ColoAttention(
-            embed_dim=self.embed_dim, num_heads=self.num_heads, dropout=self.dropout, scale=self.scaling
-        )
-        attn_output = attention(
-            query_states, key_states, value_states, attn_mask=flash_attention_mask, attn_mask_type=attn_type
+        return attn_output, None, past_key_value
+
+    return forward
+
+
+def get_whisper_decoder_forward_for_flash_attention(shard_config: ShardConfig):
+    def forward(
+        self: WhisperDecoder,
+        input_ids=None,
+        attention_mask=None,
+        encoder_hidden_states=None,
+        head_mask=None,
+        cross_attn_head_mask=None,
+        past_key_values=None,
+        inputs_embeds=None,
+        use_cache=None,
+        output_attentions=None,
+        output_hidden_states=None,
+        return_dict=None,
+    ):
+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
+        output_hidden_states = (
+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
         )
+        use_cache = use_cache if use_cache is not None else self.config.use_cache
+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
 
-        attn_output = self.out_proj(attn_output)
+        # retrieve input_ids and inputs_embeds
+        if input_ids is not None and inputs_embeds is not None:
+            raise ValueError("You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time")
+        elif input_ids is not None:
+            input_shape = input_ids.size()
+            input_ids = input_ids.view(-1, input_shape[-1])
+        elif inputs_embeds is not None:
+            input_shape = inputs_embeds.size()[:-1]
+        else:
+            raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
 
-        return attn_output, None, past_key_value
+        # past_key_values_length
+        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0
+
+        if inputs_embeds is None:
+            inputs_embeds = self.embed_tokens(input_ids)
+
+        attention_mask = _get_attention_mask(self, shard_config, inputs_embeds, past_key_values_length, attention_mask)
+
+        # embed positions
+        if input_ids is not None:
+            positions = self.embed_positions(input_ids, past_key_values_length=past_key_values_length)
+        else:
+            positions = self.embed_positions(inputs_embeds, past_key_values_length=past_key_values_length)
+
+        hidden_states = inputs_embeds + positions
+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
+
+        if self.gradient_checkpointing and self.training:
+            if use_cache:
+                logger.warning_once(
+                    "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`..."
+                )
+                use_cache = False
+        # decoder layers
+        all_hidden_states = () if output_hidden_states else None
+        all_self_attns = () if output_attentions else None
+        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None
+        next_decoder_cache = () if use_cache else None
+
+        # check if head_mask/cross_attn_head_mask has a correct number of layers specified if desired
+        for attn_mask, mask_name in zip([head_mask, cross_attn_head_mask], ["head_mask", "cross_attn_head_mask"]):
+            if attn_mask is not None:
+                assert attn_mask.size()[0] == (len(self.layers)), (
+                    f"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for"
+                    f" {head_mask.size()[0]}."
+                )
+        for idx, decoder_layer in enumerate(self.layers):
+            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
+            if output_hidden_states:
+                all_hidden_states += (hidden_states,)
+            if self.training:
+                dropout_probability = torch.rand([])
+                if dropout_probability < self.layerdrop:
+                    continue
+
+            past_key_value = past_key_values[idx] if past_key_values is not None else None
+
+            if self.gradient_checkpointing and self.training:
+
+                def create_custom_forward(module):
+                    def custom_forward(*inputs):
+                        # None for past_key_value
+                        return module(*inputs, output_attentions, use_cache)
+
+                    return custom_forward
+
+                layer_outputs = torch.utils.checkpoint.checkpoint(
+                    create_custom_forward(decoder_layer),
+                    hidden_states,
+                    attention_mask,
+                    encoder_hidden_states,
+                    None,  # encoder attention mask
+                    head_mask[idx] if head_mask is not None else None,
+                    (cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),
+                    None,  # past_key_value
+                )
+            else:
+                layer_outputs = decoder_layer(
+                    hidden_states,
+                    attention_mask=attention_mask,
+                    encoder_hidden_states=encoder_hidden_states,
+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),
+                    cross_attn_layer_head_mask=(
+                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None
+                    ),
+                    past_key_value=past_key_value,
+                    output_attentions=output_attentions,
+                    use_cache=use_cache,
+                )
+            hidden_states = layer_outputs[0]
+
+            if use_cache:
+                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)
+
+            if output_attentions:
+                all_self_attns += (layer_outputs[1],)
+
+                if encoder_hidden_states is not None:
+                    all_cross_attentions += (layer_outputs[2],)
+
+        hidden_states = self.layer_norm(hidden_states)
+        # add hidden states from the last decoder layer
+        if output_hidden_states:
+            all_hidden_states += (hidden_states,)
+
+        next_cache = next_decoder_cache if use_cache else None
+        if not return_dict:
+            return tuple(
+                v
+                for v in [
+                    hidden_states,
+                    next_cache,
+                    all_hidden_states,
+                    all_self_attns,
+                    all_cross_attentions,
+                ]
+                if v is not None
+            )
+        return BaseModelOutputWithPastAndCrossAttentions(
+            last_hidden_state=hidden_states,
+            past_key_values=next_cache,
+            hidden_states=all_hidden_states,
+            attentions=all_self_attns,
+            cross_attentions=all_cross_attentions,
+        )
 
     return forward
 
 
 def get_jit_fused_whisper_encoder_layer_forward():
     from transformers.models.whisper.modeling_whisper import WhisperEncoderLayer
 
@@ -288,14 +471,15 @@
         return_dict=None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         encoder_states=None,
         all_attentions=None,
         stage_index: Optional[List[int]] = None,
         decoder_starting_stage: Optional[int] = None,
+        shard_config: Optional[ShardConfig] = None,
     ):
         r"""
         Args:
             input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):
                 Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be
                 obtained by loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a
                 `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into
@@ -365,26 +549,20 @@
                 encoder_states = encoder_states + (hidden_states,)
             # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
             dropout_probability = random.uniform(0, 1)
             if self.training and (dropout_probability < self.layerdrop):  # skip the layer
                 layer_outputs = (None, None)
             else:
                 if self.gradient_checkpointing and self.training:
-
-                    def create_custom_forward(module):
-                        def custom_forward(*inputs):
-                            return module(*inputs, output_attentions)
-
-                        return custom_forward
-
-                    layer_outputs = torch.utils.checkpoint.checkpoint(
-                        create_custom_forward(encoder_layer),
+                    layer_outputs = self._gradient_checkpointing_func(
+                        encoder_layer.__call__,
                         hidden_states,
                         None,
                         (head_mask[idx] if head_mask is not None else None),
+                        output_attentions,
                     )
                 else:
                     layer_outputs = encoder_layer(
                         hidden_states,
                         None,
                         layer_head_mask=(head_mask[idx] if head_mask is not None else None),
                         output_attentions=output_attentions,
@@ -399,23 +577,25 @@
             hidden_states = self.layer_norm(hidden_states)
             if output_hidden_states:
                 encoder_states = encoder_states + (hidden_states,)
 
             if not return_dict:
                 return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)
             return BaseModelOutput(
-                last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions
+                last_hidden_state=hidden_states,
+                hidden_states=encoder_states,
+                attentions=all_attentions,
             )
 
         else:
             return {"hidden_states": hidden_states, "head_mask": head_mask}
 
     @staticmethod
     def whisper_decoder_forward(
-        self,
+        self: WhisperDecoder,
         input_ids=None,
         attention_mask=None,
         encoder_hidden_states=None,
         head_mask=None,
         cross_attn_head_mask=None,
         past_key_values=None,
         inputs_embeds=None,
@@ -423,14 +603,15 @@
         output_attentions=None,
         output_hidden_states=None,
         return_dict=None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         stage_index: Optional[List[int]] = None,
         decoder_starting_stage: Optional[int] = None,
+        shard_config: Optional[ShardConfig] = None,
     ):
         r"""
         Args:
             input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
                 Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
                 provide it.
 
@@ -525,24 +706,24 @@
                 input_shape = inputs_embeds.size()[:-1]
             else:
                 raise ValueError("You have to specify either decoder_input_ids or decoder_inputs_embeds")
 
             if inputs_embeds is None:
                 inputs_embeds = self.embed_tokens(input_ids)
 
+            attention_mask = _get_attention_mask(
+                self, shard_config, inputs_embeds, past_key_values_length, attention_mask
+            )
+
             # embed positions
             if input_ids is not None:
                 positions = self.embed_positions(input_ids, past_key_values_length=past_key_values_length)
             else:
                 positions = self.embed_positions(inputs_embeds, past_key_values_length=past_key_values_length)
 
-            attention_mask = self._prepare_decoder_attention_mask(
-                attention_mask, input_shape, inputs_embeds, past_key_values_length
-            )
-
             hidden_states = inputs_embeds + positions
             hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
 
             if self.gradient_checkpointing and self.training:
                 if use_cache:
                     logger.warning_once(
                         "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`..."
@@ -551,17 +732,20 @@
 
         else:
             if hidden_states is None:
                 raise ValueError(
                     "hidden_states shouldn't be None for stages other than the first stage of encoder/decoder."
                 )
             input_shape = hidden_states.size()[:-1]
-
-            attention_mask = self._prepare_decoder_attention_mask(
-                attention_mask, input_shape, hidden_states, past_key_values_length
+            attention_mask = _get_attention_mask(
+                self,
+                shard_config,
+                hidden_states,
+                past_key_values_length,
+                attention_mask,
             )
 
         start_idx, end_idx = stage_index[0], stage_index[1]
 
         for idx in range(start_idx, end_idx):
             # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
             decoder_layer = self.layers[idx]
@@ -571,31 +755,25 @@
             dropout_probability = random.uniform(0, 1)
             if self.training and (dropout_probability < self.layerdrop):
                 continue
 
             past_key_value = past_key_values[idx] if past_key_values is not None else None
 
             if self.gradient_checkpointing and self.training:
-
-                def create_custom_forward(module):
-                    def custom_forward(*inputs):
-                        # None for past_key_value
-                        return module(*inputs, output_attentions, use_cache)
-
-                    return custom_forward
-
-                layer_outputs = torch.utils.checkpoint.checkpoint(
-                    create_custom_forward(decoder_layer),
+                layer_outputs = self._gradient_checkpointing_func(
+                    decoder_layer.__call__,
                     hidden_states,
                     attention_mask,
                     encoder_hidden_states,
                     None,  # encoder attention mask
                     head_mask[idx] if head_mask is not None else None,
-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,
+                    (cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),
                     None,  # past_key_value
+                    output_attentions,
+                    use_cache,
                 )
             else:
                 layer_outputs = decoder_layer(
                     hidden_states,
                     attention_mask=attention_mask,
                     encoder_hidden_states=encoder_hidden_states,
                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),
@@ -622,15 +800,21 @@
             # add hidden states from the last decoder layer
             if output_hidden_states:
                 all_hidden_states += (hidden_states,)
             next_cache = next_decoder_cache if use_cache else None
             if not return_dict:
                 return tuple(
                     v
-                    for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]
+                    for v in [
+                        hidden_states,
+                        next_cache,
+                        all_hidden_states,
+                        all_self_attns,
+                        all_cross_attentions,
+                    ]
                     if v is not None
                 )
             return BaseModelOutputWithPastAndCrossAttentions(
                 last_hidden_state=hidden_states,
                 past_key_values=next_cache,
                 hidden_states=all_hidden_states,
                 attentions=all_self_attns,
@@ -662,14 +846,15 @@
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         encoder_hidden_states: Optional[torch.FloatTensor] = None,
         stage_index: Optional[List[int]] = None,
         decoder_starting_stage: Optional[int] = None,
+        shard_config: Optional[ShardConfig] = None,
     ):
         r"""
         Returns:
 
         Example:
          ```python
          >>> import torch
@@ -731,15 +916,15 @@
                     return {"encoder_hidden_states": encoder_outputs[0]}
                 else:
                     return encoder_outputs
             # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True
             elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):
                 encoder_outputs = BaseModelOutput(
                     last_hidden_state=encoder_outputs[0],
-                    hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,
+                    hidden_states=(encoder_outputs[1] if len(encoder_outputs) > 1 else None),
                     attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,
                 )
 
         at_last_decoder_stage = stage_manager.is_last_stage()
         at_first_decoder_stage = stage_manager.stage == decoder_starting_stage
         if encoder_outputs is not None:
             encoder_hidden_states = encoder_outputs[0]
@@ -763,14 +948,15 @@
             output_attentions=output_attentions,
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             stage_manager=stage_manager,
             hidden_states=hidden_states,
             stage_index=stage_index,
             decoder_starting_stage=decoder_starting_stage,
+            shard_config=shard_config,
         )
 
         # Directly return outputs of overloaded Whisper forward if not at last stage.
         if not at_last_decoder_stage:
             # encoder_hidden_states should be passed to the next stage
             decoder_outputs["encoder_hidden_states"] = encoder_hidden_states
             return decoder_outputs
@@ -806,14 +992,15 @@
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         encoder_hidden_states: Optional[torch.FloatTensor] = None,
         stage_index: Optional[List[int]] = None,
         decoder_starting_stage: Optional[int] = None,
+        shard_config: Optional[ShardConfig] = None,
     ) -> Union[Tuple[torch.Tensor], Seq2SeqLMOutput]:
         r"""
         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
             Labels for computing the language modeling loss. Indices should either be in `[0, ..., config.vocab_size]`
             or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored (masked), the loss is
             only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
 
@@ -866,14 +1053,15 @@
             output_hidden_states=output_hidden_states,
             return_dict=return_dict,
             stage_manager=stage_manager,
             hidden_states=hidden_states,
             encoder_hidden_states=encoder_hidden_states,
             stage_index=stage_index,
             decoder_starting_stage=decoder_starting_stage,
+            shard_config=shard_config,
         )
         if not in_decoder:
             return outputs
 
         if not at_last_decoder_stage:
             # encoder_hidden_states should be passed to the next stage
             outputs["encoder_hidden_states"] = encoder_hidden_states
@@ -916,14 +1104,15 @@
         return_dict: Optional[bool] = None,
         stage_manager: Optional[PipelineStageManager] = None,
         hidden_states: Optional[torch.FloatTensor] = None,
         encoder_states=None,
         all_attentions=None,
         stage_index: Optional[List[int]] = None,
         decoder_starting_stage: Optional[int] = None,
+        shard_config: Optional[ShardConfig] = None,
     ):
         r"""
         This function is modified on the basis of transformers.models.whisper.modeling_whisper.WhisperForAudioClassification.forward.
         Please refer to original code of transformers for more details.
         """
         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
         output_hidden_states = (
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/policies/auto_policy.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/auto_policy.py`

 * *Files 6% similar despite different names*

```diff
@@ -147,18 +147,18 @@
     "transformers.models.blip_2.modeling_blip_2.Blip2Model": PolicyLocation(
         file_name="blip2", class_name="Blip2ModelPolicy"
     ),
     "transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGeneration": PolicyLocation(
         file_name="blip2", class_name="Blip2ForConditionalGenerationPolicy"
     ),
     # ChatGLM
-    "colossalai.shardformer.modeling.chatglm2_6b.modeling_chatglm.ChatGLMModel": PolicyLocation(
+    "transformers_modules.modeling_chatglm.ChatGLMModel": PolicyLocation(
         file_name="chatglm2", class_name="ChatGLMModelPolicy"
     ),
-    "colossalai.shardformer.modeling.chatglm2_6b.modeling_chatglm.ChatGLMForConditionalGeneration": PolicyLocation(
+    "transformers_modules.modeling_chatglm.ChatGLMForConditionalGeneration": PolicyLocation(
         file_name="chatglm2", class_name="ChatGLMForConditionalGenerationPolicy"
     ),
     # Falcon
     "transformers.models.falcon.modeling_falcon.FalconModel": PolicyLocation(
         file_name="falcon", class_name="FalconModelPolicy"
     ),
     "transformers.models.falcon.modeling_falcon.FalconForCausalLM": PolicyLocation(
@@ -198,14 +198,21 @@
     """
     Return the full name of an object, including the module name.
     """
     klass = obj.__class__
     module = klass.__module__
     if module == "builtins":
         return klass.__qualname__  # avoid outputs like 'builtins.str'
+    # patch custom models which are not in transformers
+    # it can be like 'transformers_modules.THUDM.chatglm3-6b.103caa40027ebfd8450289ca2f278eac4ff26405.modeling_chatglm' (from huggingface hub)
+    # or like 'transformers_modules.chatglm.modeling_chatglm' (from local directory)
+    if module.startswith("transformers_modules"):
+        split_module = module.split(".")
+        if len(split_module) >= 2:
+            module = f"{split_module[0]}.{split_module[-1]}"
     return module + "." + klass.__qualname__
 
 
 def get_autopolicy(model: nn.Module) -> Policy:
     r"""
     Return the auto policy for the model
 
@@ -216,12 +223,12 @@
         :class:`Policy`: The auto policy for the model
     """
     full_name = _fullname(model)
     policy_location = _POLICY_LIST.get(full_name, None)
 
     if policy_location is None:
         raise NotImplementedError(
-            f"Auto policy for {model.__class__.__qualname__} is not implemented\n. Supported models are {list(_POLICY_LIST.keys())}"
+            f"Auto policy for {model.__class__.__qualname__} ({full_name}) is not implemented\n. Supported models are {list(_POLICY_LIST.keys())}"
         )
     else:
         policy = import_policy(policy_location)
     return policy()
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/policies/base_policy.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/base_policy.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # part of code modified from https://github.com/tunib-ai/parallelformers
 
 from abc import ABC, abstractmethod
 from dataclasses import dataclass
-from typing import Any, Callable, Dict, List, Optional, Tuple, Union
+from typing import Any, Callable, Dict, List, Optional, Union
 
-import numpy as np
 import torch.nn as nn
 from torch import Tensor
 from torch.nn import Module
 
 from colossalai.pipeline.stage_manager import PipelineStageManager
 
 from ..layer.normalization import BaseLayerNorm
@@ -25,14 +24,15 @@
 
     Args:
         suffix (str): used to get the submodule object
         target_module (ParallelModule): specifies the module class used to replace to submodule
         kwargs (Dict[str, Any]): the dictionary used to pass extra arguments to the `ParallelModule.from_native_module` method.
         ignore_if_not_exist (bool): if the submodule does not exist, ignore it or raise an exception
     """
+
     suffix: str
     target_module: Union[ParallelModule, BaseLayerNorm]
     kwargs: Dict[str, Any] = None
     ignore_if_not_exist: bool = False
 
 
 @dataclass
@@ -51,14 +51,15 @@
                         new_weight = shard_rowwise(weight, process_group)
                         module.weight = torch.nn.Parameter(new_weight)
                     ```
         sub_module_replacement (List[SubModuleReplacementDescription]): each element in the list is a SubModuleReplacementDescription
                     object which specifies the module to be replaced and the target module used to replacement.
         method_replace (Dict[str, Callable]): key is the method name, value is the method for replacement
     """
+
     attribute_replacement: Dict[str, Any] = None
     param_replacement: List[Callable] = None
     sub_module_replacement: List[SubModuleReplacementDescription] = None
     method_replacement: Dict[str, Callable] = None
 
 
 class Policy(ABC):
@@ -193,53 +194,15 @@
         """Get parameters that should be shared across stages. This method should be implemented by subclass.
 
         Returns:
             List[Dict[int, Tensor]]: List of parameters that should be shared across stages. E.g. [{0: module.model.embed_tokens.weight, 3: module.lm_head.weight}]
         """
         return []
 
-    @staticmethod
-    def distribute_layers(num_layers: int, num_stages: int) -> List[int]:
-        """Divide layers into stages"""
-        quotient = num_layers // num_stages
-        remainder = num_layers % num_stages
-
-        # calculate the num_layers per stage
-        layers_per_stage = [quotient] * num_stages
-
-        # deal with the rest layers
-        if remainder > 0:
-            start_position = num_stages // 2 - remainder // 2
-            for i in range(start_position, start_position + remainder):
-                layers_per_stage[i] += 1
-        return layers_per_stage
-
-    @staticmethod
-    def get_stage_index(
-        layers_per_stage: List[int],
-        stage: int,
-        num_model_chunks: int = 1,
-        num_stages: int = 0,
-    ) -> Union[Tuple[int, int], List[Tuple[int, int]]]:
-        """
-        Get the start index and end index of layers for each stage.
-
-        Args:
-            layers_per_stage (List[int]): number of layers for each stage
-            stage (int): the stage index
-            num_stages (int): number of stages
-            num_model_chunks (int): number of model chunks
-
-        Returns:
-            - Tuple[int, int]: the start index and end index of this stage
-            - List[Tuple[int, int]]: the start index and end index of this stage for each model chunk
-
-        """
-        num_layers_per_stage_accumulated = np.insert(np.cumsum(layers_per_stage), 0, 0)
-
-        stage_indices = []
-        for model_chunk in range(num_model_chunks):
-            start_idx = num_layers_per_stage_accumulated[stage + model_chunk * num_stages]
-            end_idx = num_layers_per_stage_accumulated[stage + model_chunk * num_stages + 1]
-            stage_indices.append([start_idx, end_idx])
-
-        return stage_indices[0] if num_model_chunks == 1 else stage_indices
+    def tie_weight_check(self):
+        input_embedding = self.model.get_input_embeddings()
+        output_embedding = self.model.get_output_embeddings()
+        return (
+            input_embedding is not None
+            and output_embedding is not None
+            and id(input_embedding.weight) == id(output_embedding.weight)
+        )
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/policies/bert.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/bert.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,20 +1,22 @@
+import warnings
 from functools import partial
 from typing import Callable, Dict, List
 
 import torch.nn as nn
 from torch import Tensor
 from torch.nn import Module
 
 import colossalai.shardformer.layer as col_nn
 
 from ..modeling.bert import (
     BertPipelineForwards,
     bert_sequence_parallel_forward_fn,
     get_bert_flash_attention_forward,
+    get_jit_fused_bert_intermediate_forward,
     get_jit_fused_bert_output_forward,
     get_jit_fused_bert_self_output_forward,
 )
 from ..modeling.jit import get_jit_fused_dropout_add_func
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 __all__ = [
@@ -32,47 +34,58 @@
 
 
 class BertPolicy(Policy):
     def config_sanity_check(self):
         pass
 
     def preprocess(self):
-        # reshape the embedding layer
-        r"""
-        Reshape the Embedding layer to make the embedding dimension divisible by world_size
-        """
-        # TODO:
-        if self.shard_config.enable_tensor_parallelism:
-            vocab_size = self.model.config.vocab_size
-            world_size = self.shard_config.tensor_parallel_size
-            if vocab_size % world_size != 0:
-                new_vocab_size = vocab_size + world_size - vocab_size % world_size
-                self.model.resize_token_embeddings(new_vocab_size)
+        self.tie_weight = self.tie_weight_check()
+        self.enable_bias_gelu_fused = self.shard_config.enable_jit_fused and self.model.config.hidden_act == "gelu"
         return self.model
 
     def module_policy(self):
         from transformers.models.bert.modeling_bert import (
             BertEmbeddings,
+            BertIntermediate,
             BertLayer,
             BertModel,
             BertOutput,
             BertSelfAttention,
             BertSelfOutput,
         )
 
         policy = {}
 
+        embedding_cls = None
+        if self.shard_config.enable_tensor_parallelism:
+            embedding_cls = col_nn.VocabParallelEmbedding1D
+        else:
+            if self.tie_weight:
+                embedding_cls = col_nn.PaddingEmbedding
+
         if self.shard_config.enable_fused_normalization:
             norm_cls = col_nn.FusedLayerNorm
         else:
             norm_cls = col_nn.LayerNorm
 
-        use_sequence_parallel = self.shard_config.enable_sequence_parallelism
+        sp_mode = self.shard_config.sequence_parallelism_mode if self.shard_config.enable_sequence_parallelism else None
+        assert sp_mode != "all_to_all", "all_to_all sequence parallelism is not supported for Bert"
+        if sp_mode == "ring":
+            warnings.warn(
+                f"For Bert, sequence parallelism is currently not support mode {sp_mode}, will set to be split_gather"
+            )
+            sp_mode = "split_gather"
+
         overlap = self.shard_config.enable_sequence_overlap
+        sp_partial_derived = sp_mode == "split_gather"
+
         if self.shard_config.enable_tensor_parallelism:
+            assert (
+                self.model.config.num_attention_heads % self.shard_config.tensor_parallel_size == 0
+            ), f"The number of attention heads must be divisible by tensor parallel size."
             policy[BertLayer] = ModulePolicyDescription(
                 attribute_replacement={
                     "attention.self.all_head_size": self.model.config.hidden_size
                     // self.shard_config.tensor_parallel_size,
                     "crossattention.self.all_head_size": self.model.config.hidden_size
                     // self.shard_config.tensor_parallel_size,
                     "attention.self.num_attention_heads": self.model.config.num_attention_heads
@@ -80,89 +93,118 @@
                     "crossattention.self.num_attention_heads": self.model.config.num_attention_heads
                     // self.shard_config.tensor_parallel_size,
                 },
                 sub_module_replacement=[
                     SubModuleReplacementDescription(
                         suffix="attention.self.query",
                         target_module=col_nn.Linear1D_Col,
-                        kwargs={"seq_parallel": use_sequence_parallel, "overlap": overlap},
+                        kwargs={
+                            "seq_parallel_mode": sp_mode,
+                            "overlap": overlap,
+                        },
                     ),
                     SubModuleReplacementDescription(
                         suffix="attention.self.key",
                         target_module=col_nn.Linear1D_Col,
-                        kwargs={"seq_parallel": use_sequence_parallel, "overlap": overlap},
+                        kwargs={
+                            "seq_parallel_mode": sp_mode,
+                            "overlap": overlap,
+                        },
                     ),
                     SubModuleReplacementDescription(
                         suffix="attention.self.value",
                         target_module=col_nn.Linear1D_Col,
-                        kwargs={"seq_parallel": use_sequence_parallel, "overlap": overlap},
+                        kwargs={
+                            "seq_parallel_mode": sp_mode,
+                            "overlap": overlap,
+                        },
                     ),
                     SubModuleReplacementDescription(
                         suffix="attention.self.dropout",
                         target_module=col_nn.DropoutForParallelInput,
                     ),
                     SubModuleReplacementDescription(
                         suffix="attention.output.dense",
                         target_module=col_nn.Linear1D_Row,
-                        kwargs={"seq_parallel": use_sequence_parallel},
+                        kwargs={"seq_parallel_mode": sp_mode},
                     ),
                     SubModuleReplacementDescription(
                         suffix="attention.output.dropout",
                         target_module=col_nn.DropoutForParallelInput,
                     ),
                     SubModuleReplacementDescription(
                         suffix="intermediate.dense",
                         target_module=col_nn.Linear1D_Col,
-                        kwargs={"seq_parallel": use_sequence_parallel, "overlap": overlap},
+                        kwargs={
+                            "seq_parallel_mode": sp_mode,
+                            "overlap": overlap,
+                            "skip_bias_add": self.enable_bias_gelu_fused,
+                        },
                     ),
                     SubModuleReplacementDescription(
                         suffix="output.dense",
                         target_module=col_nn.Linear1D_Row,
-                        kwargs={"seq_parallel": use_sequence_parallel},
+                        kwargs={"seq_parallel_mode": sp_mode},
                     ),
                     SubModuleReplacementDescription(
                         suffix="output.dropout",
                         target_module=col_nn.DropoutForParallelInput,
                     ),
                 ],
             )
 
             policy[BertEmbeddings] = ModulePolicyDescription(
                 sub_module_replacement=[
                     SubModuleReplacementDescription(
-                        suffix="word_embeddings",
-                        target_module=col_nn.VocabParallelEmbedding1D,
-                    ),
-                    SubModuleReplacementDescription(
                         suffix="dropout",
                         target_module=col_nn.DropoutForReplicatedInput,
                     ),
                 ]
             )
+            if self.enable_bias_gelu_fused:
+                self.append_or_create_method_replacement(
+                    description={
+                        "forward": get_jit_fused_bert_intermediate_forward(),
+                    },
+                    policy=policy,
+                    target_key=BertIntermediate,
+                )
 
-        if use_sequence_parallel:
+        if sp_mode == "split_gather":
             self.append_or_create_method_replacement(
                 description={"forward": bert_sequence_parallel_forward_fn(self.shard_config)},
                 policy=policy,
                 target_key=BertModel,
             )
 
+        if embedding_cls is not None:
+            self.append_or_create_submodule_replacement(
+                description=[
+                    SubModuleReplacementDescription(
+                        suffix="word_embeddings",
+                        target_module=embedding_cls,
+                    )
+                ],
+                policy=policy,
+                target_key=BertEmbeddings,
+            )
+
         # optimization configuration
         # Handle bert layer
         self.append_or_create_submodule_replacement(
             description=[
                 SubModuleReplacementDescription(
                     suffix="attention.output.LayerNorm",
                     target_module=norm_cls,
-                    kwargs={"sp_partial_derived": use_sequence_parallel},
+                    kwargs={"sp_partial_derived": sp_partial_derived},
                 ),
                 SubModuleReplacementDescription(
                     suffix="output.LayerNorm",
                     target_module=norm_cls,
-                    kwargs={"sp_partial_derived": use_sequence_parallel},
+                    kwargs={"sp_partial_derived": sp_partial_derived},
                 ),
             ],
             policy=policy,
             target_key=BertLayer,
         )
         # handle embedding layer
         self.append_or_create_submodule_replacement(
@@ -210,15 +252,30 @@
     def add_lm_head_policy(self, base_policy):
         from transformers.models.bert.modeling_bert import BertLMPredictionHead
 
         # optimize for tensor parallelism
         if self.shard_config.enable_tensor_parallelism:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
-                    suffix="decoder", target_module=col_nn.Linear1D_Col, kwargs={"gather_output": True}
+                    suffix="decoder",
+                    target_module=col_nn.VocabParallelLMHead1D,
+                    kwargs={
+                        "gather_output": True,
+                        "make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by,
+                    },
+                ),
+                policy=base_policy,
+                target_key=BertLMPredictionHead,
+            )
+        else:
+            self.append_or_create_submodule_replacement(
+                description=SubModuleReplacementDescription(
+                    suffix="decoder",
+                    target_module=col_nn.PaddingLMHead,
+                    kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
                 ),
                 policy=base_policy,
                 target_key=BertLMPredictionHead,
             )
 
         # optimize with fused normalization
         if self.shard_config.enable_fused_normalization:
@@ -237,15 +294,17 @@
         from transformers.models.bert.modeling_bert import BertLMPredictionHead
 
         method_replacement = {
             "_save_to_state_dict": col_nn.ParallelModule._save_to_state_dict,
             "_load_from_state_dict": col_nn.ParallelModule._load_from_state_dict,
         }
         self.append_or_create_method_replacement(
-            description=method_replacement, policy=base_policy, target_key=BertLMPredictionHead
+            description=method_replacement,
+            policy=base_policy,
+            target_key=BertLMPredictionHead,
         )
         return base_policy
 
     def postprocess(self):
         return self.model
 
     def set_pipeline_forward(self, model_cls: nn.Module, new_forward: Callable, policy: Dict) -> None:
@@ -259,33 +318,33 @@
         stage_manager = self.pipeline_stage_manager
         if self.model.__class__.__name__ == "BertModel":
             module = self.model
         else:
             module = self.model.bert
 
         if stage_manager.is_interleave:
-            layers_per_stage = self.distribute_layers(
-                len(module.encoder.layer), stage_manager.num_stages * stage_manager.num_model_chunks
-            )
-            stage_manager.stage_indices = Policy.get_stage_index(
-                layers_per_stage,
-                stage_manager.stage,
-                num_model_chunks=stage_manager.num_model_chunks,
-                num_stages=stage_manager.num_stages,
-            )
+            layers_per_stage = stage_manager.distribute_layers(len(module.encoder.layer))
+            stage_manager.stage_indices = stage_manager.get_stage_index(layers_per_stage)
             method_replacement = {
-                "forward": partial(new_forward, stage_manager=stage_manager, shard_config=self.shard_config)
+                "forward": partial(
+                    new_forward,
+                    stage_manager=stage_manager,
+                    shard_config=self.shard_config,
+                )
             }
 
         else:
-            layers_per_stage = Policy.distribute_layers(len(module.encoder.layer), stage_manager.num_stages)
-            stage_index = Policy.get_stage_index(layers_per_stage, stage_manager.stage)
+            layers_per_stage = stage_manager.distribute_layers(len(module.encoder.layer))
+            stage_index = stage_manager.get_stage_index(layers_per_stage)
             method_replacement = {
                 "forward": partial(
-                    new_forward, stage_manager=stage_manager, stage_index=stage_index, shard_config=self.shard_config
+                    new_forward,
+                    stage_manager=stage_manager,
+                    stage_index=stage_index,
+                    shard_config=self.shard_config,
                 )
             }
 
         self.append_or_create_method_replacement(description=method_replacement, policy=policy, target_key=model_cls)
 
     def get_held_layers(self) -> List[Module]:
         """Get pipeline layers for current stage."""
@@ -296,35 +355,28 @@
         else:
             module = self.model.bert
         stage_manager = self.pipeline_stage_manager
 
         held_layers = []
         if stage_manager.is_interleave:
             assert stage_manager.num_model_chunks is not None
-            layers_per_stage = self.distribute_layers(
-                len(module.encoder.layer), stage_manager.num_stages * stage_manager.num_model_chunks
-            )
-            stage_indices = Policy.get_stage_index(
-                layers_per_stage,
-                stage_manager.stage,
-                num_model_chunks=stage_manager.num_model_chunks,
-                num_stages=stage_manager.num_stages,
-            )
+            layers_per_stage = stage_manager.distribute_layers(len(module.encoder.layer))
+            stage_indices = stage_manager.get_stage_index(layers_per_stage)
             if stage_manager.is_first_stage(ignore_chunk=True):
                 held_layers.append(module.embeddings)
             for start_idx, end_idx in stage_indices:
                 held_layers.extend(module.encoder.layer[start_idx:end_idx])
             if stage_manager.is_last_stage(ignore_chunk=True):
                 held_layers.append(module.pooler)
 
         else:
-            layers_per_stage = self.distribute_layers(len(module.encoder.layer), stage_manager.num_stages)
+            layers_per_stage = stage_manager.distribute_layers(len(module.encoder.layer))
             if stage_manager.is_first_stage():
                 held_layers.append(module.embeddings)
-            start_idx, end_idx = Policy.get_stage_index(layers_per_stage, stage_manager.stage)
+            start_idx, end_idx = stage_manager.get_stage_index(layers_per_stage)
             held_layers.extend(module.encoder.layer[start_idx:end_idx])
             if stage_manager.is_last_stage():
                 held_layers.append(module.pooler)
 
         return held_layers
 
 
@@ -332,15 +384,17 @@
 class BertModelPolicy(BertPolicy):
     def module_policy(self):
         policy = super().module_policy()
         from transformers.models.bert.modeling_bert import BertModel
 
         if self.pipeline_stage_manager:
             self.set_pipeline_forward(
-                model_cls=BertModel, new_forward=BertPipelineForwards.bert_model_forward, policy=policy
+                model_cls=BertModel,
+                new_forward=BertPipelineForwards.bert_model_forward,
+                policy=policy,
             )
         return policy
 
     def get_held_layers(self) -> List[Module]:
         """Get pipeline layers for current stage."""
         held_layers = super().get_held_layers()
         return held_layers
@@ -395,15 +449,17 @@
         policy = super().module_policy()
         policy = self.add_lm_head_policy(policy)
         policy = self.add_lm_prediction_policy(policy)
         from transformers.models.bert.modeling_bert import BertLMHeadModel
 
         if self.pipeline_stage_manager:
             self.set_pipeline_forward(
-                model_cls=BertLMHeadModel, new_forward=BertPipelineForwards.bert_lm_head_model_forward, policy=policy
+                model_cls=BertLMHeadModel,
+                new_forward=BertPipelineForwards.bert_lm_head_model_forward,
+                policy=policy,
             )
         return policy
 
     def get_held_layers(self) -> List[Module]:
         """
         get pipeline layers for current stage
         """
@@ -433,15 +489,17 @@
         policy = super().module_policy()
         policy = self.add_lm_head_policy(policy)
         policy = self.add_lm_prediction_policy(policy)
         from transformers.models.bert.modeling_bert import BertForMaskedLM
 
         if self.pipeline_stage_manager:
             self.set_pipeline_forward(
-                model_cls=BertForMaskedLM, new_forward=BertPipelineForwards.bert_for_masked_lm_forward, policy=policy
+                model_cls=BertForMaskedLM,
+                new_forward=BertPipelineForwards.bert_for_masked_lm_forward,
+                policy=policy,
             )
         return policy
 
     def get_held_layers(self) -> List[Module]:
         """
         get pipeline layers for current stage
         """
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/policies/blip2.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/blip2.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,58 +1,64 @@
 import colossalai.shardformer.layer as col_nn
 
 from ..modeling.blip2 import (
     forward_fn,
     get_blip2_flash_attention_forward,
+    get_jit_fused_blip2_mlp_forward,
     get_jit_fused_blip2_QFormer_output_forward,
     get_jit_fused_blip2_QFormer_self_output_forward,
 )
 from ..modeling.jit import get_jit_fused_dropout_add_func
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 __all__ = ["BlipPolicy", "BlipModelPolicy"]
 
 
 class BlipPolicy(Policy):
     def config_sanity_check(self):
         pass
 
     def preprocess(self):
-        # reshape the embedding layer
-        r"""
-        Reshape the Embedding layer to make the embedding dimension divisible by world_size
-        """
-        # TODO:
-        vocab_size = self.model.config.qformer_config.vocab_size
-        world_size = self.shard_config.tensor_parallel_size
-        if vocab_size % world_size != 0:
-            new_vocab_size = vocab_size + world_size - vocab_size % world_size
-            self.model.resize_token_embeddings(new_vocab_size)
+        self.tie_weight = self.tie_weight_check()
+        self.enable_bias_gelu_fused = (
+            self.shard_config.enable_jit_fused and self.model.config.vision_config.hidden_act == "gelu"
+        )
         return self.model
 
     def module_policy(self):
         from transformers.models.blip_2.modeling_blip_2 import (
             Blip2Attention,
             Blip2EncoderLayer,
+            Blip2MLP,
             Blip2QFormerLayer,
             Blip2QFormerModel,
             Blip2QFormerOutput,
             Blip2QFormerSelfOutput,
             Blip2VisionModel,
         )
         from transformers.models.opt.modeling_opt import OPTDecoderLayer, OPTForCausalLM
 
         policy = {}
 
+        embedding_cls = None
+        if self.shard_config.enable_tensor_parallelism:
+            embedding_cls = col_nn.VocabParallelEmbedding1D
+        else:
+            if self.tie_weight:
+                embedding_cls = col_nn.PaddingEmbedding
+
         if self.shard_config.enable_fused_normalization:
             norm_cls = col_nn.FusedLayerNorm
         else:
             norm_cls = col_nn.LayerNorm
 
         if self.shard_config.enable_tensor_parallelism:
+            assert (
+                self.model.config.vision_config.num_attention_heads % self.shard_config.tensor_parallel_size == 0
+            ), f"The number of attention heads must be divisible by tensor parallel size."
             policy[Blip2EncoderLayer] = ModulePolicyDescription(
                 attribute_replacement={
                     "self_attn.num_heads": self.model.config.vision_config.num_attention_heads
                     // self.shard_config.tensor_parallel_size,
                     "self_attn.embed_dim": self.model.config.vision_config.hidden_size
                     // self.shard_config.tensor_parallel_size,
                 },
@@ -71,14 +77,15 @@
                     SubModuleReplacementDescription(
                         suffix="self_attn.projection",
                         target_module=col_nn.Linear1D_Row,
                     ),
                     SubModuleReplacementDescription(
                         suffix="mlp.fc1",
                         target_module=col_nn.Linear1D_Col,
+                        kwargs={"skip_bias_add": self.enable_bias_gelu_fused},
                     ),
                     SubModuleReplacementDescription(
                         suffix="mlp.fc2",
                         target_module=col_nn.Linear1D_Row,
                     ),
                 ],
             )
@@ -198,30 +205,64 @@
                     SubModuleReplacementDescription(
                         suffix="fc2",
                         target_module=col_nn.Linear1D_Row,
                     ),
                 ],
             )
 
-            policy[OPTForCausalLM] = ModulePolicyDescription(
-                sub_module_replacement=[
+            policy[Blip2Attention] = ModulePolicyDescription(method_replacement={"forward": forward_fn()})
+            if self.enable_bias_gelu_fused:
+                self.append_or_create_method_replacement(
+                    description={
+                        "forward": get_jit_fused_blip2_mlp_forward(),
+                    },
+                    policy=policy,
+                    target_key=Blip2MLP,
+                )
+
+        if embedding_cls is not None:
+            self.append_or_create_submodule_replacement(
+                description=[
                     SubModuleReplacementDescription(
                         suffix="model.decoder.embed_tokens",
-                        target_module=col_nn.VocabParallelEmbedding1D,
+                        target_module=embedding_cls,
+                        kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
                     ),
+                ],
+                policy=policy,
+                target_key=OPTForCausalLM,
+            )
+
+        if self.shard_config.enable_tensor_parallelism:
+            self.append_or_create_submodule_replacement(
+                description=[
                     SubModuleReplacementDescription(
                         suffix="lm_head",
-                        target_module=col_nn.Linear1D_Col,
-                        kwargs={"gather_output": True},
+                        target_module=col_nn.VocabParallelLMHead1D,
+                        kwargs={
+                            "gather_output": True,
+                            "make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by,
+                        },
                     ),
-                ]
+                ],
+                policy=policy,
+                target_key=OPTForCausalLM,
+            )
+        else:
+            self.append_or_create_submodule_replacement(
+                description=[
+                    SubModuleReplacementDescription(
+                        suffix="lm_head",
+                        target_module=col_nn.PaddingLMHead,
+                        kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
+                    ),
+                ],
+                policy=policy,
+                target_key=OPTForCausalLM,
             )
-
-            policy[Blip2Attention] = ModulePolicyDescription(method_replacement={"forward": forward_fn()})
-
         # optimization configuration
         # Handle Blip2EncoderLayer layer
         self.append_or_create_submodule_replacement(
             description=[
                 SubModuleReplacementDescription(
                     suffix="layer_norm1",
                     target_module=norm_cls,
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/policies/bloom.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/bloom.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+import warnings
 from functools import partial
 from typing import Callable, Dict, List
 
 import torch.nn as nn
 from torch import Tensor
 from torch.nn import Module
 
@@ -19,98 +20,110 @@
 from ..modeling.jit import get_dropout_add_func, get_jit_fused_dropout_add_func, get_jit_fused_gelu_forward_func
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 
 class BloomPolicy(Policy):
     def __init__(self) -> None:
         super().__init__()
-        import transformers
-        from packaging.version import Version
-
-        assert Version(transformers.__version__) <= Version(
-            "4.33.0"
-        ), "The Bloom model should run on a transformers version not greater than 4.33.0."
 
     def config_sanity_check(self):
         pass
 
     def preprocess(self):
-        # reshape the embedding layer
-        r"""
-        Reshape the Embedding layer to make the embedding dimension divisible by world_size
-        """
-        if self.shard_config.enable_tensor_parallelism:
-            vocab_size = self.model.config.vocab_size
-            world_size = self.shard_config.tensor_parallel_size
-            if vocab_size % world_size != 0:
-                new_vocab_size = vocab_size + world_size - vocab_size % world_size
-                self.model.resize_token_embeddings(new_vocab_size)
+        self.tie_weight = self.tie_weight_check()
         return self.model
 
     def module_policy(self):
         from transformers.models.bloom.modeling_bloom import BloomAttention, BloomBlock, BloomGelu, BloomMLP, BloomModel
 
         policy = {}
 
+        embedding_cls = None
+        if self.shard_config.enable_tensor_parallelism:
+            embedding_cls = col_nn.VocabParallelEmbedding1D
+        else:
+            if self.tie_weight:
+                embedding_cls = col_nn.PaddingEmbedding
+
         if self.shard_config.enable_fused_normalization:
             norm_cls = col_nn.FusedLayerNorm
         else:
             norm_cls = col_nn.LayerNorm
-        use_sequence_parallel = self.shard_config.enable_sequence_parallelism
+
+        sp_mode = self.shard_config.sequence_parallelism_mode if self.shard_config.enable_sequence_parallelism else None
+        assert sp_mode != "all_to_all", "all_to_all sequence parallelism is not supported for BLOOM"
+        if sp_mode == "ring":
+            warnings.warn(
+                f"For BLOOM, sequence parallelism is currently not support mode {sp_mode}, will set to be split_gather"
+            )
+            sp_mode = "split_gather"
+
         overlap = self.shard_config.enable_sequence_overlap
+        sp_partial_derived = sp_mode == "split_gather"
+
         if self.shard_config.enable_tensor_parallelism:
+            assert (
+                self.model.config.n_head % self.shard_config.tensor_parallel_size == 0
+            ), f"The number of attention heads must be divisible by tensor parallel size."
             policy[BloomBlock] = ModulePolicyDescription(
                 attribute_replacement={
                     "self_attention.hidden_size": self.model.config.hidden_size
                     // self.shard_config.tensor_parallel_size,
                     "self_attention.split_size": self.model.config.hidden_size
                     // self.shard_config.tensor_parallel_size,
                     "self_attention.num_heads": self.model.config.n_head // self.shard_config.tensor_parallel_size,
                 },
                 sub_module_replacement=[
                     SubModuleReplacementDescription(
                         suffix="self_attention.query_key_value",
                         target_module=col_nn.Linear1D_Col,
-                        kwargs={"seq_parallel": use_sequence_parallel, "overlap": overlap},
+                        kwargs={"seq_parallel_mode": sp_mode, "overlap": overlap},
                     ),
                     SubModuleReplacementDescription(
                         suffix="self_attention.dense",
                         target_module=col_nn.Linear1D_Row,
-                        kwargs={"seq_parallel": use_sequence_parallel},
+                        kwargs={"seq_parallel_mode": sp_mode},
                     ),
                     SubModuleReplacementDescription(
                         suffix="self_attention.attention_dropout",
                         target_module=col_nn.DropoutForParallelInput,
                     ),
                     SubModuleReplacementDescription(
                         suffix="mlp.dense_h_to_4h",
                         target_module=col_nn.Linear1D_Col,
-                        kwargs={"seq_parallel": use_sequence_parallel, "overlap": overlap},
+                        kwargs={"seq_parallel_mode": sp_mode, "overlap": overlap},
                     ),
                     SubModuleReplacementDescription(
                         suffix="mlp.dense_4h_to_h",
                         target_module=col_nn.Linear1D_Row,
-                        kwargs={"seq_parallel": use_sequence_parallel},
+                        kwargs={"seq_parallel_mode": sp_mode},
                     ),
                 ],
             )
 
             policy[BloomModel] = ModulePolicyDescription(
                 attribute_replacement={
                     "num_heads": self.model.config.n_head // self.shard_config.tensor_parallel_size,
                 },
                 method_replacement={
                     "build_alibi_tensor": build_bloom_alibi_tensor_fn(self.shard_config.tensor_parallel_process_group)
                 },
-                sub_module_replacement=[
+            )
+
+        if embedding_cls is not None:
+            self.append_or_create_submodule_replacement(
+                description=[
                     SubModuleReplacementDescription(
                         suffix="word_embeddings",
-                        target_module=col_nn.VocabParallelEmbedding1D,
-                    )
+                        target_module=embedding_cls,
+                        kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
+                    ),
                 ],
+                policy=policy,
+                target_key=BloomModel,
             )
 
         # optimization configuration
         # handle bloom model
         self.append_or_create_submodule_replacement(
             description=[
                 SubModuleReplacementDescription(
@@ -128,27 +141,27 @@
 
         # handle bloom block
         self.append_or_create_submodule_replacement(
             description=[
                 SubModuleReplacementDescription(
                     suffix="input_layernorm",
                     target_module=norm_cls,
-                    kwargs={"sp_partial_derived": use_sequence_parallel},
+                    kwargs={"sp_partial_derived": sp_partial_derived},
                 ),
                 SubModuleReplacementDescription(
                     suffix="post_attention_layernorm",
                     target_module=norm_cls,
-                    kwargs={"sp_partial_derived": use_sequence_parallel},
+                    kwargs={"sp_partial_derived": sp_partial_derived},
                 ),
             ],
             policy=policy,
             target_key=BloomBlock,
         )
 
-        if use_sequence_parallel:
+        if sp_mode == "split_gather":
             self.append_or_create_method_replacement(
                 description={"forward": get_bloom_sequence_parallel_forward_fn(self.shard_config)},
                 policy=policy,
                 target_key=BloomModel,
             )
 
         if self.shard_config.enable_flash_attention:
@@ -199,16 +212,16 @@
         if self.pipeline_stage_manager:
             stage_manager = self.pipeline_stage_manager
             if self.model.__class__.__name__ == "BloomModel":
                 module = self.model
             else:
                 module = self.model.transformer
 
-            layers_per_stage = Policy.distribute_layers(len(module.h), stage_manager.num_stages)
-            stage_index = Policy.get_stage_index(layers_per_stage, stage_manager.stage)
+            layers_per_stage = stage_manager.distribute_layers(len(module.h))
+            stage_index = stage_manager.get_stage_index(layers_per_stage)
             method_replacement = {
                 "forward": partial(
                     new_forward, stage_manager=stage_manager, stage_index=stage_index, shard_config=self.shard_config
                 )
             }
             self.append_or_create_method_replacement(
                 description=method_replacement, policy=policy, target_key=model_cls
@@ -222,19 +235,19 @@
         if self.model.__class__.__name__ == "BloomModel":
             module = self.model
         else:
             module = self.model.transformer
         stage_manager = self.pipeline_stage_manager
 
         held_layers = []
-        layers_per_stage = self.distribute_layers(len(module.h), stage_manager.num_stages)
+        layers_per_stage = stage_manager.distribute_layers(len(module.h))
         if stage_manager.is_first_stage():
             held_layers.append(module.word_embeddings)
             held_layers.append(module.word_embeddings_layernorm)
-        start_idx, end_idx = self.get_stage_index(layers_per_stage, stage_manager.stage)
+        start_idx, end_idx = stage_manager.get_stage_index(layers_per_stage)
         held_layers.extend(module.h[start_idx:end_idx])
         if stage_manager.is_last_stage():
             held_layers.append(module.ln_f)
 
         return held_layers
 
 
@@ -267,15 +280,29 @@
 
         policy = super().module_policy()
 
         # handle tensor parallelism
         if self.shard_config.enable_tensor_parallelism:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
-                    suffix="lm_head", target_module=col_nn.Linear1D_Col, kwargs=dict(gather_output=True)
+                    suffix="lm_head",
+                    target_module=col_nn.VocabParallelLMHead1D,
+                    kwargs=dict(
+                        gather_output=True, make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by
+                    ),
+                ),
+                policy=policy,
+                target_key=BloomForCausalLM,
+            )
+        else:
+            self.append_or_create_submodule_replacement(
+                description=SubModuleReplacementDescription(
+                    suffix="lm_head",
+                    target_module=col_nn.PaddingLMHead,
+                    kwargs=dict(make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by),
                 ),
                 policy=policy,
                 target_key=BloomForCausalLM,
             )
         if self.pipeline_stage_manager:
             self.set_pipeline_forward(
                 model_cls=BloomForCausalLM, new_forward=BloomPipelineForwards.bloom_for_causal_lm_forward, policy=policy
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/policies/chatglm2.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/chatglm2.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,170 +1,204 @@
+import warnings
 from functools import partial
 from typing import Callable, Dict, List, Union
 
 import torch.nn as nn
 from torch import Tensor
 
 import colossalai.shardformer.layer as col_nn
 from colossalai.shardformer.modeling.chatglm2 import ChatGLMPipelineForwards
-from colossalai.shardformer.modeling.chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration, ChatGLMModel
 
 from ..modeling.chatglm2 import (
     get_chatglm_sequence_parallel_forward_fn,
     get_flash_core_attention_forward,
     get_jit_fused_glm_block_forward,
 )
 from ..modeling.jit import get_jit_fused_dropout_add_func
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
-__all__ = ["ChatGLMPolicy", "ChatGLMModelPolicy", "ChatGLMForConditionalGenerationPolicy"]
+__all__ = [
+    "ChatGLMPolicy",
+    "ChatGLMModelPolicy",
+    "ChatGLMForConditionalGenerationPolicy",
+]
 
 
 class ChatGLMPolicy(Policy):
     def config_sanity_check(self):
         pass
 
     def preprocess(self):
-        # Resize embedding
-        if self.shard_config.enable_tensor_parallelism:
-            vocab_size = self.model.config.padded_vocab_size
-            world_size = self.shard_config.tensor_parallel_size
-
-            if vocab_size % world_size != 0:
-                new_vocab_size = vocab_size + world_size - vocab_size % world_size
-                self.model.resize_token_embeddings(new_vocab_size)
-
         if self.pipeline_stage_manager is not None:
             # the batch_size_dim is bounded to Model
             bsz_dim = 1
             setattr(self.model, "batch_size_dim", bsz_dim)
 
+        self.tie_weight = self.tie_weight_check()
         return self.model
 
     def module_policy(self) -> Dict[Union[str, nn.Module], ModulePolicyDescription]:
-        from colossalai.shardformer.modeling.chatglm2_6b.modeling_chatglm import ChatGLMModel, CoreAttention, GLMBlock
-
         policy = {}
 
+        embedding_cls = None
+        if self.shard_config.enable_tensor_parallelism:
+            embedding_cls = col_nn.VocabParallelEmbedding1D
+        else:
+            if self.tie_weight:
+                embedding_cls = col_nn.PaddingEmbedding
+
         if self.shard_config.enable_fused_normalization:
             if self.model.config.rmsnorm:
                 norm_cls = col_nn.FusedRMSNorm
             else:
                 norm_cls = col_nn.FusedLayerNorm
         else:
             if self.model.config.rmsnorm:
                 norm_cls = col_nn.RMSNorm
             else:
                 norm_cls = col_nn.LayerNorm
-        use_sequence_parallel = self.shard_config.enable_sequence_parallelism
-        overlap = self.shard_config.enable_sequence_overlap
-        if self.shard_config.enable_tensor_parallelism:
-            policy[ChatGLMModel] = ModulePolicyDescription(
-                attribute_replacement={},
-                sub_module_replacement=[
-                    SubModuleReplacementDescription(
-                        suffix="embedding.word_embeddings",
-                        target_module=col_nn.VocabParallelEmbedding1D,
-                    )
-                ],
+
+        sp_mode = self.shard_config.sequence_parallelism_mode if self.shard_config.enable_sequence_parallelism else None
+        assert sp_mode != "all_to_all", "all_to_all sequence parallelism is not supported for ChatGLM2"
+        if sp_mode == "ring":
+            warnings.warn(
+                f"For ChatGLM2, sequence parallelism is currently not support mode {sp_mode}, will set to be split_gather"
             )
+            sp_mode = "split_gather"
+        overlap = self.shard_config.enable_sequence_overlap
+        sp_partial_derived = sp_mode == "split_gather"
 
-            policy[GLMBlock] = ModulePolicyDescription(
+        if self.shard_config.enable_tensor_parallelism:
+            assert (
+                self.model.config.num_attention_heads % self.shard_config.tensor_parallel_size == 0
+            ), f"num_attention_heads {self.model.config.num_attention_heads} should be divisible by tensor_parallel_size {self.shard_config.tensor_parallel_size}"
+            attn_kwargs = {
+                "self_attention.qkv_hidden_size": (
+                    self.model.config.kv_channels * self.model.config.num_attention_heads * 3
+                )
+                // self.shard_config.tensor_parallel_size,
+            }
+            if self.model.config.multi_query_attention:
+                assert (
+                    self.model.config.multi_query_group_num % self.shard_config.tensor_parallel_size == 0
+                ), f"multi_query_group_num {self.model.config.multi_query_group_num} should be divisible by tensor_parallel_size {self.shard_config.tensor_parallel_size}"
+                attn_kwargs["self_attention.num_multi_query_groups_per_partition"] = (
+                    self.model.config.multi_query_group_num // self.shard_config.tensor_parallel_size
+                )
+                attn_kwargs["self_attention.qkv_hidden_size"] = (
+                    self.model.config.kv_channels * self.model.config.num_attention_heads
+                    + 2 * self.model.config.kv_channels * self.model.config.multi_query_group_num
+                ) // self.shard_config.tensor_parallel_size
+            policy["GLMBlock"] = ModulePolicyDescription(
                 attribute_replacement={
                     "self_attention.num_attention_heads_per_partition": self.model.config.num_attention_heads
                     // self.shard_config.tensor_parallel_size,
                     "self_attention.projection_size": (
                         self.model.config.kv_channels * self.model.config.num_attention_heads
                     )
                     // self.shard_config.tensor_parallel_size,
-                    "self_attention.qkv_hidden_size": (
-                        self.model.config.kv_channels * self.model.config.num_attention_heads * 3
-                    )
-                    // self.shard_config.tensor_parallel_size,
                     "self_attention.core_attention.num_attention_heads_per_partition": self.model.config.num_attention_heads
                     // self.shard_config.tensor_parallel_size,
                     "self_attention.core_attention.hidden_size_per_partition": self.model.config.kv_channels
                     * self.model.config.num_attention_heads
                     // self.shard_config.tensor_parallel_size,
+                    **attn_kwargs,
                 },
                 param_replacement=[],
                 sub_module_replacement=[
                     SubModuleReplacementDescription(
                         suffix="self_attention.query_key_value",
                         target_module=col_nn.Linear1D_Col,
-                        kwargs={"seq_parallel": use_sequence_parallel, "seq_parallel_dim": 0, "overlap": overlap},
+                        kwargs={
+                            "seq_parallel_mode": sp_mode,
+                            "seq_parallel_dim": 0,
+                            "overlap": overlap,
+                        },
                     ),
                     SubModuleReplacementDescription(
                         suffix="self_attention.dense",
                         target_module=col_nn.Linear1D_Row,
-                        kwargs={"seq_parallel": use_sequence_parallel, "seq_parallel_dim": 0},
+                        kwargs={"seq_parallel_mode": sp_mode, "seq_parallel_dim": 0},
                     ),
                     SubModuleReplacementDescription(
                         suffix="self_attention.core_attention.attention_dropout",
                         target_module=col_nn.DropoutForParallelInput,
                     ),
                 ],
             )
+
+        if embedding_cls is not None:
+            self.append_or_create_submodule_replacement(
+                description=[
+                    SubModuleReplacementDescription(
+                        suffix="embedding.word_embeddings",
+                        target_module=embedding_cls,
+                        kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
+                    ),
+                ],
+                policy=policy,
+                target_key="ChatGLMModel",
+            )
         # optimization configuration
         self.append_or_create_submodule_replacement(
             description=[
                 SubModuleReplacementDescription(
                     suffix="input_layernorm",
                     target_module=norm_cls,
-                    kwargs={"sp_partial_derived": use_sequence_parallel},
+                    kwargs={"sp_partial_derived": sp_partial_derived},
                 ),
                 SubModuleReplacementDescription(
                     suffix="post_attention_layernorm",
                     target_module=norm_cls,
-                    kwargs={"sp_partial_derived": use_sequence_parallel},
+                    kwargs={"sp_partial_derived": sp_partial_derived},
                 ),
             ],
             policy=policy,
-            target_key=GLMBlock,
+            target_key="GLMBlock",
         )
 
         if self.model.config.post_layer_norm:
             self.append_or_create_submodule_replacement(
                 description=[
                     SubModuleReplacementDescription(
                         suffix="encoder.final_layernorm",
                         target_module=norm_cls,
                     )
                 ],
                 policy=policy,
-                target_key=ChatGLMModel,
+                target_key="ChatGLMModel",
             )
 
         # use flash attention
         if self.shard_config.enable_flash_attention:
             self.append_or_create_method_replacement(
                 description={
                     "forward": get_flash_core_attention_forward(),
                 },
                 policy=policy,
-                target_key=CoreAttention,
+                target_key="CoreAttention",
             )
 
         # use sequence parallel
-        if use_sequence_parallel:
+        if sp_mode == "split_gather":
             self.append_or_create_method_replacement(
                 description={"forward": get_chatglm_sequence_parallel_forward_fn(self.shard_config)},
                 policy=policy,
-                target_key=ChatGLMModel,
+                target_key="ChatGLMModel",
             )
 
         # use jit fused operator
         if self.shard_config.enable_jit_fused:
             self.append_or_create_method_replacement(
                 description={
                     "forward": get_jit_fused_glm_block_forward(),
                     "dropout_add": get_jit_fused_dropout_add_func(),
                 },
                 policy=policy,
-                target_key=GLMBlock,
+                target_key="GLMBlock",
             )
 
         return policy
 
     def postprocess(self):
         return self.model
 
@@ -175,18 +209,18 @@
         if self.model.__class__.__name__ == "ChatGLMModel":
             module = self.model
         else:
             module = self.model.transformer
         stage_manager = self.pipeline_stage_manager
 
         held_layers = []
-        layers_per_stage = self.distribute_layers(module.num_layers, stage_manager.num_stages)
+        layers_per_stage = stage_manager.distribute_layers(module.num_layers)
         if stage_manager.is_first_stage():
             held_layers.append(module.embedding)
-        start_idx, end_idx = self.get_stage_index(layers_per_stage, stage_manager.stage)
+        start_idx, end_idx = stage_manager.get_stage_index(layers_per_stage)
         held_layers.extend(module.encoder.layers[start_idx:end_idx])
         if stage_manager.is_last_stage():
             if module.encoder.post_layer_norm:
                 held_layers.append(module.encoder.final_layernorm)
 
         # rotary_pos_emb is needed for all stages
         held_layers.append(module.rotary_pos_emb)
@@ -200,33 +234,38 @@
             raise ValueError("set_pipeline_forward method can only be called when pipeline parallel is enabled.")
         stage_manager = self.pipeline_stage_manager
         if self.model.__class__.__name__ == "ChatGLMModel":
             module = self.model
         else:
             module = self.model.transformer
 
-        layers_per_stage = Policy.distribute_layers(module.num_layers, stage_manager.num_stages)
-        stage_index = Policy.get_stage_index(layers_per_stage, stage_manager.stage)
+        layers_per_stage = stage_manager.distribute_layers(module.num_layers)
+        stage_index = stage_manager.get_stage_index(layers_per_stage)
         method_replacement = {
             "forward": partial(
-                new_forward, stage_manager=stage_manager, stage_index=stage_index, shard_config=self.shard_config
+                new_forward,
+                stage_manager=stage_manager,
+                stage_index=stage_index,
+                shard_config=self.shard_config,
             )
         }
         self.append_or_create_method_replacement(description=method_replacement, policy=policy, target_key=model_cls)
 
 
 class ChatGLMModelPolicy(ChatGLMPolicy):
     def module_policy(self):
         pass
 
         policy = super().module_policy()
 
         if self.pipeline_stage_manager is not None:
             self.set_pipeline_forward(
-                model_cls=ChatGLMModel, new_forward=ChatGLMPipelineForwards.chatglm_model_forward, policy=policy
+                model_cls="ChatGLMModel",
+                new_forward=ChatGLMPipelineForwards.chatglm_model_forward,
+                policy=policy,
             )
         return policy
 
     def get_held_layers(self) -> List[nn.Module]:
         return super().get_held_layers()
 
     def get_shared_params(self) -> List[Dict[int, Tensor]]:
@@ -236,15 +275,15 @@
 
 class ChatGLMForConditionalGenerationPolicy(ChatGLMModelPolicy):
     def module_policy(self):
         policy = super().module_policy()
 
         if self.pipeline_stage_manager is not None:
             self.set_pipeline_forward(
-                model_cls=ChatGLMForConditionalGeneration,
+                model_cls="ChatGLMForConditionalGeneration",
                 new_forward=ChatGLMPipelineForwards.chatglm_for_conditional_generation_forward,
                 policy=policy,
             )
         return policy
 
     def get_held_layers(self) -> List[nn.Module]:
         held_layers = super().get_held_layers()
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/policies/falcon.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/falcon.py`

 * *Files 4% similar despite different names*

```diff
@@ -3,66 +3,60 @@
 from typing import Callable, Dict, List
 
 from torch import Tensor, nn
 from torch.nn import Module
 
 import colossalai.shardformer.layer as col_nn
 
-from ..modeling.falcon import (
-    FalconPipelineForwards,
-    build_falcon_alibi_tensor_fn,
-    get_falcon_flash_attention_forward,
-    get_tp_falcon_decoder_layer_forward,
-)
+from ..modeling.falcon import FalconPipelineForwards, build_falcon_alibi_tensor_fn, get_tp_falcon_decoder_layer_forward
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 __all__ = ["FalconPolicy"]
 
 
 class FalconPolicy(Policy):
     def __init__(self) -> None:
         super().__init__()
-        import transformers
-        from packaging.version import Version
-
-        assert Version(transformers.__version__) <= Version(
-            "4.33.0"
-        ), "The Falcon model should run on a transformers version not greater than 4.33.0."
 
     def config_sanity_check(self):
         pass
 
     def preprocess(self):
-        # reshape the embedding layer
-        r"""
-        Reshape the Embedding layer to make the embedding dimension divisible by world_size
-        """
-        if self.shard_config.enable_tensor_parallelism:
-            vocab_size = self.model.config.vocab_size
-            world_size = self.shard_config.tensor_parallel_size
-            if vocab_size % world_size != 0:
-                new_vocab_size = vocab_size + world_size - vocab_size % world_size
-                self.model.resize_token_embeddings(new_vocab_size)
+        self.tie_weight = self.tie_weight_check()
         return self.model
 
     def module_policy(self):
-        from transformers.models.falcon.modeling_falcon import FalconAttention, FalconDecoderLayer, FalconModel
+        from transformers.models.falcon.modeling_falcon import FalconDecoderLayer, FalconModel
 
         if not self.model.config.new_decoder_architecture and self.model.config.multi_query:
             warnings.warn(
                 "Falcon doesn't support tensor parallelism when (not new_decoder_architecture and multi_query) is True, will ignore the tensor parallelism flag."
             )
             self.shard_config.enable_tensor_parallelism = False
 
         if self.shard_config.enable_sequence_parallelism:
             self.shard_config.enable_sequence_parallelism = False
             warnings.warn("Falcon doesn't support sequence parallelism now, will ignore the sequence parallelism flag.")
 
         policy = {}
+
+        embedding_cls = None
+        if self.shard_config.enable_tensor_parallelism:
+            embedding_cls = col_nn.VocabParallelEmbedding1D
+        else:
+            if self.tie_weight:
+                embedding_cls = col_nn.PaddingEmbedding
+
         if self.shard_config.enable_tensor_parallelism:
+            assert (
+                self.model.config.num_attention_heads % self.shard_config.tensor_parallel_size == 0
+            ), f"The number of attention heads must be divisible by tensor parallel size."
+            assert (
+                self.model.config.num_kv_heads % self.shard_config.tensor_parallel_size == 0
+            ), f"The number of key_value heads must be divisible by tensor parallel size."
             attn_attribute_replacement = {
                 "self_attention.hidden_size": self.model.config.hidden_size // self.shard_config.tensor_parallel_size,
                 "self_attention.split_size": self.model.config.hidden_size // self.shard_config.tensor_parallel_size,
                 "self_attention.num_heads": self.model.config.num_attention_heads
                 // self.shard_config.tensor_parallel_size,
                 "self_attention.num_kv_heads": self.model.config.num_kv_heads // self.shard_config.tensor_parallel_size,
             }
@@ -94,20 +88,27 @@
             policy[FalconModel] = ModulePolicyDescription(
                 attribute_replacement={
                     "num_heads": self.model.config.num_attention_heads // self.shard_config.tensor_parallel_size,
                 },
                 method_replacement={
                     "build_alibi_tensor": build_falcon_alibi_tensor_fn(self.shard_config.tensor_parallel_process_group)
                 },
-                sub_module_replacement=[
+            )
+
+        if embedding_cls is not None:
+            self.append_or_create_submodule_replacement(
+                description=[
                     SubModuleReplacementDescription(
                         suffix="word_embeddings",
-                        target_module=col_nn.VocabParallelEmbedding1D,
-                    )
+                        target_module=embedding_cls,
+                        kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
+                    ),
                 ],
+                policy=policy,
+                target_key=FalconModel,
             )
 
         # optimization configuration
         if self.shard_config.enable_fused_normalization:
             # handle falcon model
             self.append_or_create_submodule_replacement(
                 description=[
@@ -137,19 +138,16 @@
                     ),
                 ],
                 policy=policy,
                 target_key=FalconDecoderLayer,
             )
 
         if self.shard_config.enable_flash_attention:
-            self.append_or_create_method_replacement(
-                description={"forward": get_falcon_flash_attention_forward()},
-                policy=policy,
-                target_key=FalconAttention,
-            )
+            warnings.warn("Falcon doesn't support flash attention now, fallback to transformers attention.")
+
         return policy
 
     def postprocess(self):
         return self.model
 
     def set_pipeline_forward(self, model_cls: nn.Module, new_forward: Callable, policy: Dict) -> None:
         """If under pipeline parallel setting, replacing the original forward method of huggingface
@@ -157,16 +155,16 @@
         if self.pipeline_stage_manager:
             stage_manager = self.pipeline_stage_manager
             if self.model.__class__.__name__ == "FalconModel":
                 module = self.model
             else:
                 module = self.model.transformer
 
-            layers_per_stage = Policy.distribute_layers(len(module.h), stage_manager.num_stages)
-            stage_index = Policy.get_stage_index(layers_per_stage, stage_manager.stage)
+            layers_per_stage = stage_manager.distribute_layers(len(module.h))
+            stage_index = stage_manager.get_stage_index(layers_per_stage)
             method_replacement = {
                 "forward": partial(
                     new_forward, stage_manager=stage_manager, stage_index=stage_index, shard_config=self.shard_config
                 )
             }
             self.append_or_create_method_replacement(
                 description=method_replacement, policy=policy, target_key=model_cls
@@ -177,18 +175,18 @@
         assert self.pipeline_stage_manager is not None
         if self.model.__class__.__name__ == "FalconModel":
             module = self.model
         else:
             module = self.model.transformer
         stage_manager = self.pipeline_stage_manager
         held_layers = []
-        layers_per_stage = self.distribute_layers(len(module.h), stage_manager.num_stages)
+        layers_per_stage = stage_manager.distribute_layers(len(module.h))
         if stage_manager.is_first_stage():
             held_layers.append(module.word_embeddings)
-        start_idx, end_idx = self.get_stage_index(layers_per_stage, stage_manager.stage)
+        start_idx, end_idx = stage_manager.get_stage_index(layers_per_stage)
         held_layers.extend(module.h[start_idx:end_idx])
         if stage_manager.is_last_stage():
             held_layers.append(module.ln_f)
 
         return held_layers
 
 
@@ -228,19 +226,34 @@
 
         policy = super().module_policy()
 
         # handle tensor parallelism
         if self.shard_config.enable_tensor_parallelism:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
-                    suffix="lm_head", target_module=col_nn.Linear1D_Col, kwargs=dict(gather_output=True)
+                    suffix="lm_head",
+                    target_module=col_nn.VocabParallelLMHead1D,
+                    kwargs=dict(
+                        gather_output=True, make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by
+                    ),
+                ),
+                policy=policy,
+                target_key=FalconForCausalLM,
+            )
+        else:
+            self.append_or_create_submodule_replacement(
+                description=SubModuleReplacementDescription(
+                    suffix="lm_head",
+                    target_module=col_nn.PaddingLMHead,
+                    kwargs=dict(make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by),
                 ),
                 policy=policy,
                 target_key=FalconForCausalLM,
             )
+
         if self.pipeline_stage_manager:
             self.set_pipeline_forward(
                 model_cls=FalconForCausalLM,
                 new_forward=FalconPipelineForwards.falcon_for_causal_lm_forward,
                 policy=policy,
             )
         return policy
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/policies/gpt2.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/llama.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,443 +1,457 @@
+import warnings
 from functools import partial
-from typing import Callable, Dict, List
+from typing import Callable, Dict, List, Union
 
-from torch import Tensor, nn
-
-import colossalai.shardformer.layer as col_nn
+import torch.nn as nn
+from torch import Tensor
+from torch.nn import Module
+
+from colossalai.shardformer.layer import (
+    FusedRMSNorm,
+    Linear1D_Col,
+    Linear1D_Row,
+    PaddingEmbedding,
+    PaddingLMHead,
+    RMSNorm,
+    VocabParallelEmbedding1D,
+    VocabParallelLMHead1D,
+)
 
-from ..modeling.gpt2 import (
-    GPT2PipelineForwards,
-    get_gpt2_flash_attention_forward,
+from ..modeling.llama import (
+    LlamaPipelineForwards,
+    get_llama_flash_attention_forward,
+    get_llama_model_forward_for_flash_attn,
+    get_llama_seq_parallel_attention_forward,
+    get_llama_seq_parallel_model_forward,
     get_lm_forward_with_dist_cross_entropy,
-    gpt2_sequence_parallel_forward_fn,
 )
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
-__all__ = [
-    "GPT2Policy",
-    "GPT2ModelPolicy",
-    "GPT2LMHeadModelPolicy",
-    "GPT2DoubleHeadsModelPolicy",
-    "GPT2ForTokenClassificationPolicy",
-    "GPT2ForSequenceClassificationPolicy",
-]
+__all__ = ["LlamaPolicy", "LlamaForCausalLMPolicy", "LlamaForSequenceClassificationPolicy"]
 
 
-class GPT2Policy(Policy):
+class LlamaPolicy(Policy):
     def config_sanity_check(self):
         pass
 
     def preprocess(self):
-        # reshape the embedding layer
-        r"""
-        Reshape the Embedding layer to make the embedding dimension divisible by world_size
-        """
-        if self.shard_config.enable_tensor_parallelism:
-            vocab_size = self.model.config.vocab_size
-            world_size = self.shard_config.tensor_parallel_size
-            if vocab_size % world_size != 0:
-                new_vocab_size = vocab_size + world_size - vocab_size % world_size
-                self.model.resize_token_embeddings(new_vocab_size)
+        self.tie_weight = self.tie_weight_check()
+        self.origin_attn_implement = self.model.config._attn_implementation
         return self.model
 
-    def module_policy(self):
-        from transformers.models.gpt2.modeling_gpt2 import GPT2Attention, GPT2Block, GPT2Model
+    def module_policy(self) -> Dict[Union[str, nn.Module], ModulePolicyDescription]:
+        from transformers.models.llama.modeling_llama import (
+            LlamaAttention,
+            LlamaDecoderLayer,
+            LlamaFlashAttention2,
+            LlamaModel,
+            LlamaSdpaAttention,
+        )
 
+        ATTN_IMPLEMENTATION = {
+            "eager": LlamaAttention,
+            "flash_attention_2": LlamaFlashAttention2,
+            "sdpa": LlamaSdpaAttention,
+        }
         policy = {}
 
+        attn_cls = ATTN_IMPLEMENTATION[self.origin_attn_implement]
+        embedding_cls = None
+        if self.shard_config.enable_tensor_parallelism:
+            embedding_cls = VocabParallelEmbedding1D
+        else:
+            if self.tie_weight:
+                embedding_cls = PaddingEmbedding
+
         if self.shard_config.enable_fused_normalization:
-            norm_cls = col_nn.FusedLayerNorm
+            norm_cls = FusedRMSNorm
         else:
-            norm_cls = col_nn.LayerNorm
-        use_sequence_parallel = self.shard_config.enable_sequence_parallelism
-        overlap = self.shard_config.enable_sequence_overlap
-        if self.shard_config.enable_tensor_parallelism:
-            policy[GPT2Model] = ModulePolicyDescription(
-                sub_module_replacement=[
-                    SubModuleReplacementDescription(
-                        suffix="wte",
-                        target_module=col_nn.VocabParallelEmbedding1D,
-                    ),
-                    SubModuleReplacementDescription(
-                        suffix="drop",
-                        target_module=col_nn.DropoutForParallelInput,
+            norm_cls = RMSNorm
+
+        if self.pipeline_stage_manager is not None:
+            self.shard_config.enable_sequence_parallelism = False
+            self.shard_config.enable_sequence_overlap = False
+            self.shard_config.sequence_parallelism_mode = None
+            warnings.warn(
+                f"For llama, sequence parallelism is currently not compatible with pipeline parallelism, set to be False"
+            )
+        sp_mode = self.shard_config.sequence_parallelism_mode if self.shard_config.enable_sequence_parallelism else None
+        sp_size = self.shard_config.sequence_parallel_size if self.shard_config.enable_sequence_parallelism else None
+        sp_group = (
+            self.shard_config.sequence_parallel_process_group if self.shard_config.enable_sequence_parallelism else None
+        )
+        sp_partial_derived = sp_mode in ["split_gather", "ring"]
+
+        use_flash_attention = self.shard_config.enable_flash_attention
+        # Currently sp cannot to be used with flashattention
+        if sp_mode in ["split_gather", "ring", "all_to_all"]:
+            if use_flash_attention:
+                warnings.warn(
+                    f"Sequence parallelism mode {sp_mode} need to be used with FlashAttention, will disable FlashAttention automatically."
+                )
+                use_flash_attention = False
+
+        if sp_mode in ["split_gather", "ring"]:
+            self.append_or_create_method_replacement(
+                description={
+                    "forward": get_llama_seq_parallel_model_forward(
+                        sp_mode=sp_mode, sp_size=sp_size, sp_group=sp_group
                     ),
-                ]
+                },
+                policy=policy,
+                target_key=LlamaModel,
             )
+            self.append_or_create_method_replacement(
+                description={
+                    "forward": get_llama_seq_parallel_attention_forward(sp_mode, sp_size, sp_group),
+                },
+                policy=policy,
+                target_key=attn_cls,
+            )
+        elif sp_mode == "all_to_all":
+            decoder_attribute_replacement = {
+                "num_heads": self.model.config.num_attention_heads // sp_size,
+            }
+            if getattr(self.model.config, "num_key_value_heads", False):
+                decoder_attribute_replacement["num_key_value_heads"] = self.model.config.num_key_value_heads // sp_size
 
-            policy[GPT2Block] = ModulePolicyDescription(
-                attribute_replacement={
-                    "attn.embed_dim": self.model.config.hidden_size // self.shard_config.tensor_parallel_size,
-                    "attn.split_size": self.model.config.hidden_size // self.shard_config.tensor_parallel_size,
-                    "attn.num_heads": self.model.config.num_attention_heads // self.shard_config.tensor_parallel_size,
+            policy[attn_cls] = ModulePolicyDescription(
+                attribute_replacement=decoder_attribute_replacement,
+            )
+            self.append_or_create_method_replacement(
+                description={
+                    "forward": get_llama_seq_parallel_attention_forward(sp_mode, sp_size, sp_group),
                 },
+                policy=policy,
+                target_key=attn_cls,
+            )
+            self.append_or_create_method_replacement(
+                description={
+                    "forward": get_llama_seq_parallel_model_forward(
+                        sp_mode=sp_mode,
+                        sp_size=sp_size,
+                        sp_group=sp_group,
+                    ),
+                },
+                policy=policy,
+                target_key=LlamaModel,
+            )
+
+        if self.shard_config.enable_tensor_parallelism:
+            assert (
+                self.model.config.num_attention_heads % self.shard_config.tensor_parallel_size == 0
+            ), f"The number of attention heads must be divisible by tensor parallel size."
+            assert (
+                self.model.config.num_key_value_heads % self.shard_config.tensor_parallel_size == 0
+            ), f"The number of key_value heads must be divisible by tensor parallel size."
+            decoder_attribute_replacement = {
+                "self_attn.hidden_size": self.model.config.hidden_size // self.shard_config.tensor_parallel_size,
+                "self_attn.num_heads": self.model.config.num_attention_heads // self.shard_config.tensor_parallel_size,
+            }
+            if getattr(self.model.config, "num_key_value_heads", False):
+                decoder_attribute_replacement["self_attn.num_key_value_heads"] = (
+                    self.model.config.num_key_value_heads // self.shard_config.tensor_parallel_size
+                )
+
+            policy[LlamaDecoderLayer] = ModulePolicyDescription(
+                attribute_replacement=decoder_attribute_replacement,
                 sub_module_replacement=[
                     SubModuleReplacementDescription(
-                        suffix="attn.c_attn",
-                        target_module=col_nn.GPT2FusedLinearConv1D_Col,
-                        kwargs={"n_fused": 3, "seq_parallel": use_sequence_parallel, "overlap": overlap},
+                        suffix="self_attn.q_proj",
+                        target_module=Linear1D_Col,
+                        kwargs=dict(seq_parallel_mode=sp_mode),
                     ),
                     SubModuleReplacementDescription(
-                        suffix="attn.c_proj",
-                        target_module=col_nn.GPT2FusedLinearConv1D_Row,
-                        kwargs={
-                            "seq_parallel": use_sequence_parallel,
-                        },
+                        suffix="self_attn.k_proj",
+                        target_module=Linear1D_Col,
+                        kwargs=dict(seq_parallel_mode=sp_mode),
                     ),
                     SubModuleReplacementDescription(
-                        suffix="mlp.c_fc",
-                        target_module=col_nn.GPT2FusedLinearConv1D_Col,
-                        kwargs={"n_fused": 1, "seq_parallel": use_sequence_parallel, "overlap": overlap},
+                        suffix="self_attn.v_proj",
+                        target_module=Linear1D_Col,
+                        kwargs=dict(seq_parallel_mode=sp_mode),
                     ),
                     SubModuleReplacementDescription(
-                        suffix="mlp.c_proj",
-                        target_module=col_nn.GPT2FusedLinearConv1D_Row,
-                        kwargs={"seq_parallel": use_sequence_parallel},
+                        suffix="self_attn.o_proj",
+                        target_module=Linear1D_Row,
+                        kwargs=dict(seq_parallel_mode=sp_mode),
                     ),
                     SubModuleReplacementDescription(
-                        suffix="attn.attn_dropout",
-                        target_module=col_nn.DropoutForParallelInput,
+                        suffix="mlp.gate_proj",
+                        target_module=Linear1D_Col,
+                        kwargs=dict(seq_parallel_mode=sp_mode),
                     ),
                     SubModuleReplacementDescription(
-                        suffix="attn.resid_dropout",
-                        target_module=col_nn.DropoutForParallelInput,
+                        suffix="mlp.up_proj",
+                        target_module=Linear1D_Col,
+                        kwargs=dict(seq_parallel_mode=sp_mode),
                     ),
                     SubModuleReplacementDescription(
-                        suffix="mlp.dropout",
-                        target_module=col_nn.DropoutForParallelInput,
+                        suffix="mlp.down_proj",
+                        target_module=Linear1D_Row,
+                        kwargs=dict(seq_parallel_mode=sp_mode),
                     ),
                 ],
             )
 
-        # optimization configuration
-        self.append_or_create_submodule_replacement(
-            description=SubModuleReplacementDescription(
-                suffix="ln_f",
-                target_module=norm_cls,
-            ),
-            policy=policy,
-            target_key=GPT2Model,
-        )
+        if embedding_cls is not None:
+            self.append_or_create_submodule_replacement(
+                description=SubModuleReplacementDescription(
+                    suffix="embed_tokens",
+                    target_module=embedding_cls,
+                    kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
+                ),
+                policy=policy,
+                target_key=LlamaModel,
+            )
 
+        # optimization configuration
         self.append_or_create_submodule_replacement(
             description=[
                 SubModuleReplacementDescription(
-                    suffix="ln_1",
-                    target_module=norm_cls,
-                    kwargs={"sp_partial_derived": use_sequence_parallel},
-                ),
-                SubModuleReplacementDescription(
-                    suffix="ln_2",
+                    suffix="input_layernorm",
                     target_module=norm_cls,
-                    kwargs={"sp_partial_derived": use_sequence_parallel},
+                    kwargs={"sp_partial_derived": sp_partial_derived},
                 ),
                 SubModuleReplacementDescription(
-                    suffix="ln_cross_attn",
+                    suffix="post_attention_layernorm",
                     target_module=norm_cls,
-                    ignore_if_not_exist=True,
-                    kwargs={"sp_partial_derived": use_sequence_parallel},
+                    kwargs={"sp_partial_derived": sp_partial_derived},
                 ),
             ],
             policy=policy,
-            target_key=GPT2Block,
+            target_key=LlamaDecoderLayer,
+        )
+
+        self.append_or_create_submodule_replacement(
+            description=SubModuleReplacementDescription(
+                suffix="norm",
+                target_module=norm_cls,
+                kwargs={"sp_partial_derived": sp_partial_derived},
+            ),
+            policy=policy,
+            target_key=LlamaModel,
         )
 
-        if self.shard_config.enable_flash_attention:
+        # use flash attention
+        if use_flash_attention:
             self.append_or_create_method_replacement(
                 description={
-                    "forward": get_gpt2_flash_attention_forward(),
+                    "forward": get_llama_flash_attention_forward(self.shard_config, sp_mode, sp_group, sp_size),
                 },
                 policy=policy,
-                target_key=GPT2Attention,
+                target_key=attn_cls,
             )
-
-        if self.shard_config.enable_sequence_parallelism:
-            policy[GPT2Model].method_replacement = {"forward": gpt2_sequence_parallel_forward_fn(self.shard_config)}
+            if self.pipeline_stage_manager is None:
+                # replace llama model forward method
+                self.append_or_create_method_replacement(
+                    description={
+                        "forward": get_llama_model_forward_for_flash_attn(self.shard_config),
+                    },
+                    policy=policy,
+                    target_key=LlamaModel,
+                )
 
         return policy
 
     def postprocess(self):
         return self.model
 
-    def get_held_layers(self) -> List[nn.Module]:
-        """Get pipeline layers for current stage."""
-        assert self.pipeline_stage_manager is not None
-
-        if self.model.__class__.__name__ == "GPT2Model":
-            module = self.model
-        else:
-            module = self.model.transformer
-        stage_manager = self.pipeline_stage_manager
-
-        held_layers = []
-        if stage_manager.is_interleave:
-            assert stage_manager.num_model_chunks is not None
-            layers_per_stage = self.distribute_layers(
-                len(module.h), stage_manager.num_stages * stage_manager.num_model_chunks
-            )
-            stage_indices = Policy.get_stage_index(
-                layers_per_stage,
-                stage_manager.stage,
-                num_model_chunks=stage_manager.num_model_chunks,
-                num_stages=stage_manager.num_stages,
-            )
-            if stage_manager.is_first_stage(ignore_chunk=True):
-                held_layers.append(module.wte)
-                held_layers.append(module.wpe)
-                held_layers.append(module.drop)
-            for start_idx, end_idx in stage_indices:
-                held_layers.extend(module.h[start_idx:end_idx])
-            if stage_manager.is_last_stage(ignore_chunk=True):
-                held_layers.append(module.ln_f)
-        else:
-            layers_per_stage = self.distribute_layers(len(module.h), stage_manager.num_stages)
-            if stage_manager.is_first_stage():
-                held_layers.append(module.wte)
-                held_layers.append(module.wpe)
-                held_layers.append(module.drop)
-            start_idx, end_idx = self.get_stage_index(layers_per_stage, stage_manager.stage)
-            held_layers.extend(module.h[start_idx:end_idx])
-            if stage_manager.is_last_stage():
-                held_layers.append(module.ln_f)
-        return held_layers
-
     def set_pipeline_forward(self, model_cls: nn.Module, new_forward: Callable, policy: Dict) -> None:
         """If under pipeline parallel setting, replacing the original forward method of huggingface
         to customized forward method, and add this changing to policy."""
-        if not self.pipeline_stage_manager:
-            raise ValueError("set_pipeline_forward method can only be called when pipeline parallel is enabled.")
+        if self.pipeline_stage_manager is None:
+            return
+
         stage_manager = self.pipeline_stage_manager
-        if self.model.__class__.__name__ == "GPT2Model":
+        if self.model.__class__.__name__ == "LlamaModel":
             module = self.model
         else:
-            module = self.model.transformer
+            module = self.model.model
 
         if stage_manager.is_interleave:
-            layers_per_stage = self.distribute_layers(
-                len(module.h), stage_manager.num_stages * stage_manager.num_model_chunks
-            )
-            stage_manager.stage_indices = Policy.get_stage_index(
-                layers_per_stage,
-                stage_manager.stage,
-                num_model_chunks=stage_manager.num_model_chunks,
-                num_stages=stage_manager.num_stages,
-            )
+            layers_per_stage = stage_manager.distribute_layers(len(module.layers))
+            stage_manager.stage_indices = stage_manager.get_stage_index(layers_per_stage)
             method_replacement = {
                 "forward": partial(new_forward, stage_manager=stage_manager, shard_config=self.shard_config)
             }
+
         else:
-            layers_per_stage = Policy.distribute_layers(len(module.h), stage_manager.num_stages)
-            stage_index = Policy.get_stage_index(layers_per_stage, stage_manager.stage)
+            layers_per_stage = stage_manager.distribute_layers(len(module.layers))
+            stage_index = stage_manager.get_stage_index(layers_per_stage)
             method_replacement = {
                 "forward": partial(
                     new_forward, stage_manager=stage_manager, stage_index=stage_index, shard_config=self.shard_config
                 )
             }
+
         self.append_or_create_method_replacement(description=method_replacement, policy=policy, target_key=model_cls)
 
+    def get_held_layers(self) -> List[Module]:
+        """Get pipeline layers for current stage."""
+        assert self.pipeline_stage_manager is not None
+
+        if self.model.__class__.__name__ == "LlamaModel":
+            module = self.model
+        else:
+            module = self.model.model
+        stage_manager = self.pipeline_stage_manager
+
+        held_layers = []
+        if stage_manager.is_interleave:
+            assert stage_manager.num_model_chunks is not None
+            layers_per_stage = stage_manager.distribute_layers(len(module.layers))
+            stage_indices = stage_manager.get_stage_index(layers_per_stage)
+            if stage_manager.is_first_stage(ignore_chunk=True):
+                held_layers.append(module.embed_tokens)
+            for start_idx, end_idx in stage_indices:
+                held_layers.extend(module.layers[start_idx:end_idx])
+            if stage_manager.is_last_stage(ignore_chunk=True):
+                held_layers.append(module.norm)
+
+        else:
+            layers_per_stage = stage_manager.distribute_layers(len(module.layers))
+            if stage_manager.is_first_stage():
+                held_layers.append(module.embed_tokens)
+            start_idx, end_idx = stage_manager.get_stage_index(layers_per_stage)
+            held_layers.extend(module.layers[start_idx:end_idx])
+            if stage_manager.is_last_stage():
+                held_layers.append(module.norm)
+
+        return held_layers
 
-# GPT2Model
-class GPT2ModelPolicy(GPT2Policy):
-    def module_policy(self):
-        from transformers.models.gpt2.modeling_gpt2 import GPT2Model
 
+class LlamaModelPolicy(LlamaPolicy):
+    def module_policy(self):
         policy = super().module_policy()
+        from transformers.models.llama.modeling_llama import LlamaModel
 
-        if self.pipeline_stage_manager is not None:
+        if self.pipeline_stage_manager:
+            # set None as default
             self.set_pipeline_forward(
-                model_cls=GPT2Model, new_forward=GPT2PipelineForwards.gpt2_model_forward, policy=policy
+                model_cls=LlamaModel, new_forward=LlamaPipelineForwards.llama_model_forward, policy=policy
             )
         return policy
 
-    def get_held_layers(self) -> List[nn.Module]:
-        return super().get_held_layers()
+    def get_held_layers(self) -> List[Module]:
+        """Get pipeline layers for current stage."""
+        held_layers = super().get_held_layers()
+        return held_layers
 
     def get_shared_params(self) -> List[Dict[int, Tensor]]:
-        """No shared params in GPT2Model."""
+        """No shared params in llama model"""
         return []
 
 
-# GPT2LMHeadModel
-class GPT2LMHeadModelPolicy(GPT2Policy):
+class LlamaForCausalLMPolicy(LlamaPolicy):
     def module_policy(self):
-        from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel
+        from transformers import LlamaForCausalLM
 
-        module_policy = super().module_policy()
+        policy = super().module_policy()
 
-        if self.shard_config.enable_tensor_parallelism:
-            addon_module = {
-                GPT2LMHeadModel: ModulePolicyDescription(
+        if self.shard_config.enable_tensor_parallelism and not self.shard_config.enable_sequence_parallelism:
+            # add a new item for casual lm
+            new_item = {
+                LlamaForCausalLM: ModulePolicyDescription(
                     sub_module_replacement=[
                         SubModuleReplacementDescription(
-                            suffix="lm_head", target_module=col_nn.Linear1D_Col, kwargs={"gather_output": False}
+                            suffix="lm_head",
+                            target_module=VocabParallelLMHead1D,
+                            kwargs={
+                                "gather_output": not self.shard_config.parallel_output,
+                                "make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by,
+                            },
                         )
                     ],
-                    method_replacement={"forward": get_lm_forward_with_dist_cross_entropy(self.shard_config)},
                 )
             }
-            module_policy.update(addon_module)
-
-        if self.pipeline_stage_manager is not None:
-            self.set_pipeline_forward(
-                model_cls=GPT2LMHeadModel,
-                new_forward=GPT2PipelineForwards.gpt2_lmhead_model_forward,
-                policy=module_policy,
-            )
-        return module_policy
-
-    def get_held_layers(self) -> List[nn.Module]:
-        held_layers = super().get_held_layers()
-        if self.pipeline_stage_manager.is_last_stage(ignore_chunk=True):
-            held_layers.append(self.model.lm_head)
-        return held_layers
-
-    def get_shared_params(self) -> List[Dict[int, Tensor]]:
-        """The weights of wte and lm_head are shared."""
-        module = self.model
-        stage_manager = self.pipeline_stage_manager
-        if stage_manager is not None:
-            if stage_manager.num_stages > 1 and id(module.transformer.wte.weight) == id(module.lm_head.weight):
-                first_stage, last_stage = 0, stage_manager.num_stages - 1
-                return [{first_stage: module.transformer.wte.weight, last_stage: module.lm_head.weight}]
-        return []
-
-
-# GPT2DoubleHeadsModel
-class GPT2DoubleHeadsModelPolicy(GPT2Policy):
-    def module_policy(self):
-        from transformers.models.gpt2.modeling_gpt2 import GPT2DoubleHeadsModel
-
-        module_policy = super().module_policy()
-
-        if self.shard_config.enable_tensor_parallelism:
-            addon_module = {
-                GPT2DoubleHeadsModel: ModulePolicyDescription(
+            if self.shard_config.parallel_output:
+                new_item[LlamaForCausalLM].method_replacement = {
+                    "forward": get_lm_forward_with_dist_cross_entropy(self.shard_config)
+                }
+        else:
+            new_item = {
+                LlamaForCausalLM: ModulePolicyDescription(
                     sub_module_replacement=[
                         SubModuleReplacementDescription(
-                            suffix="lm_head", target_module=col_nn.Linear1D_Col, kwargs={"gather_output": True}
+                            suffix="lm_head",
+                            target_module=PaddingLMHead,
+                            kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
                         )
-                    ]
+                    ],
                 )
             }
-            module_policy.update(addon_module)
+        policy.update(new_item)
 
-        if self.pipeline_stage_manager is not None:
+        if self.pipeline_stage_manager:
+            # set None as default
             self.set_pipeline_forward(
-                model_cls=GPT2DoubleHeadsModel,
-                new_forward=GPT2PipelineForwards.gpt2_double_heads_model_forward,
-                policy=module_policy,
+                model_cls=LlamaForCausalLM, new_forward=LlamaPipelineForwards.llama_for_causal_lm_forward, policy=policy
             )
 
-        return module_policy
-
-    def get_held_layers(self) -> List[nn.Module]:
-        held_layers = super().get_held_layers()
-        if self.pipeline_stage_manager.is_last_stage():
-            multiple_choice_head = self.model.multiple_choice_head
-            held_layers.append(self.model.lm_head)
-            held_layers.append(multiple_choice_head.summary)
-            held_layers.append(multiple_choice_head.activation)
-            held_layers.append(multiple_choice_head.first_dropout)
-            held_layers.append(multiple_choice_head.last_dropout)
-
-        return held_layers
+        return policy
 
-    def get_shared_params(self) -> List[Dict[int, Tensor]]:
-        """The weights of wte and lm_head are shared."""
-        module = self.model
+    def get_held_layers(self) -> List[Module]:
+        """Get pipeline layers for current stage."""
         stage_manager = self.pipeline_stage_manager
-        if stage_manager is not None:
-            if stage_manager.num_stages > 1 and id(module.transformer.wte.weight) == id(module.lm_head.weight):
-                first_stage, last_stage = 0, stage_manager.num_stages - 1
-                return [{first_stage: module.transformer.wte.weight, last_stage: module.lm_head.weight}]
-        return []
-
-
-# GPT2ForQuestionAnswering
-class GPT2ForQuestionAnsweringPolicy(GPT2Policy):
-    def module_policy(self):
-        from transformers.models.gpt2.modeling_gpt2 import GPT2ForQuestionAnswering
-
-        module_policy = super().module_policy()
-
-        if self.pipeline_stage_manager is not None:
-            self.set_pipeline_forward(
-                model_cls=GPT2ForQuestionAnswering,
-                new_forward=GPT2PipelineForwards.gpt2_for_question_answering_forward,
-                policy=module_policy,
-            )
-
-        return module_policy
-
-    def get_held_layers(self) -> List[nn.Module]:
         held_layers = super().get_held_layers()
-        if self.pipeline_stage_manager.is_last_stage():
-            held_layers.append(self.model.qa_outputs)
+        if stage_manager.is_last_stage(ignore_chunk=True):
+            held_layers.append(self.model.lm_head)
         return held_layers
 
     def get_shared_params(self) -> List[Dict[int, Tensor]]:
-        """No shared_params in gpt2 for QA."""
+        llama_model = self.model.model
+        if self.pipeline_stage_manager and self.pipeline_stage_manager.num_stages > 1:
+            if (
+                id(llama_model.embed_tokens.weight) == id(self.model.lm_head.weight)
+                and self.pipeline_stage_manager.num_stages > 1
+            ):
+                # tie weights
+                return [
+                    {
+                        0: llama_model.embed_tokens.weight,
+                        self.pipeline_stage_manager.num_stages - 1: self.model.lm_head.weight,
+                    }
+                ]
         return []
 
 
-# GPT2ForTokenClassification
-class GPT2ForTokenClassificationPolicy(GPT2Policy):
+class LlamaForSequenceClassificationPolicy(LlamaPolicy):
     def module_policy(self):
-        from transformers.models.gpt2.modeling_gpt2 import GPT2ForTokenClassification
+        from transformers import LlamaForSequenceClassification
 
-        module_policy = super().module_policy()
+        policy = super().module_policy()
 
         if self.shard_config.enable_tensor_parallelism:
-            addon_module = {
-                GPT2ForTokenClassification: ModulePolicyDescription(
+            # add a new item for sequence classification
+            new_item = {
+                LlamaForSequenceClassification: ModulePolicyDescription(
                     sub_module_replacement=[
-                        SubModuleReplacementDescription(suffix="dropout", target_module=col_nn.DropoutForParallelInput)
+                        SubModuleReplacementDescription(
+                            suffix="score", target_module=Linear1D_Col, kwargs=dict(gather_output=True)
+                        )
                     ]
                 )
             }
-            module_policy.update(addon_module)
-
-        if self.pipeline_stage_manager is not None:
+            policy.update(new_item)
+        # to be confirmed
+        if self.pipeline_stage_manager:
+            # set None as default
             self.set_pipeline_forward(
-                model_cls=GPT2ForTokenClassification,
-                new_forward=GPT2PipelineForwards.gpt2_for_token_classification_forward,
-                policy=module_policy,
-            )
-        return module_policy
-
-    def get_held_layers(self) -> List[nn.Module]:
-        held_layers = super().get_held_layers()
-        if self.pipeline_stage_manager.is_last_stage():
-            held_layers.append(self.model.dropout)
-            held_layers.append(self.model.classifier)
-        return held_layers
-
-    def get_shared_params(self) -> List[Dict[int, Tensor]]:
-        """No shared params in GPT2ForTokenClassification."""
-        return []
-
-
-# GPT2ForSequenceClassification
-class GPT2ForSequenceClassificationPolicy(GPT2Policy):
-    def module_policy(self):
-        from transformers.models.gpt2.modeling_gpt2 import GPT2ForSequenceClassification
-
-        module_policy = super().module_policy()
-
-        if self.pipeline_stage_manager is not None:
-            self.set_pipeline_forward(
-                model_cls=GPT2ForSequenceClassification,
-                new_forward=GPT2PipelineForwards.gpt2_for_sequence_classification_forward,
-                policy=module_policy,
+                model_cls=LlamaForSequenceClassification,
+                new_forward=LlamaPipelineForwards.llama_for_sequence_classification_forward,
+                policy=policy,
             )
-        return module_policy
+        return policy
 
-    def get_held_layers(self) -> List[nn.Module]:
+    def get_held_layers(self) -> List[Module]:
+        """Get pipeline layers for current stage."""
+        stage_manager = self.pipeline_stage_manager
         held_layers = super().get_held_layers()
-        if self.pipeline_stage_manager.is_last_stage():
+        if stage_manager.is_last_stage(ignore_chunk=True):
             held_layers.append(self.model.score)
         return held_layers
 
     def get_shared_params(self) -> List[Dict[int, Tensor]]:
-        """No shared params in GPT2ForTokenClassification."""
+        """No shared params in llama for sequence classification model"""
         return []
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/policies/gptj.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/gptj.py`

 * *Files 7% similar despite different names*

```diff
@@ -2,15 +2,19 @@
 from functools import partial
 from typing import Callable, Dict, List
 
 from torch import Tensor, nn
 
 import colossalai.shardformer.layer as col_nn
 
-from ..modeling.gptj import GPTJPipelineForwards, get_gptj_flash_attention_forward
+from ..modeling.gptj import (
+    GPTJPipelineForwards,
+    get_gptj_flash_attention_forward,
+    gptj_model_forward_for_flash_attention,
+)
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 __all__ = [
     "GPTJPolicy",
     "GPTJModelPolicy",
     "GPTJForCausalLMPolicy",
     "GPTJForSequenceClassificationPolicy",
@@ -21,44 +25,48 @@
 
 
 class GPTJPolicy(Policy):
     def config_sanity_check(self):
         pass
 
     def preprocess(self):
-        # reshape the embedding layer
-        r"""
-        Reshape the Embedding layer to make the embedding dimension divisible by world_size
-        """
-        if self.shard_config.enable_tensor_parallelism:
-            vocab_size = self.model.config.vocab_size
-            world_size = self.shard_config.tensor_parallel_size
-            if vocab_size % world_size != 0:
-                new_vocab_size = vocab_size + world_size - vocab_size % world_size
-                self.model.resize_token_embeddings(new_vocab_size)
+        self.tie_weight = self.tie_weight_check()
+        self.origin_attn_implement = self.model.config._attn_implementation
         return self.model
 
     def module_policy(self):
         from transformers.models.gptj.modeling_gptj import GPTJAttention, GPTJBlock, GPTJModel
 
+        ATTN_IMPLEMENTATION = {
+            "eager": GPTJAttention,
+        }
+
         policy = {}
+
+        attn_cls = ATTN_IMPLEMENTATION[self.origin_attn_implement]
+
+        embedding_cls = None
+        if self.shard_config.enable_tensor_parallelism:
+            embedding_cls = col_nn.VocabParallelEmbedding1D
+        else:
+            if self.tie_weight:
+                embedding_cls = col_nn.PaddingEmbedding
+
         if self.shard_config.enable_sequence_parallelism:
             self.shard_config.enable_sequence_parallelism = False
             warnings.warn("GPTJ doesn't support sequence parallelism now, will ignore the sequence parallelism flag.")
-        use_sequence_parallel = self.shard_config.enable_sequence_parallelism
 
         overlap = self.shard_config.enable_sequence_overlap
         if self.shard_config.enable_tensor_parallelism:
+            assert (
+                self.model.config.num_attention_heads % self.shard_config.tensor_parallel_size == 0
+            ), f"The number of attention heads must be divisible by tensor parallel size."
             policy[GPTJModel] = ModulePolicyDescription(
                 sub_module_replacement=[
                     SubModuleReplacementDescription(
-                        suffix="wte",
-                        target_module=col_nn.VocabParallelEmbedding1D,
-                    ),
-                    SubModuleReplacementDescription(
                         suffix="drop",
                         target_module=col_nn.DropoutForParallelInput,
                     ),
                 ]
             )
 
             policy[GPTJBlock] = ModulePolicyDescription(
@@ -67,40 +75,43 @@
                     "attn.num_attention_heads": self.model.config.num_attention_heads
                     // self.shard_config.tensor_parallel_size,
                 },
                 sub_module_replacement=[
                     SubModuleReplacementDescription(
                         suffix="attn.k_proj",
                         target_module=col_nn.Linear1D_Col,
-                        kwargs={"seq_parallel": use_sequence_parallel, "overlap": overlap},
+                        kwargs={
+                            "overlap": overlap,
+                        },
                     ),
                     SubModuleReplacementDescription(
                         suffix="attn.q_proj",
                         target_module=col_nn.Linear1D_Col,
-                        kwargs={"seq_parallel": use_sequence_parallel, "overlap": overlap},
+                        kwargs={
+                            "overlap": overlap,
+                        },
                     ),
                     SubModuleReplacementDescription(
                         suffix="attn.v_proj",
                         target_module=col_nn.Linear1D_Col,
-                        kwargs={"seq_parallel": use_sequence_parallel, "overlap": overlap},
+                        kwargs={
+                            "overlap": overlap,
+                        },
                     ),
                     SubModuleReplacementDescription(
                         suffix="attn.out_proj",
                         target_module=col_nn.Linear1D_Row,
-                        kwargs={"seq_parallel": use_sequence_parallel},
                     ),
                     SubModuleReplacementDescription(
                         suffix="mlp.fc_in",
                         target_module=col_nn.Linear1D_Col,
-                        kwargs={"seq_parallel": use_sequence_parallel},
                     ),
                     SubModuleReplacementDescription(
                         suffix="mlp.fc_out",
                         target_module=col_nn.Linear1D_Row,
-                        kwargs={"seq_parallel": use_sequence_parallel},
                     ),
                     SubModuleReplacementDescription(
                         suffix="attn.attn_dropout",
                         target_module=col_nn.DropoutForParallelInput,
                     ),
                     SubModuleReplacementDescription(
                         suffix="attn.resid_dropout",
@@ -109,14 +120,25 @@
                     SubModuleReplacementDescription(
                         suffix="mlp.dropout",
                         target_module=col_nn.DropoutForParallelInput,
                     ),
                 ],
             )
 
+        if embedding_cls is not None:
+            self.append_or_create_submodule_replacement(
+                description=SubModuleReplacementDescription(
+                    suffix="wte",
+                    target_module=embedding_cls,
+                    kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
+                ),
+                policy=policy,
+                target_key=GPTJModel,
+            )
+
         # optimization configuration
         if self.shard_config.enable_fused_normalization:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
                     suffix="ln_f",
                     target_module=col_nn.FusedLayerNorm,
                 ),
@@ -137,16 +159,22 @@
 
         if self.shard_config.enable_flash_attention:
             self.append_or_create_method_replacement(
                 description={
                     "forward": get_gptj_flash_attention_forward(),
                 },
                 policy=policy,
-                target_key=GPTJAttention,
+                target_key=attn_cls,
             )
+            if not self.shard_config.pipeline_stage_manager:
+                self.append_or_create_method_replacement(
+                    description={"forward": gptj_model_forward_for_flash_attention(self.shard_config)},
+                    policy=policy,
+                    target_key=GPTJModel,
+                )
 
         return policy
 
     def postprocess(self):
         return self.model
 
     def get_held_layers(self) -> List[nn.Module]:
@@ -156,19 +184,19 @@
         if self.model.__class__.__name__ == "GPTJModel":
             module = self.model
         else:
             module = self.model.transformer
         stage_manager = self.pipeline_stage_manager
 
         held_layers = []
-        layers_per_stage = self.distribute_layers(len(module.h), stage_manager.num_stages)
+        layers_per_stage = stage_manager.distribute_layers(len(module.h))
         if stage_manager.is_first_stage():
             held_layers.append(module.wte)
             held_layers.append(module.drop)
-        start_idx, end_idx = self.get_stage_index(layers_per_stage, stage_manager.stage)
+        start_idx, end_idx = stage_manager.get_stage_index(layers_per_stage)
         held_layers.extend(module.h[start_idx:end_idx])
         if stage_manager.is_last_stage():
             held_layers.append(module.ln_f)
         return held_layers
 
     def set_pipeline_forward(self, model_cls: nn.Module, new_forward: Callable, policy: Dict) -> None:
         """If under pipeline parallel setting, replacing the original forward method of huggingface
@@ -177,19 +205,22 @@
             raise ValueError("set_pipeline_forward method can only be called when pipeline parallel is enabled.")
         stage_manager = self.pipeline_stage_manager
         if self.model.__class__.__name__ == "GPTJModel":
             module = self.model
         else:
             module = self.model.transformer
 
-        layers_per_stage = Policy.distribute_layers(len(module.h), stage_manager.num_stages)
-        stage_index = Policy.get_stage_index(layers_per_stage, stage_manager.stage)
+        layers_per_stage = stage_manager.distribute_layers(len(module.h))
+        stage_index = stage_manager.get_stage_index(layers_per_stage)
         method_replacement = {
             "forward": partial(
-                new_forward, stage_manager=stage_manager, stage_index=stage_index, shard_config=self.shard_config
+                new_forward,
+                stage_manager=stage_manager,
+                stage_index=stage_index,
+                shard_config=self.shard_config,
             )
         }
         self.append_or_create_method_replacement(description=method_replacement, policy=policy, target_key=model_cls)
 
 
 # GPTJModel
 class GPTJModelPolicy(GPTJPolicy):
@@ -199,15 +230,17 @@
     def module_policy(self):
         from transformers.models.gptj.modeling_gptj import GPTJModel
 
         policy = super().module_policy()
 
         if self.pipeline_stage_manager is not None:
             self.set_pipeline_forward(
-                model_cls=GPTJModel, new_forward=GPTJPipelineForwards.gptj_model_forward, policy=policy
+                model_cls=GPTJModel,
+                new_forward=GPTJPipelineForwards.gptj_model_forward,
+                policy=policy,
             )
         return policy
 
     def get_held_layers(self) -> List[nn.Module]:
         return super().get_held_layers()
 
     def get_shared_params(self) -> List[Dict[int, Tensor]]:
@@ -226,24 +259,43 @@
         policy = super().module_policy()
 
         if self.shard_config.enable_tensor_parallelism:
             addon_module = {
                 GPTJForCausalLM: ModulePolicyDescription(
                     sub_module_replacement=[
                         SubModuleReplacementDescription(
-                            suffix="lm_head", target_module=col_nn.Linear1D_Col, kwargs={"gather_output": True}
+                            suffix="lm_head",
+                            target_module=col_nn.VocabParallelLMHead1D,
+                            kwargs={
+                                "gather_output": True,
+                                "make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by,
+                            },
                         )
                     ]
                 )
             }
-            policy.update(addon_module)
+        else:
+            addon_module = {
+                GPTJForCausalLM: ModulePolicyDescription(
+                    sub_module_replacement=[
+                        SubModuleReplacementDescription(
+                            suffix="lm_head",
+                            target_module=col_nn.PaddingLMHead,
+                            kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
+                        )
+                    ]
+                )
+            }
+        policy.update(addon_module)
 
         if self.pipeline_stage_manager is not None:
             self.set_pipeline_forward(
-                model_cls=GPTJForCausalLM, new_forward=GPTJPipelineForwards.gptj_causallm_model_forward, policy=policy
+                model_cls=GPTJForCausalLM,
+                new_forward=GPTJPipelineForwards.gptj_causallm_model_forward,
+                policy=policy,
             )
         return policy
 
     def get_held_layers(self) -> List[nn.Module]:
         held_layers = super().get_held_layers()
         if self.pipeline_stage_manager.is_last_stage():
             held_layers.append(self.model.lm_head)
@@ -252,15 +304,20 @@
     def get_shared_params(self) -> List[Dict[int, Tensor]]:
         """The weights of wte and lm_head are shared."""
         module = self.model
         stage_manager = self.pipeline_stage_manager
         if stage_manager is not None:
             if stage_manager.num_stages > 1 and id(module.transformer.wte.weight) == id(module.lm_head.weight):
                 first_stage, last_stage = 0, stage_manager.num_stages - 1
-                return [{first_stage: module.transformer.wte.weight, last_stage: module.lm_head.weight}]
+                return [
+                    {
+                        first_stage: module.transformer.wte.weight,
+                        last_stage: module.lm_head.weight,
+                    }
+                ]
         return []
 
 
 # GPTJForSequenceClassification
 class GPTJForSequenceClassificationPolicy(GPTJPolicy):
     def __init__(self) -> None:
         super().__init__()
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/policies/llama.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/mistral.py`

 * *Files 10% similar despite different names*

```diff
@@ -2,67 +2,88 @@
 from functools import partial
 from typing import Callable, Dict, List, Union
 
 import torch.nn as nn
 from torch import Tensor
 from torch.nn import Module
 
-from colossalai.shardformer.layer import FusedRMSNorm, Linear1D_Col, Linear1D_Row, RMSNorm, VocabParallelEmbedding1D
+from colossalai.shardformer.layer import (
+    FusedRMSNorm,
+    Linear1D_Col,
+    Linear1D_Row,
+    PaddingEmbedding,
+    PaddingLMHead,
+    VocabParallelEmbedding1D,
+    VocabParallelLMHead1D,
+)
 
-from ..modeling.llama import (
-    LlamaPipelineForwards,
-    get_llama_flash_attention_forward,
-    get_lm_forward_with_dist_cross_entropy,
+from ..modeling.mistral import (
+    MistralForwards,
+    get_mistral_flash_attention_forward,
+    get_mistral_model_forward_for_flash_attn,
 )
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
-__all__ = ["LlamaPolicy", "LlamaForCausalLMPolicy", "LlamaForSequenceClassificationPolicy"]
+__all__ = ["MistralPolicy", "MistralModelPolicy", "MistralForCausalLMPolicy", "MistralForSequenceClassificationPolicy"]
 
 
-class LlamaPolicy(Policy):
+class MistralPolicy(Policy):
     def config_sanity_check(self):
         pass
 
     def preprocess(self):
-        if self.shard_config.enable_tensor_parallelism:
-            # Resize embedding
-            vocab_size = self.model.config.vocab_size
-            world_size = self.shard_config.tensor_parallel_size
-
-            if vocab_size % world_size != 0:
-                new_vocab_size = vocab_size + world_size - vocab_size % world_size
-                self.model.resize_token_embeddings(new_vocab_size)
-
+        self.tie_weight = self.tie_weight_check()
+        self.origin_attn_implement = self.model.config._attn_implementation
         return self.model
 
     def module_policy(self) -> Dict[Union[str, nn.Module], ModulePolicyDescription]:
-        from transformers.models.llama.modeling_llama import LlamaAttention, LlamaDecoderLayer, LlamaModel
+        from transformers.models.mistral.modeling_mistral import (
+            MistralAttention,
+            MistralDecoderLayer,
+            MistralFlashAttention2,
+            MistralModel,
+        )
+
+        ATTN_IMPLEMENTATION = {
+            "eager": MistralAttention,
+            "flash_attention_2": MistralFlashAttention2,
+        }
 
         policy = {}
 
-        if self.shard_config.enable_fused_normalization:
-            norm_cls = FusedRMSNorm
+        attn_cls = ATTN_IMPLEMENTATION[self.model.config._attn_implementation]
+
+        embedding_cls = None
+        if self.shard_config.enable_tensor_parallelism:
+            embedding_cls = VocabParallelEmbedding1D
         else:
-            norm_cls = RMSNorm
+            if self.tie_weight:
+                embedding_cls = PaddingEmbedding
 
         if self.shard_config.enable_sequence_parallelism:
             self.shard_config.enable_sequence_parallelism = False
-            warnings.warn("Llama doesn't support sequence parallelism now, will ignore the sequence parallelism flag.")
+            warnings.warn(
+                "Mistral doesn't support sequence parallelism now, will ignore the sequence parallelism flag."
+            )
 
         if self.shard_config.enable_tensor_parallelism:
+            assert (
+                self.model.config.num_attention_heads % self.shard_config.tensor_parallel_size == 0
+            ), f"The number of attention heads must be divisible by tensor parallel size."
+            assert (
+                self.model.config.num_key_value_heads % self.shard_config.tensor_parallel_size == 0
+            ), f"The number of key_value heads must be divisible by tensor parallel size."
             decoder_attribute_replacement = {
                 "self_attn.hidden_size": self.model.config.hidden_size // self.shard_config.tensor_parallel_size,
                 "self_attn.num_heads": self.model.config.num_attention_heads // self.shard_config.tensor_parallel_size,
+                "self_attn.num_key_value_heads": self.model.config.num_key_value_heads
+                // self.shard_config.tensor_parallel_size,
             }
-            if getattr(self.model.config, "num_key_value_heads", False):
-                decoder_attribute_replacement["self_attn.num_key_value_heads"] = (
-                    self.model.config.num_key_value_heads // self.shard_config.tensor_parallel_size
-                )
 
-            policy[LlamaDecoderLayer] = ModulePolicyDescription(
+            policy[MistralDecoderLayer] = ModulePolicyDescription(
                 attribute_replacement=decoder_attribute_replacement,
                 sub_module_replacement=[
                     SubModuleReplacementDescription(
                         suffix="self_attn.q_proj",
                         target_module=Linear1D_Col,
                     ),
                     SubModuleReplacementDescription(
@@ -88,245 +109,259 @@
                     SubModuleReplacementDescription(
                         suffix="mlp.down_proj",
                         target_module=Linear1D_Row,
                     ),
                 ],
             )
 
+        if embedding_cls is not None:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
                     suffix="embed_tokens",
-                    target_module=VocabParallelEmbedding1D,
+                    target_module=embedding_cls,
+                    kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
                 ),
                 policy=policy,
-                target_key=LlamaModel,
+                target_key=MistralModel,
             )
 
         # optimization configuration
-        self.append_or_create_submodule_replacement(
-            description=[
-                SubModuleReplacementDescription(
-                    suffix="input_layernorm",
-                    target_module=norm_cls,
-                ),
-                SubModuleReplacementDescription(
-                    suffix="post_attention_layernorm",
-                    target_module=norm_cls,
-                ),
-            ],
-            policy=policy,
-            target_key=LlamaDecoderLayer,
-        )
+        if self.shard_config.enable_fused_normalization:
+            self.append_or_create_submodule_replacement(
+                description=[
+                    SubModuleReplacementDescription(
+                        suffix="input_layernorm",
+                        target_module=FusedRMSNorm,
+                    ),
+                    SubModuleReplacementDescription(
+                        suffix="post_attention_layernorm",
+                        target_module=FusedRMSNorm,
+                    ),
+                ],
+                policy=policy,
+                target_key=MistralDecoderLayer,
+            )
 
-        self.append_or_create_submodule_replacement(
-            description=SubModuleReplacementDescription(
-                suffix="norm",
-                target_module=norm_cls,
-            ),
-            policy=policy,
-            target_key=LlamaModel,
-        )
+            self.append_or_create_submodule_replacement(
+                description=SubModuleReplacementDescription(
+                    suffix="norm",
+                    target_module=FusedRMSNorm,
+                ),
+                policy=policy,
+                target_key=MistralModel,
+            )
 
-        # use flash attention
         if self.shard_config.enable_flash_attention:
             self.append_or_create_method_replacement(
                 description={
-                    "forward": get_llama_flash_attention_forward(self.shard_config),
+                    "forward": get_mistral_flash_attention_forward(self.shard_config),
                 },
                 policy=policy,
-                target_key=LlamaAttention,
+                target_key=attn_cls,
             )
+            if self.pipeline_stage_manager is None:
+                # replace llama model forward method
+                self.append_or_create_method_replacement(
+                    description={
+                        "forward": get_mistral_model_forward_for_flash_attn(self.shard_config),
+                    },
+                    policy=policy,
+                    target_key=MistralModel,
+                )
 
         return policy
 
     def postprocess(self):
         return self.model
 
     def set_pipeline_forward(self, model_cls: nn.Module, new_forward: Callable, policy: Dict) -> None:
         """If under pipeline parallel setting, replacing the original forward method of huggingface
         to customized forward method, and add this changing to policy."""
         if self.pipeline_stage_manager is None:
             return
 
         stage_manager = self.pipeline_stage_manager
-        if self.model.__class__.__name__ == "LlamaModel":
+        if self.model.__class__.__name__ == "MistralModel":
             module = self.model
         else:
             module = self.model.model
 
         if stage_manager.is_interleave:
-            layers_per_stage = self.distribute_layers(
-                len(module.layers), stage_manager.num_stages * stage_manager.num_model_chunks
-            )
-            stage_manager.stage_indices = Policy.get_stage_index(
-                layers_per_stage,
-                stage_manager.stage,
-                num_model_chunks=stage_manager.num_model_chunks,
-                num_stages=stage_manager.num_stages,
-            )
+            layers_per_stage = stage_manager.distribute_layers(len(module.layers))
+            stage_manager.stage_indices = stage_manager.get_stage_index(layers_per_stage)
             method_replacement = {
                 "forward": partial(new_forward, stage_manager=stage_manager, shard_config=self.shard_config)
             }
 
         else:
-            layers_per_stage = Policy.distribute_layers(len(module.layers), stage_manager.num_stages)
-            stage_index = Policy.get_stage_index(layers_per_stage, stage_manager.stage)
+            layers_per_stage = stage_manager.distribute_layers(len(module.layers))
+            stage_index = stage_manager.get_stage_index(layers_per_stage)
             method_replacement = {
                 "forward": partial(
                     new_forward, stage_manager=stage_manager, stage_index=stage_index, shard_config=self.shard_config
                 )
             }
-            self.append_or_create_method_replacement(
-                description=method_replacement, policy=policy, target_key=model_cls
-            )
 
         self.append_or_create_method_replacement(description=method_replacement, policy=policy, target_key=model_cls)
 
     def get_held_layers(self) -> List[Module]:
         """Get pipeline layers for current stage."""
         assert self.pipeline_stage_manager is not None
 
-        if self.model.__class__.__name__ == "LlamaModel":
+        if self.model.__class__.__name__ == "MistralModel":
             module = self.model
         else:
             module = self.model.model
         stage_manager = self.pipeline_stage_manager
 
         held_layers = []
         if stage_manager.is_interleave:
             assert stage_manager.num_model_chunks is not None
-            layers_per_stage = self.distribute_layers(
-                len(module.layers), stage_manager.num_stages * stage_manager.num_model_chunks
-            )
-            stage_indices = Policy.get_stage_index(
-                layers_per_stage,
-                stage_manager.stage,
-                num_model_chunks=stage_manager.num_model_chunks,
-                num_stages=stage_manager.num_stages,
-            )
+            layers_per_stage = stage_manager.distribute_layers(len(module.layers))
+            stage_indices = stage_manager.get_stage_index(layers_per_stage)
             if stage_manager.is_first_stage(ignore_chunk=True):
                 held_layers.append(module.embed_tokens)
             for start_idx, end_idx in stage_indices:
                 held_layers.extend(module.layers[start_idx:end_idx])
             if stage_manager.is_last_stage(ignore_chunk=True):
                 held_layers.append(module.norm)
 
         else:
-            layers_per_stage = self.distribute_layers(len(module.layers), stage_manager.num_stages)
+            layers_per_stage = stage_manager.distribute_layers(len(module.layers))
             if stage_manager.is_first_stage():
                 held_layers.append(module.embed_tokens)
-            start_idx, end_idx = self.get_stage_index(layers_per_stage, stage_manager.stage)
+            start_idx, end_idx = stage_manager.get_stage_index(layers_per_stage)
             held_layers.extend(module.layers[start_idx:end_idx])
             if stage_manager.is_last_stage():
                 held_layers.append(module.norm)
-
         return held_layers
 
 
-class LlamaModelPolicy(LlamaPolicy):
+class MistralModelPolicy(MistralPolicy):
+    def __init__(self) -> None:
+        super().__init__()
+
     def module_policy(self):
         policy = super().module_policy()
-        from transformers.models.llama.modeling_llama import LlamaModel
+        from transformers.models.mistral.modeling_mistral import MistralModel
 
         if self.pipeline_stage_manager:
-            # set None as default
             self.set_pipeline_forward(
-                model_cls=LlamaModel, new_forward=LlamaPipelineForwards.llama_model_forward, policy=policy
+                model_cls=MistralModel, new_forward=MistralForwards.mistral_model_forward, policy=policy
             )
+
         return policy
 
     def get_held_layers(self) -> List[Module]:
         """Get pipeline layers for current stage."""
         held_layers = super().get_held_layers()
         return held_layers
 
     def get_shared_params(self) -> List[Dict[int, Tensor]]:
-        """No shared params in llama model"""
+        """No shared params in mistral model"""
         return []
 
 
-class LlamaForCausalLMPolicy(LlamaPolicy):
+class MistralForCausalLMPolicy(MistralPolicy):
     def module_policy(self):
-        from transformers import LlamaForCausalLM
+        from transformers import MistralForCausalLM
 
         policy = super().module_policy()
 
-        setattr(self.shard_config, "causal_lm", True)
-
         if self.shard_config.enable_tensor_parallelism:
             # add a new item for casual lm
             new_item = {
-                LlamaForCausalLM: ModulePolicyDescription(
+                MistralForCausalLM: ModulePolicyDescription(
                     sub_module_replacement=[
-                        SubModuleReplacementDescription(suffix="lm_head", target_module=Linear1D_Col)
-                    ],
-                    method_replacement={"forward": get_lm_forward_with_dist_cross_entropy(self.shard_config)},
+                        SubModuleReplacementDescription(
+                            suffix="lm_head",
+                            target_module=VocabParallelLMHead1D,
+                            kwargs=dict(
+                                gather_output=True,
+                                make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by,
+                            ),
+                        )
+                    ]
+                )
+            }
+        else:
+            new_item = {
+                MistralForCausalLM: ModulePolicyDescription(
+                    sub_module_replacement=[
+                        SubModuleReplacementDescription(
+                            suffix="lm_head",
+                            target_module=PaddingLMHead,
+                            kwargs=dict(make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by),
+                        )
+                    ]
                 )
             }
-            policy.update(new_item)
+
+        policy.update(new_item)
 
         if self.pipeline_stage_manager:
             # set None as default
             self.set_pipeline_forward(
-                model_cls=LlamaForCausalLM, new_forward=LlamaPipelineForwards.llama_for_causal_lm_forward, policy=policy
+                model_cls=MistralForCausalLM, new_forward=MistralForwards.mistral_for_causal_lm_forward, policy=policy
             )
 
         return policy
 
     def get_held_layers(self) -> List[Module]:
         """Get pipeline layers for current stage."""
         stage_manager = self.pipeline_stage_manager
         held_layers = super().get_held_layers()
         if stage_manager.is_last_stage(ignore_chunk=True):
             held_layers.append(self.model.lm_head)
         return held_layers
 
     def get_shared_params(self) -> List[Dict[int, Tensor]]:
-        llama_model = self.model.model
+        mistral_model = self.model.model
         if self.pipeline_stage_manager and self.pipeline_stage_manager.num_stages > 1:
             if (
-                id(llama_model.embed_tokens.weight) == id(self.model.lm_head.weight)
+                id(mistral_model.embed_tokens.weight) == id(self.model.lm_head.weight)
                 and self.pipeline_stage_manager.num_stages > 1
             ):
                 # tie weights
                 return [
                     {
-                        0: llama_model.embed_tokens.weight,
+                        0: mistral_model.embed_tokens.weight,
                         self.pipeline_stage_manager.num_stages - 1: self.model.lm_head.weight,
                     }
                 ]
         return []
 
 
-class LlamaForSequenceClassificationPolicy(LlamaPolicy):
+class MistralForSequenceClassificationPolicy(MistralPolicy):
     def module_policy(self):
-        from transformers import LlamaForSequenceClassification
+        from transformers import MistralForSequenceClassification
 
         policy = super().module_policy()
 
         if self.shard_config.enable_tensor_parallelism:
             # add a new item for sequence classification
             new_item = {
-                LlamaForSequenceClassification: ModulePolicyDescription(
+                MistralForSequenceClassification: ModulePolicyDescription(
                     sub_module_replacement=[
                         SubModuleReplacementDescription(
                             suffix="score", target_module=Linear1D_Col, kwargs=dict(gather_output=True)
                         )
                     ]
                 )
             }
             policy.update(new_item)
-        # to be confirmed
+
         if self.pipeline_stage_manager:
             # set None as default
             self.set_pipeline_forward(
-                model_cls=LlamaForSequenceClassification,
-                new_forward=LlamaPipelineForwards.llama_for_sequence_classification_forward,
+                model_cls=MistralForSequenceClassification,
+                new_forward=MistralForwards.mistral_for_sequence_classification_forward,
                 policy=policy,
             )
+
         return policy
 
     def get_held_layers(self) -> List[Module]:
         """Get pipeline layers for current stage."""
         stage_manager = self.pipeline_stage_manager
         held_layers = super().get_held_layers()
         if stage_manager.is_last_stage(ignore_chunk=True):
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/policies/opt.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/opt.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,93 +1,102 @@
 import warnings
 from functools import partial
 from typing import Callable, Dict, List
 
 import torch.nn as nn
 from torch import Tensor, nn
 
-from colossalai.shardformer.layer import FusedLayerNorm, LayerNorm, Linear1D_Col, Linear1D_Row, VocabParallelEmbedding1D
+from colossalai.shardformer.layer import (
+    FusedLayerNorm,
+    LayerNorm,
+    Linear1D_Col,
+    Linear1D_Row,
+    PaddingEmbedding,
+    PaddingLMHead,
+    VocabParallelEmbedding1D,
+    VocabParallelLMHead1D,
+)
 
 from .._utils import getattr_
 from ..modeling.jit import get_jit_fused_dropout_add_func
-from ..modeling.opt import OPTPipelineForwards, get_jit_fused_opt_decoder_layer_forward, get_opt_flash_attention_forward
+from ..modeling.opt import (
+    OPTPipelineForwards,
+    get_jit_fused_opt_decoder_layer_forward,
+    get_opt_decoder_forward_for_flash_attention,
+    get_opt_flash_attention_forward,
+)
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 __all__ = [
     "OPTPolicy",
     "OPTModelPolicy",
     "OPTForCausalLMPolicy",
     "OPTForSequenceClassificationPolicy",
     "OPTForQuestionAnsweringPolicy",
 ]
 
 
 class OPTPolicy(Policy):
     def __init__(self) -> None:
         super().__init__()
-        import transformers
-        from packaging.version import Version
-
-        assert Version(transformers.__version__) <= Version(
-            "4.33.0"
-        ), "The OPT model should run on a transformers version not greater than 4.33.0."
 
     def config_sanity_check(self):
         pass
 
     def preprocess(self):
-        # reshape the embedding layer
-        r"""
-        Reshape the Embedding layer to make the embedding dimension divisible by world_size
-        """
-        if self.shard_config.enable_tensor_parallelism:
-            vocab_size = self.model.config.vocab_size
-            world_size = self.shard_config.tensor_parallel_size
-            if vocab_size % world_size != 0:
-                new_vocab_size = vocab_size + world_size - vocab_size % world_size
-                self.model.resize_token_embeddings(new_vocab_size)
+        self.tie_weight = self.tie_weight_check()
+        self.origin_attn_implement = self.model.config._attn_implementation
         return self.model
 
     def module_policy(self):
-        from transformers.models.opt.modeling_opt import OPTAttention, OPTDecoder, OPTDecoderLayer
+        from transformers.models.opt.modeling_opt import OPTAttention, OPTDecoder, OPTDecoderLayer, OptFlashAttention2
+
+        ATTN_IMPLEMENTATION = {
+            "eager": OPTAttention,
+            "flash_attention_2": OptFlashAttention2,
+        }
 
         policy = {}
 
+        attn_cls = ATTN_IMPLEMENTATION[self.model.config._attn_implementation]
+
+        embedding_cls = None
+        if self.shard_config.enable_tensor_parallelism:
+            embedding_cls = VocabParallelEmbedding1D
+        else:
+            if self.tie_weight:
+                embedding_cls = PaddingEmbedding
+
         if self.shard_config.enable_fused_normalization:
             norm_cls = FusedLayerNorm
         else:
             norm_cls = LayerNorm
 
         if self.shard_config.enable_sequence_parallelism:
             self.shard_config.enable_sequence_parallelism = False
             warnings.warn("OPT doesn't support sequence parallelism now, will ignore the sequence parallelism flag.")
 
         if self.shard_config.enable_tensor_parallelism:
-            policy[OPTDecoder] = ModulePolicyDescription(
-                sub_module_replacement=[
-                    SubModuleReplacementDescription(
-                        suffix="embed_tokens",
-                        target_module=VocabParallelEmbedding1D,
-                    )
-                ]
-            )
+            assert (
+                self.model.config.num_attention_heads % self.shard_config.tensor_parallel_size == 0
+            ), f"The number of attention heads must be divisible by tensor parallel size."
             policy[OPTDecoderLayer] = ModulePolicyDescription(
                 sub_module_replacement=[
                     SubModuleReplacementDescription(
                         suffix="fc1",
                         target_module=Linear1D_Col,
                     ),
                     SubModuleReplacementDescription(
                         suffix="fc2",
                         target_module=Linear1D_Row,
                     ),
                 ]
             )
 
-            policy[OPTAttention] = ModulePolicyDescription(
+            policy[attn_cls] = ModulePolicyDescription(
                 attribute_replacement={
                     "embed_dim": self.model.config.hidden_size // self.shard_config.tensor_parallel_size,
                     "num_heads": self.model.config.num_attention_heads // self.shard_config.tensor_parallel_size,
                 },
                 sub_module_replacement=[
                     SubModuleReplacementDescription(
                         suffix="q_proj",
@@ -104,44 +113,69 @@
                     SubModuleReplacementDescription(
                         suffix="out_proj",
                         target_module=Linear1D_Row,
                     ),
                 ],
             )
 
+        if embedding_cls is not None:
+            self.append_or_create_submodule_replacement(
+                description=SubModuleReplacementDescription(
+                    suffix="embed_tokens",
+                    target_module=embedding_cls,
+                    kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
+                ),
+                policy=policy,
+                target_key=OPTDecoder,
+            )
+
         # optimization configuration
         self.append_or_create_submodule_replacement(
             description=SubModuleReplacementDescription(
-                suffix="final_layer_norm", target_module=norm_cls, ignore_if_not_exist=True
+                suffix="final_layer_norm",
+                target_module=norm_cls,
+                ignore_if_not_exist=True,
             ),
             policy=policy,
             target_key=OPTDecoder,
         )
         self.append_or_create_submodule_replacement(
             description=[
                 SubModuleReplacementDescription(
-                    suffix="self_attn_layer_norm", target_module=norm_cls, ignore_if_not_exist=True
+                    suffix="self_attn_layer_norm",
+                    target_module=norm_cls,
+                    ignore_if_not_exist=True,
                 ),
                 SubModuleReplacementDescription(
-                    suffix="final_layer_norm", target_module=norm_cls, ignore_if_not_exist=True
+                    suffix="final_layer_norm",
+                    target_module=norm_cls,
+                    ignore_if_not_exist=True,
                 ),
             ],
             policy=policy,
             target_key=OPTDecoderLayer,
         )
 
         # use flash attention
         if self.shard_config.enable_flash_attention:
             self.append_or_create_method_replacement(
                 description={
-                    "forward": get_opt_flash_attention_forward(),
+                    "forward": get_opt_flash_attention_forward(self.shard_config),
                 },
                 policy=policy,
-                target_key=OPTAttention,
+                target_key=attn_cls,
             )
+            if not self.shard_config.pipeline_stage_manager:
+                self.append_or_create_method_replacement(
+                    description={
+                        "forward": get_opt_decoder_forward_for_flash_attention(self.shard_config),
+                    },
+                    policy=policy,
+                    target_key=OPTDecoder,
+                )
 
         # use jit fused operator
         if self.shard_config.enable_jit_fused:
             self.append_or_create_method_replacement(
                 description={
                     "forward": get_jit_fused_opt_decoder_layer_forward(),
                     "dropout_add": get_jit_fused_dropout_add_func(),
@@ -162,20 +196,20 @@
         if self.model.__class__.__name__ == "OPTModel":
             module = self.model.decoder
         else:
             module = self.model.model.decoder
         stage_manager = self.pipeline_stage_manager
 
         held_layers = []
-        layers_per_stage = self.distribute_layers(len(module.layers), stage_manager.num_stages)
+        layers_per_stage = stage_manager.distribute_layers(len(module.layers))
         if stage_manager.is_first_stage():
             held_layers.append(module.embed_tokens)
             held_layers.append(module.embed_positions)
             held_layers.append(module.project_in)
-        start_idx, end_idx = self.get_stage_index(layers_per_stage, stage_manager.stage)
+        start_idx, end_idx = stage_manager.get_stage_index(layers_per_stage)
         held_layers.extend(module.layers[start_idx:end_idx])
         if stage_manager.is_last_stage():
             held_layers.append(module.final_layer_norm)
             held_layers.append(module.project_out)
         return held_layers
 
     def set_pipeline_forward(self, model_cls: nn.Module, new_forward: Callable, policy: Dict) -> None:
@@ -184,30 +218,39 @@
         if self.pipeline_stage_manager:
             stage_manager = self.pipeline_stage_manager
             if self.model.__class__.__name__ == "OPTModel":
                 module = self.model.decoder
             else:
                 module = self.model.model.decoder
 
-            layers_per_stage = Policy.distribute_layers(len(module.layers), stage_manager.num_stages)
-            stage_index = Policy.get_stage_index(layers_per_stage, stage_manager.stage)
-            method_replacement = {"forward": partial(new_forward, stage_manager=stage_manager, stage_index=stage_index)}
+            layers_per_stage = stage_manager.distribute_layers(len(module.layers))
+            stage_index = stage_manager.get_stage_index(layers_per_stage)
+            method_replacement = {
+                "forward": partial(
+                    new_forward,
+                    stage_manager=stage_manager,
+                    stage_index=stage_index,
+                    shard_config=self.shard_config,
+                )
+            }
             self.append_or_create_method_replacement(
                 description=method_replacement, policy=policy, target_key=model_cls
             )
 
 
 class OPTModelPolicy(OPTPolicy):
     def module_policy(self):
         from transformers.models.opt.modeling_opt import OPTModel
 
         policy = super().module_policy()
         if self.pipeline_stage_manager:
             self.set_pipeline_forward(
-                model_cls=OPTModel, new_forward=OPTPipelineForwards.opt_model_forward, policy=policy
+                model_cls=OPTModel,
+                new_forward=OPTPipelineForwards.opt_model_forward,
+                policy=policy,
             )
         return policy
 
     def get_held_layers(self) -> List[nn.Module]:
         return super().get_held_layers()
 
     def get_shared_params(self) -> List[Dict[int, Tensor]]:
@@ -219,22 +262,38 @@
     def module_policy(self):
         from transformers.models.opt.modeling_opt import OPTForCausalLM
 
         policy = super().module_policy()
         if self.shard_config.enable_tensor_parallelism:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
-                    suffix="lm_head", target_module=Linear1D_Col, kwargs=dict(gather_output=True)
+                    suffix="lm_head",
+                    target_module=VocabParallelLMHead1D,
+                    kwargs=dict(
+                        gather_output=True, make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by
+                    ),
+                ),
+                policy=policy,
+                target_key=OPTForCausalLM,
+            )
+        else:
+            self.append_or_create_submodule_replacement(
+                description=SubModuleReplacementDescription(
+                    suffix="lm_head",
+                    target_module=PaddingLMHead,
+                    kwargs=dict(make_vocab_size_divisible_by=self.shard_config.make_vocab_size_divisible_by),
                 ),
                 policy=policy,
                 target_key=OPTForCausalLM,
             )
         if self.pipeline_stage_manager:
             self.set_pipeline_forward(
-                model_cls=OPTForCausalLM, new_forward=OPTPipelineForwards.opt_for_causal_lm_forward, policy=policy
+                model_cls=OPTForCausalLM,
+                new_forward=OPTPipelineForwards.opt_for_causal_lm_forward,
+                policy=policy,
             )
 
         return policy
 
     def get_held_layers(self) -> List[nn.Module]:
         held_layers = super().get_held_layers()
         if self.pipeline_stage_manager.is_last_stage():
@@ -242,15 +301,20 @@
         return held_layers
 
     def get_shared_params(self) -> List[Dict[int, Tensor]]:
         opt_model = self.model
         if self.pipeline_stage_manager and self.pipeline_stage_manager.num_stages > 1:
             num_stages = self.pipeline_stage_manager.num_stages
             if id(opt_model.model.decoder.embed_tokens.weight) == id(opt_model.lm_head.weight):
-                return [{0: opt_model.model.decoder.embed_tokens.weight, num_stages - 1: opt_model.lm_head.weight}]
+                return [
+                    {
+                        0: opt_model.model.decoder.embed_tokens.weight,
+                        num_stages - 1: opt_model.lm_head.weight,
+                    }
+                ]
         return []
 
     def postprocess(self):
         if self.shard_config.enable_tensor_parallelism and self.pipeline_stage_manager is None:
             binding_map = {
                 "model.decoder.embed_tokens": "lm_head",
             }
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/policies/sam.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/sam.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,39 +1,43 @@
+import warnings
+
 import colossalai.shardformer.layer as col_nn
 
-from ..modeling.sam import forward_fn, get_sam_flash_attention_forward, get_sam_vision_flash_attention_forward
+from ..modeling.sam import forward_fn
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 __all__ = ["SamPolicy", "SamModelPolicy"]
 
 
 class SamPolicy(Policy):
     def config_sanity_check(self):
         pass
 
     def preprocess(self):
         return self.model
 
     def module_policy(self):
         from transformers.models.sam.modeling_sam import (
-            SamAttention,
             SamTwoWayAttentionBlock,
             SamTwoWayTransformer,
             SamVisionAttention,
             SamVisionLayer,
         )
 
         policy = {}
 
         if self.shard_config.enable_fused_normalization:
             norm_cls = col_nn.FusedLayerNorm
         else:
             norm_cls = col_nn.LayerNorm
 
         if self.shard_config.enable_tensor_parallelism:
+            assert (
+                self.model.config.vision_config.num_attention_heads % self.shard_config.tensor_parallel_size == 0
+            ), f"The number of attention heads must be divisible by tensor parallel size."
             policy[SamVisionLayer] = ModulePolicyDescription(
                 attribute_replacement={
                     "attn.num_attention_heads": self.model.config.vision_config.num_attention_heads
                     // self.shard_config.tensor_parallel_size,
                 },
                 sub_module_replacement=[
                     SubModuleReplacementDescription(
@@ -206,28 +210,29 @@
             ],
             policy=policy,
             target_key=SamTwoWayTransformer,
         )
 
         # use flash attention
         if self.shard_config.enable_flash_attention:
-            self.append_or_create_method_replacement(
-                description={
-                    "forward": get_sam_flash_attention_forward(),
-                },
-                policy=policy,
-                target_key=SamAttention,
-            )
-            self.append_or_create_method_replacement(
-                description={
-                    "forward": get_sam_vision_flash_attention_forward(),
-                },
-                policy=policy,
-                target_key=SamVisionAttention,
-            )
+            warnings.warn("Flash attention is not supported in SAM model. Fallback to normal attention.")
+            # self.append_or_create_method_replacement(
+            #     description={
+            #         "forward": get_sam_flash_attention_forward(),
+            #     },
+            #     policy=policy,
+            #     target_key=SamAttention,
+            # )
+            # self.append_or_create_method_replacement(
+            #     description={
+            #         "forward": get_sam_vision_flash_attention_forward(),
+            #     },
+            #     policy=policy,
+            #     target_key=SamVisionAttention,
+            # )
 
         return policy
 
     def postprocess(self):
         return self.model
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/policies/t5.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/t5.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,22 +1,27 @@
+from __future__ import annotations
+
 import warnings
 from functools import partial
 from typing import Callable, Dict, List, Tuple
 
 import numpy as np
 from torch import Tensor, nn
 
 from colossalai.shardformer.layer import (
     DropoutForParallelInput,
     Embedding1D,
     FusedRMSNorm,
     Linear1D_Col,
     Linear1D_Row,
+    PaddingEmbedding,
+    PaddingLMHead,
     RMSNorm,
     VocabParallelEmbedding1D,
+    VocabParallelLMHead1D,
 )
 from colossalai.shardformer.policies.base_policy import ModulePolicyDescription
 
 from ..modeling.jit import get_jit_fused_dropout_add_func
 from ..modeling.t5 import (
     T5PipelineForwards,
     get_jit_fused_T5_layer_ff_forward,
@@ -30,24 +35,15 @@
 
 
 class T5BasePolicy(Policy):
     def config_sanity_check(self):
         pass
 
     def preprocess(self):
-        # reshape the embedding layer
-        r"""
-        Reshape the Embedding layer to make the embedding dimension divisible by world_size
-        """
-        if self.shard_config.enable_tensor_parallelism:
-            vocab_size = self.model.config.vocab_size
-            world_size = self.shard_config.tensor_parallel_size
-            if vocab_size % world_size != 0:
-                new_vocab_size = vocab_size + world_size - vocab_size % world_size
-                self.model.resize_token_embeddings(new_vocab_size)
+        self.tie_weight = self.tie_weight_check()
         return self.model
 
     def module_policy(self):
         from transformers.models.t5.modeling_t5 import (
             T5Attention,
             T5DenseActDense,
             T5DenseGatedActDense,
@@ -55,34 +51,40 @@
             T5LayerFF,
             T5LayerSelfAttention,
             T5Stack,
         )
 
         policy = {}
 
+        embedding_cls = None
+        if self.shard_config.enable_tensor_parallelism:
+            embedding_cls = VocabParallelEmbedding1D
+        else:
+            if self.tie_weight:
+                embedding_cls = PaddingEmbedding
+
         if self.shard_config.enable_fused_normalization:
             norm_cls = FusedRMSNorm
         else:
             norm_cls = RMSNorm
 
         if self.shard_config.enable_sequence_parallelism:
             self.shard_config.enable_sequence_parallelism = False
             warnings.warn("T5 doesn't support sequence parallelism now, will ignore the sequence parallelism flag.")
 
         if self.shard_config.enable_tensor_parallelism:
+            assert (
+                self.model.config.num_heads % self.shard_config.tensor_parallel_size == 0
+            ), f"The number of attention heads must be divisible by tensor parallel size."
             policy[T5Stack] = ModulePolicyDescription(
                 sub_module_replacement=[
                     SubModuleReplacementDescription(
                         suffix="dropout",
                         target_module=DropoutForParallelInput,
                     ),
-                    SubModuleReplacementDescription(
-                        suffix="embed_tokens",
-                        target_module=VocabParallelEmbedding1D,
-                    ),
                 ]
             )
             policy[T5LayerSelfAttention] = ModulePolicyDescription(
                 sub_module_replacement=[
                     SubModuleReplacementDescription(
                         suffix="dropout",
                         target_module=DropoutForParallelInput,
@@ -170,14 +172,25 @@
                     SubModuleReplacementDescription(
                         suffix="dropout",
                         target_module=DropoutForParallelInput,
                     ),
                 ]
             )
 
+        if embedding_cls is not None:
+            self.append_or_create_submodule_replacement(
+                description=SubModuleReplacementDescription(
+                    suffix="embed_tokens",
+                    target_module=embedding_cls,
+                    kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
+                ),
+                policy=policy,
+                target_key=T5Stack,
+            )
+
         # optimization configuration
         self.append_or_create_submodule_replacement(
             description=SubModuleReplacementDescription(
                 suffix="layer_norm",
                 target_module=norm_cls,
             ),
             policy=policy,
@@ -237,81 +250,84 @@
             )
 
         return policy
 
     def postprocess(self):
         return self.model
 
-    @staticmethod
     def distribute_t5_layers(
-        num_encoder_layers: int, num_decoder_layers: int, num_stages: int
+        self, num_encoder_layers: int, num_decoder_layers: int, num_stages: int
     ) -> Tuple[List[int], int]:
         """
         Distribute t5 layers into stages when pipeline parallel is used.
         Return the layer distribution as a list and the starting stage of decoder.
         If decoder doesn't exist, returned decoder starting stage is set to num_encoder_layers.
         """
+        stage_manager = self.pipeline_stage_manager
+        assert stage_manager is not None, "Pipeline stage manager is not set."
 
         # number of encoder layers must be a positive integer
         if num_encoder_layers <= 0:
             raise ValueError("The number of encoder layers for T5 must be a positive integer.")
 
         # number of layers should be large enough to fill in every stage
         if num_encoder_layers + num_decoder_layers < num_stages:
             raise ValueError("The total number of layers can't be smaller than number of stages.")
 
         # in the case of T5EncoderModel, set decoder starting stage to num_stages since it doesn't exist
         if num_decoder_layers == 0:
-            return Policy.distribute_layers(num_encoder_layers, num_stages), num_stages
+            return stage_manager.distribute_layers(num_encoder_layers, num_stages), num_stages
 
         # the number of stages distributed between encoder and decoder is optimized in this way:
         # num_encoder_stages = argmin(abs(num_encoder_layers / encoder_stages - num_decoder_layers / decoder_stages))
         #                   s.t. num_encoder_stages + num_decoder_stages = num_stages, num_encoder_stages >= 1, num_decoder_stages >= 1
         def objective(num_encoder_stages):
             return abs(num_encoder_layers / num_encoder_stages - num_decoder_layers / (num_stages - num_encoder_stages))
 
         num_encoder_stages = np.argmin([objective(i) for i in range(1, num_stages)]) + 1
         num_decoder_stages = num_stages - num_encoder_stages
 
-        encoder_distribution = Policy.distribute_layers(num_encoder_layers, num_encoder_stages)
-        decoder_distribution = Policy.distribute_layers(num_decoder_layers, num_decoder_stages)
+        encoder_distribution = stage_manager.distribute_layers(num_encoder_layers, num_encoder_stages)
+        decoder_distribution = stage_manager.distribute_layers(num_decoder_layers, num_decoder_stages)
         return encoder_distribution + decoder_distribution, num_encoder_stages
 
-    @staticmethod
     def get_t5_stage_index(
-        layers_per_stage: List[int], stage: int, decoder_starting_stage: int
-    ) -> Tuple[bool, int, int]:
+        self, layers_per_stage: List[int], stage: int, decoder_starting_stage: int
+    ) -> Tuple[int, int]:
         """
         Input the distribution of layers among stages, the current stage and the first stage of decoder.
         Return the starting/ending idx of layers in encoder/decoder
         """
+        stage_manager = self.pipeline_stage_manager
+        assert stage_manager is not None, "Pipeline stage manager is not set."
+
         if stage < decoder_starting_stage:
-            return Policy.get_stage_index(layers_per_stage[:decoder_starting_stage], stage)
+            return stage_manager.get_stage_index(layers_per_stage[:decoder_starting_stage], stage)
         else:
-            return Policy.get_stage_index(layers_per_stage[decoder_starting_stage:], stage - decoder_starting_stage)
+            return stage_manager.get_stage_index(
+                layers_per_stage[decoder_starting_stage:], stage - decoder_starting_stage
+            )
 
     def get_held_layers(self) -> List[nn.Module]:
         """Get pipeline layers for current stage."""
         assert self.pipeline_stage_manager is not None
         stage_manager = self.pipeline_stage_manager
 
         model = self.model
         encoder = self.model.encoder
         decoder = getattr(self.model, "decoder", None)
 
         num_encoder_layers = len(encoder.block)
         num_decoder_layers = len(decoder.block) if decoder else 0
 
         held_layers = []
-        layers_per_stage, decoder_starting_stage = T5BasePolicy.distribute_t5_layers(
+        layers_per_stage, decoder_starting_stage = self.distribute_t5_layers(
             num_encoder_layers, num_decoder_layers, stage_manager.num_stages
         )
-        start_idx, end_idx = T5BasePolicy.get_t5_stage_index(
-            layers_per_stage, stage_manager.stage, decoder_starting_stage
-        )
+        start_idx, end_idx = self.get_t5_stage_index(layers_per_stage, stage_manager.stage, decoder_starting_stage)
 
         if stage_manager.stage < decoder_starting_stage:
             # current stage is in t5's encoder
             if stage_manager.is_first_stage():
                 held_layers.append(model.shared)
                 held_layers.append(encoder.embed_tokens)
                 held_layers.append(encoder.dropout)
@@ -339,18 +355,18 @@
 
         encoder = self.model.encoder
         decoder = getattr(self.model, "decoder", None)
 
         num_encoder_layers = len(encoder.block)
         num_decoder_layers = len(decoder.block) if decoder else 0
 
-        layers_per_stage, decoder_starting_stage = T5BasePolicy.distribute_t5_layers(
+        layers_per_stage, decoder_starting_stage = self.distribute_t5_layers(
             num_encoder_layers, num_decoder_layers, stage_manager.num_stages
         )
-        stage_index = T5BasePolicy.get_t5_stage_index(layers_per_stage, stage_manager.stage, decoder_starting_stage)
+        stage_index = self.get_t5_stage_index(layers_per_stage, stage_manager.stage, decoder_starting_stage)
 
         method_replacement = {
             "forward": partial(
                 new_forward,
                 stage_manager=stage_manager,
                 stage_index=stage_index,
                 decoder_starting_stage=decoder_starting_stage,
@@ -361,19 +377,27 @@
 
 class T5ModelPolicy(T5BasePolicy):
     def module_policy(self):
         from transformers import T5Model
 
         policy = super().module_policy()
 
+        embedding_cls = None
         if self.shard_config.enable_tensor_parallelism:
+            embedding_cls = VocabParallelEmbedding1D
+        else:
+            if self.tie_weight:
+                embedding_cls = PaddingEmbedding
+
+        if embedding_cls is not None:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
                     suffix="shared",
-                    target_module=VocabParallelEmbedding1D,
+                    target_module=embedding_cls,
+                    kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
                 ),
                 policy=policy,
                 target_key=T5Model,
             )
         if self.pipeline_stage_manager is not None:
             self.set_pipeline_forward(model_cls=T5Model, new_forward=T5PipelineForwards.t5_model_forward, policy=policy)
 
@@ -382,40 +406,67 @@
     def get_held_layers(self) -> List[nn.Module]:
         return super().get_held_layers()
 
     def get_shared_params(self) -> List[Dict[int, Tensor]]:
         module = self.model
         stage_manager = self.pipeline_stage_manager
         if stage_manager is not None and stage_manager.num_stages > 1:
-            _, decoder_starting_stage = T5BasePolicy.distribute_t5_layers(
+            _, decoder_starting_stage = self.distribute_t5_layers(
                 len(module.encoder.block), len(module.decoder.block), stage_manager.num_stages
             )
 
             if id(module.decoder.embed_tokens.weight) == id(module.shared.weight):
                 return [{0: module.shared.weight, decoder_starting_stage: module.decoder.embed_tokens.weight}]
         return []
 
 
 class T5ForConditionalGenerationPolicy(T5BasePolicy):
     def module_policy(self):
         from transformers import T5ForConditionalGeneration
 
         policy = super().module_policy()
 
+        embedding_cls = None
         if self.shard_config.enable_tensor_parallelism:
+            embedding_cls = VocabParallelEmbedding1D
+        else:
+            if self.tie_weight:
+                embedding_cls = PaddingEmbedding
+
+        if embedding_cls is not None:
             self.append_or_create_submodule_replacement(
-                description=[
-                    SubModuleReplacementDescription(
-                        suffix="shared",
-                        target_module=VocabParallelEmbedding1D,
-                    ),
-                    SubModuleReplacementDescription(
-                        suffix="lm_head", target_module=Linear1D_Col, kwargs=dict(gather_output=True)
-                    ),
-                ],
+                description=SubModuleReplacementDescription(
+                    suffix="shared",
+                    target_module=embedding_cls,
+                    kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
+                ),
+                policy=policy,
+                target_key=T5ForConditionalGeneration,
+            )
+
+        if self.shard_config.enable_tensor_parallelism:
+            self.append_or_create_submodule_replacement(
+                description=SubModuleReplacementDescription(
+                    suffix="lm_head",
+                    target_module=VocabParallelLMHead1D,
+                    kwargs={
+                        "gather_output": True,
+                        "make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by,
+                    },
+                ),
+                policy=policy,
+                target_key=T5ForConditionalGeneration,
+            )
+        else:
+            self.append_or_create_submodule_replacement(
+                description=SubModuleReplacementDescription(
+                    suffix="lm_head",
+                    target_module=PaddingLMHead,
+                    kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
+                ),
                 policy=policy,
                 target_key=T5ForConditionalGeneration,
             )
 
         if self.pipeline_stage_manager is not None:
             self.set_pipeline_forward(
                 model_cls=T5ForConditionalGeneration,
@@ -430,15 +481,15 @@
             held_layers.append(self.model.lm_head)
         return held_layers
 
     def get_shared_params(self) -> List[Dict[int, Tensor]]:
         module = self.model
         stage_manager = self.pipeline_stage_manager
         if stage_manager is not None and stage_manager.num_stages > 1:
-            _, decoder_starting_stage = T5BasePolicy.distribute_t5_layers(
+            _, decoder_starting_stage = self.distribute_t5_layers(
                 len(module.encoder.block), len(module.decoder.block), stage_manager.num_stages
             )
 
             shared_params = []
             shared_embedding = {}
             if id(module.decoder.embed_tokens.weight) == id(module.shared.weight):
                 shared_embedding[0] = module.shared.weight
@@ -458,19 +509,27 @@
 
 class T5EncoderPolicy(T5BasePolicy):
     def module_policy(self):
         from transformers import T5EncoderModel
 
         policy = super().module_policy()
 
+        embedding_cls = None
         if self.shard_config.enable_tensor_parallelism:
+            embedding_cls = VocabParallelEmbedding1D
+        else:
+            if self.tie_weight:
+                embedding_cls = PaddingEmbedding
+
+        if embedding_cls is not None:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
                     suffix="shared",
-                    target_module=VocabParallelEmbedding1D,
+                    target_module=embedding_cls,
+                    kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
                 ),
                 policy=policy,
                 target_key=T5EncoderModel,
             )
 
         if self.pipeline_stage_manager is not None:
             self.set_pipeline_forward(
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/policies/vit.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/vit.py`

 * *Files 4% similar despite different names*

```diff
@@ -7,39 +7,50 @@
 from colossalai.shardformer.layer import DropoutForReplicatedInput, Linear1D_Col
 
 from ..modeling.jit import get_jit_fused_dropout_add_func
 from ..modeling.vit import (
     ViTForImageClassification_pipeline_forward,
     ViTForMaskedImageModeling_pipeline_forward,
     ViTModel_pipeline_forward,
+    get_jit_fused_vit_intermediate_forward,
     get_jit_fused_vit_output_forward,
     get_vit_flash_self_attention_forward,
 )
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 __all__ = ["ViTPolicy", "ViTModelPolicy", "ViTForImageClassificationPolicy", "ViTForMaskedImageModelingPolicy"]
 
 
 class ViTPolicy(Policy):
     def config_sanity_check(self):
         pass
 
     def preprocess(self):
+        self.enable_bias_gelu_fused = self.shard_config.enable_jit_fused and self.model.config.hidden_act == "gelu"
         return self.model
 
     def module_policy(self) -> Dict[Union[str, nn.Module], ModulePolicyDescription]:
-        from transformers.models.vit.modeling_vit import ViTEmbeddings, ViTLayer, ViTOutput, ViTSelfAttention
+        from transformers.models.vit.modeling_vit import (
+            ViTEmbeddings,
+            ViTIntermediate,
+            ViTLayer,
+            ViTOutput,
+            ViTSelfAttention,
+        )
 
         policy = {}
 
         if self.shard_config.enable_sequence_parallelism:
             self.shard_config.enable_sequence_parallelism = False
             warnings.warn("Vit doesn't support sequence parallelism now, will ignore the sequence parallelism flag.")
 
         if self.shard_config.enable_tensor_parallelism:
+            assert (
+                self.model.config.num_attention_heads % self.shard_config.tensor_parallel_size == 0
+            ), f"The number of attention heads must be divisible by tensor parallel size."
             policy[ViTEmbeddings] = ModulePolicyDescription(
                 attribute_replacement={},
                 param_replacement=[],
                 sub_module_replacement=[
                     SubModuleReplacementDescription(
                         suffix="dropout",
                         target_module=DropoutForReplicatedInput,
@@ -79,25 +90,36 @@
                     SubModuleReplacementDescription(
                         suffix="attention.output.dropout",
                         target_module=col_nn.DropoutForReplicatedInput,
                     ),
                     SubModuleReplacementDescription(
                         suffix="intermediate.dense",
                         target_module=col_nn.Linear1D_Col,
+                        kwargs={
+                            "skip_bias_add": self.enable_bias_gelu_fused,
+                        },
                     ),
                     SubModuleReplacementDescription(
                         suffix="output.dense",
                         target_module=col_nn.Linear1D_Row,
                     ),
                     SubModuleReplacementDescription(
                         suffix="output.dropout",
                         target_module=col_nn.DropoutForReplicatedInput,
                     ),
                 ],
             )
+            if self.enable_bias_gelu_fused:
+                self.append_or_create_method_replacement(
+                    description={
+                        "forward": get_jit_fused_vit_intermediate_forward(),
+                    },
+                    policy=policy,
+                    target_key=ViTIntermediate,
+                )
 
         # use flash attention
         if self.shard_config.enable_flash_attention:
             self.append_or_create_method_replacement(
                 description={
                     "forward": get_vit_flash_self_attention_forward(),
                 },
@@ -111,14 +133,15 @@
                 description={
                     "forward": get_jit_fused_vit_output_forward(),
                     "dropout_add": get_jit_fused_dropout_add_func(),
                 },
                 policy=policy,
                 target_key=ViTOutput,
             )
+
         return policy
 
     def new_model_class(self):
         return None
 
     def postprocess(self):
         return self.model
@@ -130,31 +153,31 @@
         if self.model.__class__.__name__ == "ViTModel":
             module = self.model
         else:
             module = self.model.vit
         stage_manager = self.pipeline_stage_manager
 
         held_layers = []
-        layers_per_stage = self.distribute_layers(len(module.encoder.layer), stage_manager.num_stages)
+        layers_per_stage = stage_manager.distribute_layers(len(module.encoder.layer))
         if stage_manager.is_first_stage():
             held_layers.append(module.embeddings)
-        start_idx, end_idx = self.get_stage_index(layers_per_stage, stage_manager.stage)
+        start_idx, end_idx = stage_manager.get_stage_index(layers_per_stage)
         held_layers.extend(module.encoder.layer[start_idx:end_idx])
         return held_layers
 
     def set_pipeline_forward(self, model_cls: nn.Module, pipeline_forward: Callable, policy: Dict):
         if self.pipeline_stage_manager:
             stage_manager = self.pipeline_stage_manager
             if self.model.__class__.__name__ == "ViTModel":
                 module = self.model
             else:
                 module = self.model.vit
 
-            layers_per_stage = Policy.distribute_layers(len(module.encoder.layer), stage_manager.num_stages)
-            stage_index = Policy.get_stage_index(layers_per_stage, stage_manager.stage)
+            layers_per_stage = stage_manager.distribute_layers(len(module.encoder.layer))
+            stage_index = stage_manager.get_stage_index(layers_per_stage)
             method_replacement = {"forward": pipeline_forward(stage_manager=stage_manager, stage_index=stage_index)}
             self.append_or_create_method_replacement(
                 description=method_replacement, policy=policy, target_key=model_cls
             )
 
 
 # ViTModel
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/policies/whisper.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/policies/whisper.py`

 * *Files 12% similar despite different names*

```diff
@@ -9,14 +9,15 @@
 import colossalai.shardformer.layer as col_nn
 
 from ..modeling.jit import get_jit_fused_dropout_add_func
 from ..modeling.whisper import (
     WhisperPipelineForwards,
     get_jit_fused_whisper_decoder_layer_forward,
     get_jit_fused_whisper_encoder_layer_forward,
+    get_whisper_decoder_forward_for_flash_attention,
     get_whisper_flash_attention_forward,
 )
 from .base_policy import ModulePolicyDescription, Policy, SubModuleReplacementDescription
 
 __all__ = [
     "WhisperPolicy",
     "WhisperModelPolicy",
@@ -24,47 +25,46 @@
     "WhisperForAudioClassificationPolicy",
 ]
 
 
 class WhisperPolicy(Policy):
     def __init__(self) -> None:
         super().__init__()
-        import transformers
-        from packaging.version import Version
-
-        assert Version(transformers.__version__) <= Version(
-            "4.33.0"
-        ), "The Whisper model should run on a transformers version not greater than 4.33.0."
 
     def config_sanity_check(self):
         pass
 
     def preprocess(self):
         # reshape the embedding layer
         r"""
         Reshape the Embedding layer to make the embedding dimension divisible by world_size
         """
-        vocab_size = self.model.config.vocab_size
-        world_size = self.shard_config.tensor_parallel_size
-        if vocab_size % world_size != 0:
-            new_vocab_size = vocab_size + world_size - vocab_size % world_size
-            self.model.resize_token_embeddings(new_vocab_size)
+        self.tie_weight = self.tie_weight_check()
         return self.model
 
     def module_policy(self):
         from transformers.models.whisper.modeling_whisper import (
             WhisperAttention,
             WhisperDecoder,
             WhisperDecoderLayer,
             WhisperEncoder,
             WhisperEncoderLayer,
+            WhisperFlashAttention2,
+            WhisperSdpaAttention,
         )
 
         policy = {}
 
+        embedding_cls = None
+        if self.shard_config.enable_tensor_parallelism:
+            embedding_cls = col_nn.VocabParallelEmbedding1D
+        else:
+            if self.tie_weight:
+                embedding_cls = col_nn.PaddingEmbedding
+
         if self.shard_config.enable_fused_normalization:
             norm_cls = col_nn.FusedLayerNorm
         else:
             norm_cls = col_nn.LayerNorm
 
         if self.shard_config.enable_sequence_parallelism:
             self.shard_config.enable_sequence_parallelism = False
@@ -74,14 +74,17 @@
 
         # TODO using the jit fused add_and_dropout affect the accuracy
         if self.shard_config.enable_jit_fused:
             self.shard_config.enable_jit_fused = False
             warnings.warn("Whisper doesn't support jit fused operator now, will ignore the jit fused operator flag.")
 
         if self.shard_config.enable_tensor_parallelism:
+            assert (
+                self.model.config.encoder_attention_heads % self.shard_config.tensor_parallel_size == 0
+            ), f"The number of attention heads must be divisible by tensor parallel size."
             policy[WhisperEncoderLayer] = ModulePolicyDescription(
                 attribute_replacement={
                     "self_attn.embed_dim": self.model.config.d_model // self.shard_config.tensor_parallel_size,
                     "self_attn.num_heads": self.model.config.encoder_attention_heads
                     // self.shard_config.tensor_parallel_size,
                 },
                 sub_module_replacement=[
@@ -161,21 +164,25 @@
                     SubModuleReplacementDescription(
                         suffix="fc2",
                         target_module=col_nn.Linear1D_Row,
                     ),
                 ],
             )
 
-            policy[WhisperDecoder] = ModulePolicyDescription(
-                sub_module_replacement=[
+        if embedding_cls is not None:
+            self.append_or_create_submodule_replacement(
+                description=[
                     SubModuleReplacementDescription(
                         suffix="embed_tokens",
-                        target_module=col_nn.VocabParallelEmbedding1D,
+                        target_module=embedding_cls,
+                        kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
                     ),
-                ]
+                ],
+                policy=policy,
+                target_key=WhisperDecoder,
             )
 
         # optimization configuration
         # Handle encoder layer
         self.append_or_create_submodule_replacement(
             description=[
                 SubModuleReplacementDescription(
@@ -236,14 +243,36 @@
             self.append_or_create_method_replacement(
                 description={
                     "forward": get_whisper_flash_attention_forward(),
                 },
                 policy=policy,
                 target_key=WhisperAttention,
             )
+            self.append_or_create_method_replacement(
+                description={
+                    "forward": get_whisper_flash_attention_forward(),
+                },
+                policy=policy,
+                target_key=WhisperFlashAttention2,
+            )
+            self.append_or_create_method_replacement(
+                description={
+                    "forward": get_whisper_flash_attention_forward(),
+                },
+                policy=policy,
+                target_key=WhisperSdpaAttention,
+            )
+            if not self.shard_config.pipeline_stage_manager:
+                self.append_or_create_method_replacement(
+                    description={
+                        "forward": get_whisper_decoder_forward_for_flash_attention(self.shard_config),
+                    },
+                    policy=policy,
+                    target_key=WhisperDecoder,
+                )
 
         # use jit fused operator
         if self.shard_config.enable_jit_fused:
             self.append_or_create_method_replacement(
                 description={
                     "forward": get_jit_fused_whisper_decoder_layer_forward(),
                     "dropout_add": get_jit_fused_dropout_add_func(),
@@ -265,72 +294,93 @@
     def add_lm_head_policy(self, base_policy):
         from transformers.models.whisper.modeling_whisper import WhisperForConditionalGeneration
 
         # optimize for tensor parallelism
         if self.shard_config.enable_tensor_parallelism:
             self.append_or_create_submodule_replacement(
                 description=SubModuleReplacementDescription(
-                    suffix="proj_out", target_module=col_nn.Linear1D_Col, kwargs={"gather_output": True}
+                    suffix="proj_out",
+                    target_module=col_nn.VocabParallelLMHead1D,
+                    kwargs={
+                        "gather_output": True,
+                        "make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by,
+                    },
+                ),
+                policy=base_policy,
+                target_key=WhisperForConditionalGeneration,
+            )
+        else:
+            self.append_or_create_submodule_replacement(
+                description=SubModuleReplacementDescription(
+                    suffix="proj_out",
+                    target_module=col_nn.PaddingLMHead,
+                    kwargs={"make_vocab_size_divisible_by": self.shard_config.make_vocab_size_divisible_by},
                 ),
                 policy=base_policy,
                 target_key=WhisperForConditionalGeneration,
             )
 
         return base_policy
 
     def postprocess(self):
         return self.model
 
-    @staticmethod
     def distribute_whisper_layers(
-        num_encoder_layers: int, num_decoder_layers: int, num_stages: int
+        self, num_encoder_layers: int, num_decoder_layers: int, num_stages: int
     ) -> Tuple[List[int], int]:
         """
         Distribute whisper layers into stages when pipeline parallel is used.
         Return the layer distribution as a list and the starting stage of decoder.
         If decoder doesn't exist, returned decoder starting stage is set to num_encoder_layers.
         """
+        stage_manager = self.pipeline_stage_manager
+        assert stage_manager is not None, "pipeline_stage_manager is None"
 
         # number of encoder layers must be a positive integer
         if num_encoder_layers <= 0:
             raise ValueError("The number of encoder layers for whisper must be a positive integer.")
 
         # number of layers should be large enough to fill in every stage
         if num_encoder_layers + num_decoder_layers < num_stages:
             raise ValueError("The total number of layers can't be smaller than number of stages.")
 
         # in the case of whisperEncoderModel, set decoder starting stage to num_stages since it doesn't exist
         if num_decoder_layers == 0:
-            return Policy.distribute_layers(num_encoder_layers, num_stages), num_stages
+            return stage_manager.distribute_layers(num_encoder_layers, num_stages), num_stages
 
         # the number of stages distributed between encoder and decoder is optimized in this way:
         # num_encoder_stages = argmin(abs(num_encoder_layers / encoder_stages - num_decoder_layers / decoder_stages))
         #                   s.t. num_encoder_stages + num_decoder_stages = num_stages, num_encoder_stages >= 1, num_decoder_stages >= 1
         def objective(num_encoder_stages):
             return abs(num_encoder_layers / num_encoder_stages - num_decoder_layers / (num_stages - num_encoder_stages))
 
         num_encoder_stages = np.argmin([objective(i) for i in range(1, num_stages)]) + 1
         num_decoder_stages = num_stages - num_encoder_stages
 
-        encoder_distribution = Policy.distribute_layers(num_encoder_layers, num_encoder_stages)
-        decoder_distribution = Policy.distribute_layers(num_decoder_layers, num_decoder_stages)
+        encoder_distribution = stage_manager.distribute_layers(num_encoder_layers, num_encoder_stages)
+        decoder_distribution = stage_manager.distribute_layers(num_decoder_layers, num_decoder_stages)
         return encoder_distribution + decoder_distribution, num_encoder_stages
 
-    @staticmethod
     def get_whisper_stage_index(
-        layers_per_stage: List[int], stage: int, decoder_starting_stage: int
-    ) -> Tuple[bool, int, int]:
+        self, layers_per_stage: List[int], stage: int, decoder_starting_stage: int
+    ) -> Tuple[int, int]:
         """
         Input the distribution of layers among stages, the current stage and the first stage of decoder.
         Return the starting/ending idx of layers in encoder/decoder
         """
+        stage_manager = self.pipeline_stage_manager
+        assert stage_manager is not None, "pipeline_stage_manager is None"
+
         if stage < decoder_starting_stage:
-            return Policy.get_stage_index(layers_per_stage[:decoder_starting_stage], stage)
+            return stage_manager.get_stage_index(layers_per_stage[:decoder_starting_stage], stage)
         else:
-            return Policy.get_stage_index(layers_per_stage[decoder_starting_stage:], stage - decoder_starting_stage)
+            return stage_manager.get_stage_index(
+                layers_per_stage[decoder_starting_stage:],
+                stage - decoder_starting_stage,
+            )
 
     def get_held_layers(self) -> List[nn.Module]:
         assert self.pipeline_stage_manager is not None, "pipeline_stage_manager is None"
         stage_manager = self.pipeline_stage_manager
 
         if self.model.__class__.__name__ == "WhisperModel":
             model = self.model
@@ -350,20 +400,18 @@
         num_encoder_layers = len(encoder.layers)
         if decoder:
             num_decoder_layers = len(decoder.layers)
         else:
             num_decoder_layers = 0
 
         held_layers = []
-        layers_per_stage, decoder_starting_stage = WhisperPolicy.distribute_whisper_layers(
+        layers_per_stage, decoder_starting_stage = self.distribute_whisper_layers(
             num_encoder_layers, num_decoder_layers, stage_manager.num_stages
         )
-        start_idx, end_idx = WhisperPolicy.get_whisper_stage_index(
-            layers_per_stage, stage_manager.stage, decoder_starting_stage
-        )
+        start_idx, end_idx = self.get_whisper_stage_index(layers_per_stage, stage_manager.stage, decoder_starting_stage)
 
         if stage_manager.stage < decoder_starting_stage:
             # current stage is in whisper's encoder
             if stage_manager.is_first_stage():
                 held_layers.append(encoder.embed_positions)
                 held_layers.append(encoder.conv1)
                 held_layers.append(encoder.conv2)
@@ -405,42 +453,43 @@
 
         num_encoder_layers = len(encoder.layers)
         if decoder:
             num_decoder_layers = len(decoder.layers)
         else:
             num_decoder_layers = 0
 
-        layers_per_stage, decoder_starting_stage = WhisperPolicy.distribute_whisper_layers(
+        layers_per_stage, decoder_starting_stage = self.distribute_whisper_layers(
             num_encoder_layers, num_decoder_layers, stage_manager.num_stages
         )
-        stage_index = WhisperPolicy.get_whisper_stage_index(
-            layers_per_stage, stage_manager.stage, decoder_starting_stage
-        )
+        stage_index = self.get_whisper_stage_index(layers_per_stage, stage_manager.stage, decoder_starting_stage)
 
         method_replacement = {
             "forward": partial(
                 new_forward,
                 stage_manager=stage_manager,
                 stage_index=stage_index,
                 decoder_starting_stage=decoder_starting_stage,
+                shard_config=self.shard_config,
             )
         }
         self.append_or_create_method_replacement(description=method_replacement, policy=policy, target_key=model_cls)
 
 
 # WhisperModel
 class WhisperModelPolicy(WhisperPolicy):
     def module_policy(self):
         from transformers import WhisperModel
 
         policy = super().module_policy()
 
         if self.pipeline_stage_manager is not None:
             self.set_pipeline_forward(
-                model_cls=WhisperModel, new_forward=WhisperPipelineForwards.whisper_model_forward, policy=policy
+                model_cls=WhisperModel,
+                new_forward=WhisperPipelineForwards.whisper_model_forward,
+                policy=policy,
             )
 
         return policy
 
     def get_held_layers(self) -> List[nn.Module]:
         return super().get_held_layers()
 
@@ -489,15 +538,15 @@
         if decoder:
             num_decoder_layers = len(decoder.layers)
         else:
             num_decoder_layers = 0
 
         stage_manager = self.pipeline_stage_manager
         if stage_manager is not None and stage_manager.num_stages > 1:
-            _, decoder_starting_stage = WhisperPolicy.distribute_whisper_layers(
+            _, decoder_starting_stage = self.distribute_whisper_layers(
                 num_encoder_layers, num_decoder_layers, stage_manager.num_stages
             )
             shared_params = []
             shared_embedding = {}
             if id(module.proj_out) == id(model.decoder.embed_tokens):
                 shared_embedding[decoder_starting_stage] = model.decoder.embed_tokens
                 shared_embedding[stage_manager.num_stages - 1] = module.proj_out
@@ -505,17 +554,14 @@
                 shared_params.append(shared_embedding)
             return shared_params
         return []
 
 
 # WhisperForAudioClassification
 class WhisperForAudioClassificationPolicy(WhisperPolicy):
-    def preprocess(self):
-        return self.model
-
     def module_policy(self):
         from transformers import WhisperForAudioClassification
 
         policy = super().module_policy()
 
         if self.pipeline_stage_manager is not None:
             self.set_pipeline_forward(
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/shard/sharder.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/shard/sharder.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/shard/shardformer.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/shard/shardformer.py`

 * *Files 6% similar despite different names*

```diff
@@ -22,15 +22,15 @@
 
     ```python
     from colossalai.shardformer import ShardFormer, ShardConfig
     from transformers import BertForMaskedLM
     import colossalai
     import torch
 
-    colossalai.launch_from_torch(config={})
+    colossalai.launch_from_torch()
 
     org_model = BertForMaskedLM.from_pretrained('bert-base-uncased')
     shard_config = ShardConfig()
     shard_former = ShardFormer(shard_config=shard_config)
     model, shared_params = shard_former.optimize(org_model)
     ```
     """
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/shardformer/shard/utils.py` & `colossalai-nightly-2024.5.4/colossalai/shardformer/shard/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/tensor/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/tensor/colo_parameter.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/colo_parameter.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/tensor/colo_tensor.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/colo_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/tensor/d_tensor/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/__init__.py`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 from .api import (
     compute_global_numel,
     customized_distributed_tensor_to_param,
     distribute_tensor,
-    init_as_dtensor,
     distribute_tensor_with_customization,
-    init_tensor_as_customization_distributed,
     get_device_mesh,
     get_global_shape,
     get_layout,
     get_sharding_spec,
+    init_as_dtensor,
+    init_tensor_as_customization_distributed,
     is_customized_distributed_tensor,
     is_distributed_tensor,
     is_sharded,
     redistribute,
     shard_colwise,
     shard_rowwise,
     sharded_tensor_to_param,
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/tensor/d_tensor/api.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -124,26 +124,30 @@
     sharded_tensor = _apply_layout(tensor, dist_layout)
 
     # hack some tensor methods
     _hijack_detach_and_clone(sharded_tensor)
 
     return sharded_tensor
 
-def init_as_dtensor(tensor: torch.Tensor, device_mesh: DeviceMesh, sharding_spec: ShardingSpec, global_shape: torch.Size) -> torch.Tensor:
+
+def init_as_dtensor(
+    tensor: torch.Tensor, device_mesh: DeviceMesh, sharding_spec: ShardingSpec, global_shape: torch.Size
+) -> torch.Tensor:
     assert not is_distributed_tensor(tensor), "The input tensor is already a distributed tensor."
     dist_layout = Layout(device_mesh=device_mesh, sharding_spec=sharding_spec, global_shape=global_shape)
 
     # shard tensor
     tensor.dist_layout = dist_layout
 
     # hack some tensor methods
     _hijack_detach_and_clone(tensor)
 
     return tensor
 
+
 def redistribute(dtensor: torch.Tensor, device_mesh: DeviceMesh, sharding_spec: ShardingSpec) -> None:
     """
     Convert the layout of the tensor from source_spec to target_spec.
     This will update the `local_tensor` and `dist_layout` in place.
 
     Args:
         dtensor (torch.Tensor): the distributed tensor to be converted.
@@ -464,15 +468,14 @@
     Returns:
         torch.Tensor: The distributed tensor.
     """
     assert callable(shard_fn), "The shard_fn must be callable."
     assert callable(gather_fn), "The gather_fn must be callable."
     assert not is_distributed_tensor(tensor), "The input tensor is already a distributed tensor."
 
-
     # set the shard_fn and gather_fn as attributes of the distributed tensor
     tensor.shard_fn = shard_fn
     tensor.gather_fn = gather_fn
 
     # set the shard_fn and gather_fn as attributes of the distributed tensor
     _hijack_detach_and_clone_for_customized_distributed_tensor(tensor)
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/tensor/d_tensor/comm_spec.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/comm_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/tensor/d_tensor/layout.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/layout.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/tensor/d_tensor/layout_converter.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/layout_converter.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,14 +6,15 @@
 import torch
 import torch.distributed as dist
 
 from colossalai.context.singleton_meta import SingletonMeta
 from colossalai.tensor.d_tensor.comm_spec import *
 from colossalai.tensor.d_tensor.layout import Layout
 from colossalai.tensor.d_tensor.misc import LayoutException
+from colossalai.tensor.padded_tensor.api import init_as_padded_tensor, is_padded_tensor
 from colossalai.tensor.utils import all_gather_simulator, all_to_all_simulator, shard_simulator
 
 from .sharding_spec import ShardingSpec
 from .utils import get_comm_cost
 
 __all__ = ["LayoutConverter", "LayoutConverterOptions", "set_layout_converting_options"]
 
@@ -436,15 +437,18 @@
         """
         source_spec = source_layout.sharding_spec
         target_spec = target_layout.sharding_spec
         MAX_TRANSFORM_STEPS = 20
         total_steps = 0
         transform_path = []
         comm_action_sequence: List[CommSpec] = []
-        spec_pairs = (str(source_spec.sharding_sequence), str(target_spec.sharding_sequence))
+
+        src_shape = source_layout.get_sharded_shape_per_device()
+        dst_shape = target_layout.get_sharded_shape_per_device()
+        spec_pairs = ((str(source_spec.sharding_sequence), src_shape), (str(target_spec.sharding_sequence), dst_shape))
 
         if spec_pairs in self.cached_solution:
             # Solution Cache hit
 
             def _group_alive_check(cached_comm_action_sequence):
                 r"""
                 Check if the process groups required for sharding have been deleted by torch.distributed.destroy_process_group method.
@@ -600,12 +604,22 @@
 
         Output in rank2 and rank3:
             tensor([[1.],
                     [1.],
                     [3.],
                     [3.]])
         """
+
         _, comm_action_sequence = self.layout_converting(source_layout, target_layout)
+
+        target_tensor = tensor
         for comm_spec in comm_action_sequence:
-            tensor = comm_spec.covert_spec_to_action(tensor)
-        tensor.dist_layout = target_layout
-        return tensor
+            target_tensor = comm_spec.covert_spec_to_action(target_tensor)
+        target_tensor.dist_layout = target_layout
+
+        # restore the padding information
+        if is_padded_tensor(tensor) and not is_padded_tensor(target_tensor):
+            target_tensor = init_as_padded_tensor(
+                target_tensor, tensor._current_length, tensor._origin_length, tensor._padding_dim
+            )
+
+        return target_tensor
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/tensor/d_tensor/sharding_spec.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/sharding_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/tensor/d_tensor/utils.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/d_tensor/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/tensor/moe_tensor/api.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/api.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/tensor/moe_tensor/moe_info.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/moe_tensor/moe_info.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/tensor/param_op_hook.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/param_op_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/tensor/shape_consistency.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/shape_consistency.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/tensor/sharding_spec.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/sharding_spec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/tensor/utils.py` & `colossalai-nightly-2024.5.4/colossalai/tensor/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/testing/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/testing/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/testing/comparison.py` & `colossalai-nightly-2024.5.4/colossalai/testing/comparison.py`

 * *Files 4% similar despite different names*

```diff
@@ -19,15 +19,15 @@
 def assert_close_loose(a: Tensor, b: Tensor, rtol: float = 1e-3, atol: float = 1e-3):
     assert_close(
         a,
         b,
         rtol=rtol,
         atol=atol,
         msg=f"Tensor not close, shape: {a.shape} vs {b.shape}, \
-                                                   dtype: {a.dtype} vs {b.dtype}",
+                                                dtype: {a.dtype} vs {b.dtype}",
     )
 
 
 def assert_equal_in_group(tensor: Tensor, process_group: ProcessGroup = None):
     # all gather tensors from different ranks
     world_size = dist.get_world_size(process_group)
     tensor_list = [torch.empty_like(tensor) for _ in range(world_size)]
@@ -36,15 +36,20 @@
     # check if they are equal one by one
     for i in range(world_size - 1):
         a = tensor_list[i]
         b = tensor_list[i + 1]
         assert torch.all(a == b), f"expected tensors on rank {i} and {i + 1} to be equal but they are not, {a} vs {b}"
 
 
-def check_state_dict_equal(d1: OrderedDict, d2: OrderedDict, ignore_device: bool = True, ignore_dtype: bool = False):
+def check_state_dict_equal(
+    d1: OrderedDict,
+    d2: OrderedDict,
+    ignore_device: bool = True,
+    ignore_dtype: bool = False,
+):
     assert len(list(d1.keys())) == len(
         list(d2.keys())
     ), f"Number of keys unequal: {len(list(d1.keys()))} vs {len(list(d2.keys()))}"
     for k, v1 in d1.items():
         assert k in d2
         v2 = d2[k]
         if isinstance(v1, dict):
@@ -90,15 +95,20 @@
                 v2 = v2.to("cpu")
             assert_close_loose(v1, v2)
         else:
             assert v1 == v2, f"{v1} not equals to {v2}"
 
 
 def assert_hf_output_close(
-    out1: Any, out2: Any, ignore_keys: List[str] = None, track_name: str = "", atol=1e-5, rtol=1e-5
+    out1: Any,
+    out2: Any,
+    ignore_keys: List[str] = None,
+    track_name: str = "",
+    atol=1e-5,
+    rtol=1e-5,
 ):
     """
     Check if two outputs from huggingface are equal.
 
     Args:
         out1 (Any): the first output
         out2 (Any): the second output
@@ -109,25 +119,35 @@
         # if two values are dict
         # we recursively check the keys
         assert set(out1.keys()) == set(out2.keys())
         for k in out1.keys():
             if ignore_keys is not None and k in ignore_keys:
                 continue
             assert_hf_output_close(
-                out1[k], out2[k], track_name=f"{track_name}.{k}", ignore_keys=ignore_keys, atol=atol, rtol=rtol
+                out1[k],
+                out2[k],
+                track_name=f"{track_name}.{k}",
+                ignore_keys=ignore_keys,
+                atol=atol,
+                rtol=rtol,
             )
     elif isinstance(out1, (list, tuple)) and isinstance(out2, (list, tuple)):
         # if two values are list
         # we recursively check the elements
         assert len(out1) == len(out2)
         for i in range(len(out1)):
             assert_hf_output_close(
-                out1[i], out2[i], track_name=f"{track_name}.{i}", ignore_keys=ignore_keys, atol=atol, rtol=rtol
+                out1[i],
+                out2[i],
+                track_name=f"{track_name}.{i}",
+                ignore_keys=ignore_keys,
+                atol=atol,
+                rtol=rtol,
             )
     elif isinstance(out1, Tensor) and isinstance(out2, Tensor):
         if out1.shape != out2.shape:
             raise AssertionError(f"{track_name}: shape mismatch: {out1.shape} vs {out2.shape}")
-        assert torch.allclose(
+        assert_close(
             out1, out2, atol=atol, rtol=rtol
         ), f"{track_name}: tensor value mismatch\nvalue 1: {out1}\nvalue 2: {out2}, \nmean error: {torch.abs(out1 - out2).mean()}"
     else:
         assert out1 == out2, f"{track_name}: value mismatch.\nout1: {out1}\nout2: {out2}"
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/testing/pytest_wrapper.py` & `colossalai-nightly-2024.5.4/colossalai/testing/pytest_wrapper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/testing/random.py` & `colossalai-nightly-2024.5.4/colossalai/testing/random.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/testing/utils.py` & `colossalai-nightly-2024.5.4/colossalai/testing/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/utils/__init__.py` & `colossalai-nightly-2024.5.4/colossalai/utils/__init__.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/utils/common.py` & `colossalai-nightly-2024.5.4/colossalai/utils/common.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/utils/memory.py` & `colossalai-nightly-2024.5.4/colossalai/utils/memory.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/utils/model/utils.py` & `colossalai-nightly-2024.5.4/colossalai/utils/model/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/utils/multi_tensor_apply/multi_tensor_apply.py` & `colossalai-nightly-2024.5.4/colossalai/utils/multi_tensor_apply/multi_tensor_apply.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/utils/rank_recorder/rank_recorder.py` & `colossalai-nightly-2024.5.4/colossalai/utils/rank_recorder/rank_recorder.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/utils/tensor_detector/tensor_detector.py` & `colossalai-nightly-2024.5.4/colossalai/utils/tensor_detector/tensor_detector.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/utils/timer.py` & `colossalai-nightly-2024.5.4/colossalai/utils/timer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/chunk/chunk.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/chunk.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/chunk/manager.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/manager.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/chunk/search_utils.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/search_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/chunk/utils.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/chunk/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/gemini_ddp.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_ddp.py`

 * *Files 6% similar despite different names*

```diff
@@ -23,14 +23,20 @@
     get_global_shape,
     get_sharding_spec,
     init_as_dtensor,
     init_tensor_as_customization_distributed,
     is_customized_distributed_tensor,
     is_distributed_tensor,
 )
+from colossalai.tensor.padded_tensor import (
+    init_as_padded_tensor,
+    is_padded_tensor,
+    to_padded_tensor,
+    to_unpadded_tensor,
+)
 from colossalai.tensor.param_op_hook import ColoParamOpHookManager
 from colossalai.utils import _cast_float, free_storage, is_ddp_ignored
 
 from .chunk import Chunk, ChunkManager, TensorState, init_chunk_manager
 from .gemini_hook import GeminiZeROHook
 from .gemini_mgr import GeminiManager
 from .memory_tracer import MemStats, OrderedParamGenerator
@@ -456,14 +462,19 @@
                         record_tensor, device_mesh=device_mesh, sharding_spec=shard_spec, global_shape=global_shape
                     )
                 elif is_customized_distributed_tensor(tensor):
                     init_tensor_as_customization_distributed(
                         record_tensor, shard_fn=tensor.shard_fn, gather_fn=tensor.gather_fn
                     )
                 record_tensor = gather_distributed_param(record_tensor, keep_vars=False).cpu()
+                if is_padded_tensor(tensor):
+                    record_tensor = init_as_padded_tensor(
+                        record_tensor, tensor._current_length, tensor._origin_length, tensor._padding_dim
+                    )
+                    record_tensor = to_unpadded_tensor(record_tensor)
 
             assert tensor not in chunk_to_save_data
             chunk_to_save_data[tensor] = record_tensor
 
         del temp_chunk
         return chunk_to_save_data
 
@@ -516,14 +527,16 @@
             p_mapping = param_to_save_data
         for name, param in self.name2param.items():
             if param is not None:
                 if is_ddp_ignored(param):
                     # deal with ddp ignored parameters
                     destination[prefix + name] = param if keep_vars else param.detach()
                 else:
+                    if is_padded_tensor(p_mapping[param]):
+                        p_mapping[param] = to_unpadded_tensor(p_mapping[param])
                     destination[prefix + name] = p_mapping[param]
         del p_mapping
         del param_to_save_data
 
         # save all buffers
         for name, buf in self.named_buffers():
             if buf is not None and name not in self._non_persistent_buffers_set:
@@ -623,14 +636,15 @@
                 this list
             unexpected_keys (list of str): if ``strict=True``, add unexpected
                 keys to this list
             error_msgs (list of str): error messages should be added to this
                 list, and will be reported together in
                 :meth:`~torch.nn.Module.load_state_dict`
         """
+
         for hook in self._load_state_dict_pre_hooks.values():
             hook(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
 
         persistent_buffers = {k: v for k, v in self.named_buffers() if k not in self._non_persistent_buffers_set}
         local_name_params = itertools.chain(self.named_parameters(), persistent_buffers.items())
         local_state = {k: v for k, v in local_name_params if v is not None}
 
@@ -643,14 +657,22 @@
             shard_fn=None,
             gather_fn=None,
         ):
             state_key = prefix + param_name
             if state_key in state_dict:
                 input_param = state_dict[state_key]
 
+                global_shape = dest_tensor.shape
+                if source_device_mesh is not None and source_sharding_spec is not None:
+                    global_shape = get_global_shape(dest_tensor)
+
+                if is_padded_tensor(dest_tensor):
+                    padding_dim = dest_tensor._padding_dim
+                    input_param = to_padded_tensor(input_param, global_shape[padding_dim], padding_dim)
+
                 if source_device_mesh is not None and source_sharding_spec is not None:
                     input_param = distribute_tensor(input_param, source_device_mesh, source_sharding_spec)
                 elif shard_fn is not None and gather_fn is not None:
                     input_param = distribute_tensor_with_customization(
                         input_param, shard_fn=shard_fn, gather_fn=gather_fn
                     )
 
@@ -814,14 +836,15 @@
             if chunk_32.device_type != self.grads_device[p].type:
                 self.chunk_manager.move_chunk(chunk_32, self.grads_device[p])
 
     def _cast_buffers(self):
         for buffer in self.module.buffers():
             if isinstance(buffer, LazyTensor):
                 buffer.materialize()
+        for buffer in self.module.buffers():
             buffer.data = buffer.to(get_accelerator().get_current_device())
             if torch.is_floating_point(buffer):
                 buffer.data = buffer.to(self.mixed_precision)
 
     def _preprocess_param(self, p: Union[nn.Parameter, ColoParameter, "LazyTensor"]) -> None:
         """Convert parameter to ColoParameter in-place.
         Args:
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/gemini_hook.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_hook.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/gemini_mgr.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_mgr.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/gemini_optimizer.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/gemini_optimizer.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,20 +17,27 @@
 from colossalai.interface import OptimizerWrapper
 from colossalai.logging import get_dist_logger
 from colossalai.nn.optimizer import CPUAdam, FusedAdam, HybridAdam
 from colossalai.tensor.d_tensor import (
     distribute_tensor,
     distribute_tensor_with_customization,
     get_device_mesh,
+    get_global_shape,
     get_sharding_spec,
     init_as_dtensor,
     init_tensor_as_customization_distributed,
     is_customized_distributed_tensor,
     is_distributed_tensor,
 )
+from colossalai.tensor.padded_tensor import (
+    init_as_padded_tensor,
+    is_padded_tensor,
+    to_padded_tensor,
+    to_unpadded_tensor,
+)
 from colossalai.utils import disposable, is_ddp_ignored
 
 from .chunk import Chunk, ChunkManager
 from .gemini_ddp import GeminiDDP
 
 __all__ = ["GeminiOptimizer", "GeminiAdamOptimizer"]
 
@@ -102,15 +109,15 @@
         backoff_factor: float = 0.5,
         growth_interval: int = 1000,
         hysteresis: int = 2,
         max_scale: float = 2**32,
         max_norm: float = 0.0,
         norm_type: float = 2.0,
         tp_group: ProcessGroup = None,
-        optimizer_params_info=None,
+        params_info=None,
         verbose: bool = False,
         **defaults: Any,
     ):
         super().__init__(optim)
         assert isinstance(module, GeminiDDP)
         assert type(optim) in _AVAIL_OPTIM_LIST, (
             "You should use an optimizer in the available list:\n" f"{_AVAIL_OPTIM_LIST}"
@@ -120,15 +127,15 @@
         self.chunk_manager: ChunkManager = self.gemini_manager.chunk_manager
         self.param_to_range: Dict[Parameter, Tuple[int, int]] = dict()
         self.param_to_chunk16: Dict[Parameter, Chunk] = dict()
         self.chunk16_set: Set[Chunk] = set()
         self.clipping_flag = max_norm > 0.0
         self.max_norm = max_norm
         self.tp_group = tp_group
-        self.optimizer_params_info = optimizer_params_info
+        self.params_info = params_info
         self.tp_size = dist.get_world_size(tp_group) if tp_group is not None else 1
         self.tp_rank = dist.get_rank(tp_group) if tp_group is not None else 0
         self.verbose = verbose
         self.param_groups_backup = list()
 
         # Mapping from integer id to real/fake param tensor, used for checkpointing.
         self.id_to_real_params: Dict[int, Parameter] = dict()
@@ -455,15 +462,15 @@
         is_collector = (rank == master_rank) or (not only_rank_0)
 
         # get tensor parallelism information
         is_dtensor = is_distributed_tensor(param)
         is_customized_distributed = is_customized_distributed_tensor(param)
         shard_spec = get_sharding_spec(param) if is_dtensor else None
         device_mesh = get_device_mesh(param) if is_dtensor else None
-        global_shape = self.optimizer_params_info["id2shape"][param_id]
+        global_shape = self.params_info["id2shape"][param_id]
 
         # If the chunk is kept gathered,
         # the parameters are treated the same as that of those in strict DDP during training.
         # So states can be directly fetched from current device.
         if chunk.keep_gathered:
             assert param_id in self.id_to_fake_params
             if is_collector:
@@ -473,29 +480,35 @@
                         # To keep aligned with pytorch, state 'step' is stored as a pytorch tensor with type float32.
                         collected_states[state_name] = torch.tensor(
                             states["step"], dtype=torch.float32, requires_grad=False
                         ).cpu()
                     else:
                         state_tensor = states[state_name].detach().clone().to(torch.float32).cpu()
                         if is_dtensor:
+                            global_shape = get_global_shape(param)
                             state_tensor = torch.reshape(state_tensor, param.shape).to(param.device)
                             state_tensor = init_as_dtensor(
                                 state_tensor,
                                 device_mesh=device_mesh,
                                 sharding_spec=shard_spec,
                                 global_shape=global_shape,
                             )
                         elif is_customized_distributed:
                             state_tensor = torch.reshape(state_tensor, param.shape).to(param.device)
                             init_tensor_as_customization_distributed(
                                 state_tensor, shard_fn=param.shard_fn, gather_fn=param.gather_fn
                             )
                         state_tensor = gather_distributed_param(state_tensor, keep_vars=False).cpu()
-
-                        collected_states[state_name] = state_tensor.reshape(global_shape)
+                        state_tensor = state_tensor.reshape(global_shape)
+                        if is_padded_tensor(param):
+                            state_tensor = init_as_padded_tensor(
+                                state_tensor, param._current_length, param._origin_length, param._padding_dim
+                            )
+                            state_tensor = to_unpadded_tensor(state_tensor)
+                        collected_states[state_name] = state_tensor
             return collected_states
 
         # Check whether the param with given id is managed by current process.
         own_param = param_id in self.id_to_fake_params
 
         # Collector gets prepared for state collecting.
         if is_collector:
@@ -531,24 +544,30 @@
 
         # Reshape tensors
         if is_collector:
             for state_name, state_tensor in collected_states.items():
                 if state_tensor.numel() == param.numel():
                     collected_states[state_name] = torch.reshape(state_tensor, param.shape)
                 if is_dtensor:
+                    global_shape = get_global_shape(param)
                     state_tensor = state_tensor.to(param.device)
                     state_tensor = init_as_dtensor(
                         state_tensor, sharding_spec=shard_spec, device_mesh=device_mesh, global_shape=global_shape
                     )
                 elif is_customized_distributed:
                     state_tensor = state_tensor.to(param.device)
                     init_tensor_as_customization_distributed(
                         state_tensor, shard_fn=param.shard_fn, gather_fn=param.gather_fn
                     )
                 state_tensor = gather_distributed_param(state_tensor, keep_vars=False).cpu()
+                if is_padded_tensor(param):
+                    state_tensor = init_as_padded_tensor(
+                        state_tensor, param._current_length, param._origin_length, param._padding_dim
+                    )
+                    state_tensor = to_unpadded_tensor(state_tensor)
 
         return collected_states
 
     def pack_optimizer_states_to_tensor(
         self,
         param_id: int,
         state_names: list,
@@ -694,15 +713,15 @@
             self.optim.param_groups.append(updated_group)
 
     def load_single_param_states(self, param_id: int, saved_states: dict):
         """
         Load saved optimizer states into parameter with given id.
         """
 
-        def cast(param, state_range, value, key=None):
+        def cast(param, state_range, value, global_shape, origin_shape, key=None):
             """
             Make a copy of the needed segment of value and cast it to device of param.
             """
             assert isinstance(value, torch.Tensor)
             ret_val = value
             if key == "step":
                 assert value.numel() == 1
@@ -710,15 +729,22 @@
             else:
                 state_start, state_end = state_range
                 ret_val = torch.zeros(
                     state_end - state_start, dtype=torch.float32, device=param.device, requires_grad=False
                 )
 
                 if is_dtensor:
-                    value = torch.reshape(value, global_shape)
+                    global_shape = get_global_shape(real_param)
+
+                if is_padded_tensor(real_param):
+                    value = torch.reshape(value, origin_shape)
+                    padding_dim = real_param._padding_dim
+                    value = to_padded_tensor(value, global_shape[padding_dim], padding_dim)
+
+                if is_dtensor:
                     value = distribute_tensor(value, sharding_spec=shard_spec, device_mesh=device_mesh)
                 elif is_customized_distributed:
                     value = torch.reshape(value, global_shape)
                     value = distribute_tensor_with_customization(value, real_param.shard_fn, real_param.gather_fn)
 
                 ret_val.copy_(value.flatten()[state_start:state_end])
             return ret_val
@@ -733,18 +759,19 @@
 
         # get tensor parallelism information
         real_param = self.id_to_real_params[param_id]
         is_dtensor = is_distributed_tensor(real_param)
         is_customized_distributed = is_customized_distributed_tensor(real_param)
         shard_spec = get_sharding_spec(real_param) if is_dtensor else None
         device_mesh = get_device_mesh(real_param) if is_dtensor else None
-        global_shape = self.optimizer_params_info["id2shape"][param_id]
+        global_shape = self.params_info["id2shape"][param_id]
+        origin_shape = global_shape
 
         for k, v in saved_states.items():
-            updated_states[k] = cast(fake_param, state_range, v, k)
+            updated_states[k] = cast(fake_param, state_range, v, global_shape, origin_shape, k)
             del v  # clean loaded states
         self.optim.state[fake_param].update(updated_states)
 
     def load_param_states(self, param_states: dict):
         """Loads param states from a state_dict. The param_states can be complete or sharded.
            During loading, filter out the part of states not considered by current process.
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/chunk_memstats_collector.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/chunk_memstats_collector.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/memory_monitor.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memory_monitor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/memory_stats.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memory_stats.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/memstats_collector.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/memstats_collector.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/param_runtime_order.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/param_runtime_order.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/runtime_mem_tracer.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/runtime_mem_tracer.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/static_memstats_collector.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/static_memstats_collector.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/memory_tracer/utils.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/memory_tracer/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/placement_policy.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/placement_policy.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/gemini/utils.py` & `colossalai-nightly-2024.5.4/colossalai/zero/gemini/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/low_level/_utils.py` & `colossalai-nightly-2024.5.4/colossalai/zero/low_level/_utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -186,14 +186,15 @@
 def calculate_global_norm_from_list(norm_list):
     """Compute total from a list of norms"""
     total_norm = 0.0
     for norm in norm_list:
         total_norm += norm**2.0
     return math.sqrt(total_norm)
 
+
 def sync_tensor(flat_tensor, tensor_list):
     """
     Synchronize the flattened tensor and unflattened tensor list. When
     a list of tensor are flattened with `torch._utils._unflatten_dense_tensors`,
     a new tensor is created. Thus, the flat tensor and original tensor list do not
     share the same memory space. This function will update the tensor list so that
     they point to the same value.
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/low_level/bookkeeping/bucket_store.py` & `colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/bucket_store.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,15 +7,17 @@
 
 from .base_store import BaseStore
 
 
 class BucketStore(BaseStore):
     def __init__(self, torch_pg: ProcessGroup):
         super().__init__(torch_pg)
+        self.reset_all()
 
+    def reset_all(self) -> None:
         # init
         self.current_group_id = 0
         self._num_elements_in_bucket = 0
         # mapping gradient slices and parameter
         self.grad_to_param_mapping = dict()
 
         self._grad_in_bucket = dict()
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/low_level/bookkeeping/gradient_store.py` & `colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/gradient_store.py`

 * *Files 2% similar despite different names*

```diff
@@ -78,14 +78,17 @@
             group_id (int): The index of a parameter group
 
         Returns:
             List: the list working gradient slices in the group
         """
 
         grad_list = []
+        # When using LoRa and the user sets multiple param_groups, it is possible that some param_groups have no parameters with gradients.
+        if group_id not in self._grads_of_params.keys():
+            return grad_list
         for param_grads in self._grads_of_params[group_id].values():
             grad_list.append(param_grads[self._working_index])
 
         return grad_list
 
     def get_working_grad_by_param_id(self, param_id) -> Tensor:
         """
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/low_level/bookkeeping/parameter_store.py` & `colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/parameter_store.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/low_level/bookkeeping/tensor_bucket.py` & `colossalai-nightly-2024.5.4/colossalai/zero/low_level/bookkeeping/tensor_bucket.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/low_level/low_level_optim.py` & `colossalai-nightly-2024.5.4/colossalai/zero/low_level/low_level_optim.py`

 * *Files 2% similar despite different names*

```diff
@@ -36,15 +36,21 @@
         growth_factor: float = 2,
         backoff_factor: float = 0.5,
         growth_interval: int = 1000,
         hysteresis: int = 2,
         max_scale: float = 2**32,
     ) -> None:
         super().__init__(
-            initial_scale, min_scale, growth_factor, backoff_factor, growth_interval, hysteresis, max_scale
+            initial_scale,
+            min_scale,
+            growth_factor,
+            backoff_factor,
+            growth_interval,
+            hysteresis,
+            max_scale,
         )
         self.num_working_param_groups = num_working_param_groups
         self.grad_store = grad_store
 
     def check_local_overflow(self) -> bool:
         for group_id in range(self.num_working_param_groups):
             for avg_grad in self.grad_store.get_working_grads_by_group_id(group_id):
@@ -75,14 +81,15 @@
         cpu_offload: bool = False,  # cpu offload
         dp_process_group: Optional[ProcessGroup] = None,  # the dp pg for comm
         forced_dtype: Optional[torch.dtype] = None,
         moe_extra_dp_process_group: Optional[ProcessGroup] = None,
         master_weights: bool = True,  # master weights
     ):
         super(LowLevelZeroOptimizer, self).__init__(optim=optimizer)
+
         self._dtype = self.optim.param_groups[0]["params"][0].dtype
         self._logger = get_dist_logger()
         self._verbose = verbose
 
         # stage 2
         self._partition_grads = partition_grad
 
@@ -224,17 +231,18 @@
         return len(self._working_param_groups)
 
     def _sanity_checks(self):
         assert get_accelerator().name in ["cuda", "npu"], "device is required"
         for param_group in self.optim.param_groups:
             group_params = param_group["params"]
             for param in group_params:
-                assert (
-                    param.dtype == self._dtype
-                ), f"Parameters are expected to have the same dtype `{self._dtype}`, but got `{param.dtype}`"
+                if not hasattr(param, "skip_zero_check") or param.skip_zero_check is False:
+                    assert (
+                        param.dtype == self._dtype
+                    ), f"Parameters are expected to have the same dtype `{self._dtype}`, but got `{param.dtype}`"
 
     def _create_master_param_current_rank(self, param_list):
         # split each param evenly by world size
         params_current_rank = []
         device = "cpu" if self._cpu_offload else get_accelerator().get_current_device()
 
         for param in param_list:
@@ -268,28 +276,27 @@
 
         return params_current_rank
 
     ###########################
     # Backward Reduction Hook #
     ###########################
 
-    def _grad_handler(self, param, group_id, grad):
+    def _grad_handler(self, group_id, param):
         # if run with no_sync context, would not sync grad when backward
         if self.require_grad_sync:
             self._add_to_bucket(param, group_id)
-        return grad
 
     def _attach_reduction_hook(self):
         # we iterate over the working params
         # on each param, we register a hook to its AccumulateGrad object
         for group_id in range(self.num_param_groups):
             param_group = self._working_param_groups[group_id]
             for param in param_group:
                 if param.requires_grad:
-                    param.register_hook(partial(self._grad_handler, param, group_id))
+                    param.register_post_accumulate_grad_hook(partial(self._grad_handler, group_id))
 
     #######################
     # Reduction Functions #
     #######################
 
     def _run_reduction(self):
         if self._bucket_store.num_elements_in_bucket() > 0:
@@ -410,23 +417,30 @@
                         if len(non_moe_grad_list) > 0:
                             flat_grads_list = list(
                                 non_moe_flat_grads.split(len(non_moe_flat_grads) // self._world_size)
                             )
                             recieved_grad = torch.zeros_like(flat_grads_list[0])
                             dist.reduce_scatter(recieved_grad, flat_grads_list, group=self.dp_pg)
                             self._update_partitoned_grad(
-                                non_moe_grad_in_bucket_current_rank, recieved_grad, group_id, 1
+                                non_moe_grad_in_bucket_current_rank,
+                                recieved_grad,
+                                group_id,
+                                1,
                             )
 
                         if len(moe_grad_list) > 0:
                             flat_grads_list = list(
                                 moe_flat_grads.split(len(moe_flat_grads) // self.moe_extra_dp_pg_size)
                             )
                             recieved_grad = torch.zeros_like(flat_grads_list[0])
-                            dist.reduce_scatter(recieved_grad, flat_grads_list, group=self.moe_extra_dp_pg)
+                            dist.reduce_scatter(
+                                recieved_grad,
+                                flat_grads_list,
+                                group=self.moe_extra_dp_pg,
+                            )
                             param_slice = self._world_size // self.moe_extra_dp_pg_size
                             recieved_grad = list(recieved_grad.split(len(recieved_grad) // param_slice))
                             for split_recieved_grad in recieved_grad:
                                 split_recieved_grad = _unflatten_dense_tensors(
                                     split_recieved_grad, moe_grad_in_bucket_current_rank
                                 )
                                 for real_grad, grad in zip(split_recieved_grad, moe_grad_in_bucket_current_rank):
@@ -439,22 +453,33 @@
         for rank, grad_list in enumerate(origin_grad_list):
             sync_tensor(flat_grad_list[rank], grad_list)
             for grad in grad_list:
                 param_id = self._bucket_store.get_param_id_of_grad(grad)
                 self._add_grad(grad, self._world_size, group_id, param_id, rank)
 
     def _update_partitoned_grad(
-        self, origin_grad_list: List, flat_grad: torch.Tensor, group_id: int, partition_num: int
+        self,
+        origin_grad_list: List,
+        flat_grad: torch.Tensor,
+        group_id: int,
+        partition_num: int,
     ) -> None:
         sync_tensor(flat_grad, origin_grad_list)
         for grad in origin_grad_list:
             param_id = self._bucket_store.get_param_id_of_grad(grad)
             self._add_grad(grad, partition_num, group_id, param_id)
 
-    def _add_grad(self, grad: torch.Tensor, partition_num: int, group_id: int, param_id: int, rank: int = 0) -> None:
+    def _add_grad(
+        self,
+        grad: torch.Tensor,
+        partition_num: int,
+        group_id: int,
+        param_id: int,
+        rank: int = 0,
+    ) -> None:
         if len(self._grad_store.get_partitioned_gradients_by_param_id(group_id, param_id)) < partition_num:
             self._grad_store.append_gradients_by_param_id(grad, group_id, param_id)
         else:
             self._grad_store.add_gradients_by_param_id(grad, rank, group_id, param_id)
 
     def _add_to_bucket(self, param, group_id):
         param_size = param.numel()
@@ -490,15 +515,14 @@
             return
 
         self._reduce_grad(self._partition_grads)
 
         # clear reduced grads
         if self._overlap_communication:
             get_accelerator().synchronize()
-
         self.zero_grad()
 
     def backward_by_grad(self, tensor, grad):
         assert not (
             self._partition_grads and not self.require_grad_sync
         ), "ZeRO2(partition_grads) and gradient accumulation(no_sync) are not compatible"
 
@@ -530,14 +554,15 @@
             for param in param_group:
                 if set_to_none:
                     param.grad = None
                 else:
                     if param.grad is not None:
                         param.grad.detach()
                         param.grad.zero_()
+        self._bucket_store.reset_all()
 
     ####################
     # Update Parameter #
     ####################
 
     def step(self, closure=None):
         assert closure is None, "closure is not supported by step()"
@@ -651,22 +676,28 @@
                 working_param = real_working_params[group_id][idx]
                 if self.moe_extra_dp_pg is not None and is_moe_tensor(working_param):
                     all_splited_param = [
                         torch.zeros(splited_param.shape, device=device, dtype=self._dtype)
                         for _ in range(self.moe_extra_dp_pg_size)
                     ]
                     dist.all_gather(
-                        all_splited_param, splited_param.to(device).to(self._dtype), group=self.moe_extra_dp_pg
+                        all_splited_param,
+                        splited_param.to(device).to(self._dtype),
+                        group=self.moe_extra_dp_pg,
                     )
                 else:
                     all_splited_param = [
                         torch.zeros(splited_param.shape, device=device, dtype=self._dtype)
                         for _ in range(self._world_size)
                     ]
-                    dist.all_gather(all_splited_param, splited_param.to(device).to(self._dtype), group=self.dp_pg)
+                    dist.all_gather(
+                        all_splited_param,
+                        splited_param.to(device).to(self._dtype),
+                        group=self.dp_pg,
+                    )
                 working_param.data.copy_(flatten(all_splited_param)[: working_param.numel()].reshape_as(working_param))
             self.optim.param_groups[group_id]["params"] = self._master_param_groups_of_current_rank[group_id]
 
     def _compute_grad_norm(self, gradients: List[Tensor], norm_type: int = 2) -> float:
         r"""
         Compute and return the gradient norm for gradient clipping.
 
@@ -681,31 +712,37 @@
         if len(gradients) == 0:
             return 0.0
 
         norm_type = float(norm_type)
         if norm_type == inf:
             total_norm = max(grad.data.abs().max() for grad in gradients)
             total_norm_cuda = torch.tensor(
-                [float(total_norm)], device=get_accelerator().get_current_device(), dtype=torch.float
+                [float(total_norm)],
+                device=get_accelerator().get_current_device(),
+                dtype=torch.float,
             )
             dist.all_reduce(total_norm_cuda, op=torch.distributed.ReduceOp.MAX, group=self.dp_pg)
             total_norm = total_norm_cuda.item()
 
         else:
             total_norm_exponentiated = 0.0
             for grad in gradients:
                 grad_norm_exponentiated = grad.data.double().norm(norm_type) ** norm_type
                 total_norm_exponentiated += grad_norm_exponentiated
 
             # Sum across all model parallel GPUs.
             total_norm_exponentiated_cuda = torch.tensor(
-                [float(total_norm_exponentiated)], device=get_accelerator().get_current_device(), dtype=torch.float
+                [float(total_norm_exponentiated)],
+                device=get_accelerator().get_current_device(),
+                dtype=torch.float,
             )
             torch.distributed.all_reduce(
-                total_norm_exponentiated_cuda, op=torch.distributed.ReduceOp.SUM, group=self.dp_pg
+                total_norm_exponentiated_cuda,
+                op=torch.distributed.ReduceOp.SUM,
+                group=self.dp_pg,
             )
             total_norm = total_norm_exponentiated_cuda.item() ** (1.0 / norm_type)
 
         return total_norm
 
     #############################
     # Mixed Precision Utilities #
@@ -916,9 +953,12 @@
                 master_moe_param.copy_(working_moe_param)
 
     def get_working_to_master_map(self) -> Dict[int, torch.Tensor]:
         return self._param_store.working_to_master_param
 
     def get_master_to_working_map(self) -> Dict[int, torch.Tensor]:
         if hasattr(self, "moe_master_to_working_map"):
-            return {**self._param_store.master_to_working_param, **self.moe_master_to_working_map}
+            return {
+                **self._param_store.master_to_working_param,
+                **self.moe_master_to_working_map,
+            }
         return self._param_store.master_to_working_param
```

### Comparing `colossalai-nightly-2024.3.9/colossalai/zero/wrapper.py` & `colossalai-nightly-2024.5.4/colossalai/zero/wrapper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/colossalai_nightly.egg-info/PKG-INFO` & `colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: colossalai-nightly
-Version: 2024.3.9
+Version: 2024.5.4
 Summary: An integrated large-scale model training system with efficient parallelization techniques
 Home-page: https://www.colossalai.org
 License: Apache Software License 2.0
 Project-URL: Forum, https://github.com/hpcaitech/ColossalAI/discussions
 Project-URL: Bug Tracker, https://github.com/hpcaitech/ColossalAI/issues
 Project-URL: Examples, https://github.com/hpcaitech/ColossalAI-Examples
 Project-URL: Documentation, http://colossalai.readthedocs.io
@@ -32,14 +32,18 @@
         
         
            | [English](README.md) | [](docs/README-zh-Hans.md) |
         
         </div>
         
         ## Latest News
+        * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-resolution-in-open-source)
+        * [2024/04] [Most cost-effective solutions for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-pretraining-tailored-to-llama3-series)
+        * [2024/03] [314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
+        * [2024/03] [Open-Sora: Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models](https://hpc-ai.com/blog/open-sora-v1.0)
         * [2024/03] [Open-SoraSora Replication Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million](https://hpc-ai.com/blog/open-sora)
         * [2024/01] [Inference Performance Improved by 46%, Open Source Solution Breaks the Length Limit of LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-SwiftInfer)
         * [2024/01] [Construct Refined 13B Private Model With Just $5000 USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/colossal-llama-2-13b)
         * [2023/11] [Enhanced MoE Parallelism, Open-source MoE Model Training Can Be 9 Times More Efficient](https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-efficient)
         * [2023/09] [One Half-Day of Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-Free Domain-Specific LLM Solution](https://www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-large-models-open-source-and-commercial-free-domain-specific-llm-solution)
         * [2023/09] [70 Billion Parameter LLaMA2 Model Training Accelerated by 195%](https://www.hpc-ai.tech/blog/70b-llama2-training)
         * [2023/07] [HPC-AI Tech Raises 22 Million USD in Series A Funding](https://www.hpc-ai.tech/blog/hpc-ai-tech-raises-22-million-usd-in-series-a-funding-to-fuel-team-expansion-and-business-growth)
@@ -47,25 +51,25 @@
         ## Table of Contents
         <ul>
          <li><a href="#Why-Colossal-AI">Why Colossal-AI</a> </li>
          <li><a href="#Features">Features</a> </li>
          <li>
            <a href="#Colossal-AI-in-the-Real-World">Colossal-AI for Real World Applications</a>
            <ul>
-             <li><a href="#Open-Sora">Open-Sora: Open-SoraSora Replication Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million</a></li>
+             <li><a href="#Open-Sora">Open-Sora: Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models</a></li>
              <li><a href="#Colossal-LLaMA-2">Colossal-LLaMA-2: One Half-Day of Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-Free Domain-Specific Llm Solution</a></li>
              <li><a href="#ColossalChat">ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline</a></li>
              <li><a href="#AIGC">AIGC: Acceleration of Stable Diffusion</a></li>
              <li><a href="#Biomedicine">Biomedicine: Acceleration of AlphaFold Protein Structure</a></li>
            </ul>
          </li>
          <li>
            <a href="#Parallel-Training-Demo">Parallel Training Demo</a>
            <ul>
-             <li><a href="#LLaMA2">LLaMA 1/2</a></li>
+             <li><a href="#LLaMA3">LLaMA 1/2/3 </a></li>
              <li><a href="#MoE">MoE</a></li>
              <li><a href="#GPT-3">GPT-3</a></li>
              <li><a href="#GPT-2">GPT-2</a></li>
              <li><a href="#BERT">BERT</a></li>
              <li><a href="#PaLM">PaLM</a></li>
              <li><a href="#OPT">OPT</a></li>
              <li><a href="#ViT">ViT</a></li>
@@ -78,14 +82,15 @@
              <li><a href="#GPT-2-Single">GPT-2</a></li>
              <li><a href="#PaLM-Single">PaLM</a></li>
            </ul>
          </li>
          <li>
            <a href="#Inference">Inference</a>
            <ul>
+             <li><a href="#Grok-1">Grok-1: 314B model of PyTorch + HuggingFace Inference</a></li>
              <li><a href="#SwiftInfer">SwiftInfer:Breaks the Length Limit of LLM for Multi-Round Conversations with 46% Acceleration</a></li>
              <li><a href="#GPT-3-Inference">GPT-3</a></li>
              <li><a href="#OPT-Serving">OPT-175B Online Serving for Text Generation</a></li>
              <li><a href="#BLOOM-Inference">176B BLOOM</a></li>
            </ul>
          </li>
          <li>
@@ -133,26 +138,27 @@
           - Parallelism based on the configuration file
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         ## Colossal-AI in the Real World
         ### Open-Sora
         
-        [Open-Sora](https://github.com/hpcaitech/Open-Sora)Sora Replication Solution with 46% Cost Reduction, Sequence Expansion to Nearly a Million
+        [Open-Sora](https://github.com/hpcaitech/Open-Sora)Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models
         [[code]](https://github.com/hpcaitech/Open-Sora)
-        [[blog]](https://hpc-ai.com/blog/open-sora)
+        [[blog]](https://hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-resolution-in-open-source)
+        [[HuggingFace model weights]](https://huggingface.co/hpcai-tech/Open-Sora)
+        [[Demo]](https://github.com/hpcaitech/Open-Sora?tab=readme-ov-file#-latest-demo)
         
-        <p id="diffusion_demo" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/open-sora-1.png" width=600/>
-        </p>
-        
-        <p id="diffusion_demo" align="center">
-        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/open-sora-2.png" width=600/>
-        </p>
+        <div align="center">
+           <a href="https://www.youtube.com/watch?v=iDTxepqixuc">
+           <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/sora-demo.png" width="700" />
+           </a>
+        </div>
         
+        <p align="right">(<a href="#top">back to top</a>)</p>
         
         ### Colossal-LLaMA-2
         
         - 7B: One half-day of training using a few hundred dollars yields similar results to mainstream large models, open-source and commercial-free domain-specific LLM solution.
         [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Colossal-LLaMA-2)
         [[blog]](https://www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-large-models-open-source-and-commercial-free-domain-specific-llm-solution)
         [[HuggingFace model weights]](https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-7b-base)
@@ -273,30 +279,38 @@
         
         - [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer): accelerating structure prediction of protein monomers and multimer by 11x.
         
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         ## Parallel Training Demo
+        ### LLaMA3
+        <p align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/LLaMA3-70B-H100.png" width=600/>
+        </p>
+        
+        - 70 billion parameter LLaMA3 model training accelerated by 18%
+        [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)
+        
         ### LLaMA2
         <p align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/llama2_pretraining.png" width=600/>
         </p>
         
         - 70 billion parameter LLaMA2 model training accelerated by 195%
-        [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama2)
+        [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)
         [[blog]](https://www.hpc-ai.tech/blog/70b-llama2-training)
         
         ### LLaMA1
         <p align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/LLaMA_pretraining.png" width=600/>
         </p>
         
         - 65-billion-parameter large model pretraining accelerated by 38%
-        [[code]](https://github.com/hpcaitech/ColossalAI/tree/example/llama/examples/language/llama)
+        [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)
         [[blog]](https://www.hpc-ai.tech/blog/large-model-pretraining)
         
         ### MoE
         <p align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/MOE_training.png" width=800/>
         </p>
         
@@ -370,14 +384,26 @@
         
         - 34x larger model size on the same hardware
         
         <p align="right">(<a href="#top">back to top</a>)</p>
         
         
         ## Inference
+        ### Grok-1
+        <p id="Grok-1" align="center">
+        <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/grok-1-inference.jpg" width=600/>
+        </p>
+        
+         - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use Python + PyTorch + HuggingFace version for Inference.
+        
+        [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1)
+        [[blog]](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)
+        [[HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/grok-1)
+        [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/models/colossalai/grok-1-pytorch/summary)
+        
         <p id="SwiftInfer" align="center">
         <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/SwiftInfer.jpg" width=800/>
         </p>
         
         - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance improved by 46%, open source solution breaks the length limit of LLM for multi-round conversations
         
         <p id="GPT-3-Inference" align="center">
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: colossalai-nightly Version: 2024.3.9 Summary: An
+Metadata-Version: 2.1 Name: colossalai-nightly Version: 2024.5.4 Summary: An
 integrated large-scale model training system with efficient parallelization
 techniques Home-page: https://www.colossalai.org License: Apache Software
 License 2.0 Project-URL: Forum, https://github.com/hpcaitech/ColossalAI/
 discussions Project-URL: Bug Tracker, https://github.com/hpcaitech/ColossalAI/
 issues Project-URL: Examples, https://github.com/hpcaitech/ColossalAI-Examples
 Project-URL: Documentation, http://colossalai.readthedocs.io Project-URL:
 Github, https://github.com/hpcaitech/ColossalAI Description: # Colossal-AI
@@ -22,59 +22,73 @@
  (https://img.shields.io/badge/%F0%9F%A4%97HuggingFace-Join-yellow)](https://
 huggingface.co/hpcai-tech) [![slack badge](https://img.shields.io/badge/Slack-
 join-blueviolet?logo=slack&)](https://github.com/hpcaitech/public_assets/tree/
  main/colossalai/contact/slack) [![WeChat badge](https://img.shields.io/badge/
 --green?logo=wechat&)](https://raw.githubusercontent.com/hpcaitech/
 public_assets/main/colossalai/img/WeChat.png) | [English](README.md) | []
                           (docs/README-zh-Hans.md) |
-## Latest News * [2024/03] [Open-SoraSora Replication Solution with 46% Cost
-Reduction, Sequence Expansion to Nearly a Million](https://hpc-ai.com/blog/
-open-sora) * [2024/01] [Inference Performance Improved by 46%, Open Source
-Solution Breaks the Length Limit of LLM for Multi-Round Conversations](https://
-hpc-ai.com/blog/Colossal-AI-SwiftInfer) * [2024/01] [Construct Refined 13B
-Private Model With Just $5000 USD, Upgraded Colossal-AI Llama-2 Open Source]
-(https://hpc-ai.com/blog/colossal-llama-2-13b) * [2023/11] [Enhanced MoE
-Parallelism, Open-source MoE Model Training Can Be 9 Times More Efficient]
-(https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-moe-model-
-training-can-be-9-times-more-efficient) * [2023/09] [One Half-Day of Training
-Using a Few Hundred Dollars Yields Similar Results to Mainstream Large Models,
-Open-Source and Commercial-Free Domain-Specific LLM Solution](https://www.hpc-
-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-
-similar-results-to-mainstream-large-models-open-source-and-commercial-free-
-domain-specific-llm-solution) * [2023/09] [70 Billion Parameter LLaMA2 Model
-Training Accelerated by 195%](https://www.hpc-ai.tech/blog/70b-llama2-training)
-* [2023/07] [HPC-AI Tech Raises 22 Million USD in Series A Funding](https://
-www.hpc-ai.tech/blog/hpc-ai-tech-raises-22-million-usd-in-series-a-funding-to-
-fuel-team-expansion-and-business-growth) ## Table of Contents
+## Latest News * [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open
+Source with Single-Shot 16-Second Video Generation and 720p Resolution](https:/
+/hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-
+video-generation-and-720p-resolution-in-open-source) * [2024/04] [Most cost-
+effective solutions for inference, fine-tuning and pretraining, tailored to
+LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-
+inference-fine-tuning-and-pretraining-tailored-to-llama3-series) * [2024/03]
+[314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, Efficient and
+Easy-to-Use PyTorch+HuggingFace version is Here](https://hpc-ai.com/blog/314-
+billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-
+use-pytorchhuggingface-version-is-here) * [2024/03] [Open-Sora: Revealing
+Complete Model Parameters, Training Details, and Everything for Sora-like Video
+Generation Models](https://hpc-ai.com/blog/open-sora-v1.0) * [2024/03] [Open-
+SoraSora Replication Solution with 46% Cost Reduction, Sequence Expansion to
+Nearly a Million](https://hpc-ai.com/blog/open-sora) * [2024/01] [Inference
+Performance Improved by 46%, Open Source Solution Breaks the Length Limit of
+LLM for Multi-Round Conversations](https://hpc-ai.com/blog/Colossal-AI-
+SwiftInfer) * [2024/01] [Construct Refined 13B Private Model With Just $5000
+USD, Upgraded Colossal-AI Llama-2 Open Source](https://hpc-ai.com/blog/
+colossal-llama-2-13b) * [2023/11] [Enhanced MoE Parallelism, Open-source MoE
+Model Training Can Be 9 Times More Efficient](https://www.hpc-ai.tech/blog/
+enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-
+efficient) * [2023/09] [One Half-Day of Training Using a Few Hundred Dollars
+Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-
+Free Domain-Specific LLM Solution](https://www.hpc-ai.tech/blog/one-half-day-
+of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-
+large-models-open-source-and-commercial-free-domain-specific-llm-solution) *
+[2023/09] [70 Billion Parameter LLaMA2 Model Training Accelerated by 195%]
+(https://www.hpc-ai.tech/blog/70b-llama2-training) * [2023/07] [HPC-AI Tech
+Raises 22 Million USD in Series A Funding](https://www.hpc-ai.tech/blog/hpc-ai-
+tech-raises-22-million-usd-in-series-a-funding-to-fuel-team-expansion-and-
+business-growth) ## Table of Contents
     * _W_h_y_ _C_o_l_o_s_s_a_l_-_A_I
     * _F_e_a_t_u_r_e_s
     * _C_o_l_o_s_s_a_l_-_A_I_ _f_o_r_ _R_e_a_l_ _W_o_r_l_d_ _A_p_p_l_i_c_a_t_i_o_n_s
-          o _O_p_e_n_-_S_o_r_a_:_ _O_p_e_n_-_S_o_r_a____S_o_r_a_ _R_e_p_l_i_c_a_t_i_o_n_ _S_o_l_u_t_i_o_n_ _w_i_t_h_ _4_6_%_ _C_o_s_t
-            _R_e_d_u_c_t_i_o_n_,_ _S_e_q_u_e_n_c_e_ _E_x_p_a_n_s_i_o_n_ _t_o_ _N_e_a_r_l_y_ _a_ _M_i_l_l_i_o_n
+          o _O_p_e_n_-_S_o_r_a_:_ _R_e_v_e_a_l_i_n_g_ _C_o_m_p_l_e_t_e_ _M_o_d_e_l_ _P_a_r_a_m_e_t_e_r_s_,_ _T_r_a_i_n_i_n_g_ _D_e_t_a_i_l_s_,
+            _a_n_d_ _E_v_e_r_y_t_h_i_n_g_ _f_o_r_ _S_o_r_a_-_l_i_k_e_ _V_i_d_e_o_ _G_e_n_e_r_a_t_i_o_n_ _M_o_d_e_l_s
           o _C_o_l_o_s_s_a_l_-_L_L_a_M_A_-_2_:_ _O_n_e_ _H_a_l_f_-_D_a_y_ _o_f_ _T_r_a_i_n_i_n_g_ _U_s_i_n_g_ _a_ _F_e_w_ _H_u_n_d_r_e_d
             _D_o_l_l_a_r_s_ _Y_i_e_l_d_s_ _S_i_m_i_l_a_r_ _R_e_s_u_l_t_s_ _t_o_ _M_a_i_n_s_t_r_e_a_m_ _L_a_r_g_e_ _M_o_d_e_l_s_,_ _O_p_e_n_-
             _S_o_u_r_c_e_ _a_n_d_ _C_o_m_m_e_r_c_i_a_l_-_F_r_e_e_ _D_o_m_a_i_n_-_S_p_e_c_i_f_i_c_ _L_l_m_ _S_o_l_u_t_i_o_n
           o _C_o_l_o_s_s_a_l_C_h_a_t_:_ _A_n_ _O_p_e_n_-_S_o_u_r_c_e_ _S_o_l_u_t_i_o_n_ _f_o_r_ _C_l_o_n_i_n_g_ _C_h_a_t_G_P_T_ _W_i_t_h_ _a
             _C_o_m_p_l_e_t_e_ _R_L_H_F_ _P_i_p_e_l_i_n_e
           o _A_I_G_C_:_ _A_c_c_e_l_e_r_a_t_i_o_n_ _o_f_ _S_t_a_b_l_e_ _D_i_f_f_u_s_i_o_n
           o _B_i_o_m_e_d_i_c_i_n_e_:_ _A_c_c_e_l_e_r_a_t_i_o_n_ _o_f_ _A_l_p_h_a_F_o_l_d_ _P_r_o_t_e_i_n_ _S_t_r_u_c_t_u_r_e
     * _P_a_r_a_l_l_e_l_ _T_r_a_i_n_i_n_g_ _D_e_m_o
-          o _L_L_a_M_A_ _1_/_2
+          o _L_L_a_M_A_ _1_/_2_/_3
           o _M_o_E
           o _G_P_T_-_3
           o _G_P_T_-_2
           o _B_E_R_T
           o _P_a_L_M
           o _O_P_T
           o _V_i_T
           o _R_e_c_o_m_m_e_n_d_a_t_i_o_n_ _S_y_s_t_e_m_ _M_o_d_e_l_s
     * _S_i_n_g_l_e_ _G_P_U_ _T_r_a_i_n_i_n_g_ _D_e_m_o
           o _G_P_T_-_2
           o _P_a_L_M
     * _I_n_f_e_r_e_n_c_e
+          o _G_r_o_k_-_1_:_ _3_1_4_B_ _m_o_d_e_l_ _o_f_ _P_y_T_o_r_c_h_ _+_ _H_u_g_g_i_n_g_F_a_c_e_ _I_n_f_e_r_e_n_c_e
           o _S_w_i_f_t_I_n_f_e_r_:_B_r_e_a_k_s_ _t_h_e_ _L_e_n_g_t_h_ _L_i_m_i_t_ _o_f_ _L_L_M_ _f_o_r_ _M_u_l_t_i_-_R_o_u_n_d
             _C_o_n_v_e_r_s_a_t_i_o_n_s_ _w_i_t_h_ _4_6_%_ _A_c_c_e_l_e_r_a_t_i_o_n
           o _G_P_T_-_3
           o _O_P_T_-_1_7_5_B_ _O_n_l_i_n_e_ _S_e_r_v_i_n_g_ _f_o_r_ _T_e_x_t_ _G_e_n_e_r_a_t_i_o_n
           o _1_7_6_B_ _B_L_O_O_M
     * _I_n_s_t_a_l_l_a_t_i_o_n
           o _P_y_P_I
@@ -98,21 +112,24 @@
 (https://arxiv.org/abs/2105.13120) - [Zero Redundancy Optimizer (ZeRO)](https:/
 /arxiv.org/abs/1910.02054) - [Auto-Parallelism](https://arxiv.org/abs/
 2302.02599) - Heterogeneous Memory Management - [PatrickStar](https://
 arxiv.org/abs/2108.05818) - Friendly Usage - Parallelism based on the
 configuration file
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
 ## Colossal-AI in the Real World ### Open-Sora [Open-Sora](https://github.com/
-hpcaitech/Open-Sora)Sora Replication Solution with 46% Cost Reduction,
-Sequence Expansion to Nearly a Million [[code]](https://github.com/hpcaitech/
-Open-Sora) [[blog]](https://hpc-ai.com/blog/open-sora)
- [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
-                             sora/open-sora-1.png]
- [https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/
-                             sora/open-sora-2.png]
+hpcaitech/Open-Sora)Revealing Complete Model Parameters, Training Details,
+and Everything for Sora-like Video Generation Models [[code]](https://
+github.com/hpcaitech/Open-Sora) [[blog]](https://hpc-ai.com/blog/open-soras-
+comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-
+resolution-in-open-source) [[HuggingFace model weights]](https://
+huggingface.co/hpcai-tech/Open-Sora) [[Demo]](https://github.com/hpcaitech/
+Open-Sora?tab=readme-ov-file#-latest-demo)
+ _[_h_t_t_p_s_:_/_/_r_a_w_._g_i_t_h_u_b_u_s_e_r_c_o_n_t_e_n_t_._c_o_m_/_h_p_c_a_i_t_e_c_h_/_p_u_b_l_i_c___a_s_s_e_t_s_/_m_a_i_n_/_a_p_p_l_i_c_a_t_i_o_n_s_/
+                              _s_o_r_a_/_s_o_r_a_-_d_e_m_o_._p_n_g_]
+                                                                  (_b_a_c_k_ _t_o_ _t_o_p)
 ### Colossal-LLaMA-2 - 7B: One half-day of training using a few hundred dollars
 yields similar results to mainstream large models, open-source and commercial-
 free domain-specific LLM solution. [[code]](https://github.com/hpcaitech/
 ColossalAI/tree/main/applications/Colossal-LLaMA-2) [[blog]](https://www.hpc-
 ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-
 similar-results-to-mainstream-large-models-open-source-and-commercial-free-
 domain-specific-llm-solution) [[HuggingFace model weights]](https://
@@ -203,25 +220,29 @@
 - [FastFold with Intel](https://github.com/hpcaitech/FastFold): 3x inference
 acceleration and 39% cost reduce.
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                            xTrimoMultimer_Table.jpg]
 - [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer):
 accelerating structure prediction of protein monomers and multimer by 11x.
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Parallel Training Demo ### LLaMA2
+## Parallel Training Demo ### LLaMA3
+   [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
+                          images/LLaMA3-70B-H100.png]
+- 70 billion parameter LLaMA3 model training accelerated by 18% [[code]](https:
+//github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama) ### LLaMA2
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                             llama2_pretraining.png]
 - 70 billion parameter LLaMA2 model training accelerated by 195% [[code]]
-(https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama2) [
+(https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama) [
 [blog]](https://www.hpc-ai.tech/blog/70b-llama2-training) ### LLaMA1
    [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
                          images/LLaMA_pretraining.png]
 - 65-billion-parameter large model pretraining accelerated by 38% [[code]]
-(https://github.com/hpcaitech/ColossalAI/tree/example/llama/examples/language/
-llama) [[blog]](https://www.hpc-ai.tech/blog/large-model-pretraining) ### MoE
+(https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama) [
+[blog]](https://www.hpc-ai.tech/blog/large-model-pretraining) ### MoE
    [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
                            images/MOE_training.png]
 - Enhanced MoE parallelism, Open-source MoE model training can be 9 times more
 efficient [[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/
 language/openmoe) [[blog]](https://www.hpc-ai.tech/blog/enhanced-moe-
 parallelism-open-source-moe-model-training-can-be-9-times-more-efficient) ###
 GPT-3
@@ -262,15 +283,25 @@
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 GPT2-NVME.png]
 - 120x larger model size on the same hardware (RTX 3080) ### PaLM
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 PaLM-GPU1.png]
 - 34x larger model size on the same hardware
                                                                   (_b_a_c_k_ _t_o_ _t_o_p)
-## Inference
+## Inference ### Grok-1
+   [https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/
+                         images/grok-1-inference.jpg]
+- 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use
+Python + PyTorch + HuggingFace version for Inference. [[code]](https://
+github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1) [[blog]]
+(https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-
+3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here) [
+[HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/
+grok-1) [[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/
+models/colossalai/grok-1-pytorch/summary)
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                                 SwiftInfer.jpg]
 - [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance
 improved by 46%, open source solution breaks the length limit of LLM for multi-
 round conversations
 [https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/
                              inference_GPT-3.jpg]
```

### Comparing `colossalai-nightly-2024.3.9/colossalai_nightly.egg-info/SOURCES.txt` & `colossalai-nightly-2024.5.4/colossalai_nightly.egg-info/SOURCES.txt`

 * *Files 0% similar despite different names*

```diff
@@ -342,15 +342,15 @@
 colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h
 colossalai/kernel/extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu
 colossalai/kernel/extensions/csrc/cuda/type_shim.h
 colossalai/kernel/extensions/csrc/cuda/include/block_reduce.h
 colossalai/kernel/extensions/flash_attention/__init__.py
 colossalai/kernel/extensions/flash_attention/flash_attention_dao_cuda.py
 colossalai/kernel/extensions/flash_attention/flash_attention_npu.py
-colossalai/kernel/extensions/flash_attention/flash_attention_xformers_cuda.py
+colossalai/kernel/extensions/flash_attention/flash_attention_sdpa_cuda.py
 colossalai/kernel/extensions/layernorm/__init__.py
 colossalai/kernel/extensions/layernorm/layernorm_cuda.py
 colossalai/kernel/extensions/moe/__init__.py
 colossalai/kernel/extensions/moe/moe_cuda.py
 colossalai/kernel/extensions/optimizer/__init__.py
 colossalai/kernel/extensions/optimizer/fused_optimizer_cuda.py
 colossalai/kernel/extensions/softmax/__init__.py
@@ -630,15 +630,14 @@
 colossalai/moe/loss.py
 colossalai/moe/manager.py
 colossalai/moe/routers.py
 colossalai/moe/utils.py
 colossalai/nn/__init__.py
 colossalai/nn/init.py
 colossalai/nn/layer/__init__.py
-colossalai/nn/layer/colo_attention.py
 colossalai/nn/layer/layernorm.py
 colossalai/nn/layer/scaled_softmax.py
 colossalai/nn/layer/utils.py
 colossalai/nn/loss/__init__.py
 colossalai/nn/lr_scheduler/__init__.py
 colossalai/nn/lr_scheduler/cosine.py
 colossalai/nn/lr_scheduler/delayed.py
@@ -661,18 +660,22 @@
 colossalai/pipeline/stage_manager.py
 colossalai/pipeline/schedule/__init__.py
 colossalai/pipeline/schedule/_utils.py
 colossalai/pipeline/schedule/base.py
 colossalai/pipeline/schedule/generate.py
 colossalai/pipeline/schedule/interleaved_pp.py
 colossalai/pipeline/schedule/one_f_one_b.py
+colossalai/quantization/__init__.py
+colossalai/quantization/bnb.py
+colossalai/quantization/bnb_config.py
 colossalai/shardformer/__init__.py
 colossalai/shardformer/_utils.py
 colossalai/shardformer/layer/__init__.py
 colossalai/shardformer/layer/_operation.py
+colossalai/shardformer/layer/attn.py
 colossalai/shardformer/layer/dropout.py
 colossalai/shardformer/layer/embedding.py
 colossalai/shardformer/layer/linear.py
 colossalai/shardformer/layer/loss.py
 colossalai/shardformer/layer/normalization.py
 colossalai/shardformer/layer/parallel_module.py
 colossalai/shardformer/layer/qkv_fused_linear.py
@@ -710,14 +713,15 @@
 colossalai/shardformer/policies/mistral.py
 colossalai/shardformer/policies/opt.py
 colossalai/shardformer/policies/sam.py
 colossalai/shardformer/policies/t5.py
 colossalai/shardformer/policies/vit.py
 colossalai/shardformer/policies/whisper.py
 colossalai/shardformer/shard/__init__.py
+colossalai/shardformer/shard/grad_ckpt_config.py
 colossalai/shardformer/shard/shard_config.py
 colossalai/shardformer/shard/sharder.py
 colossalai/shardformer/shard/shardformer.py
 colossalai/shardformer/shard/utils.py
 colossalai/tensor/__init__.py
 colossalai/tensor/colo_parameter.py
 colossalai/tensor/colo_tensor.py
@@ -733,14 +737,16 @@
 colossalai/tensor/d_tensor/layout_converter.py
 colossalai/tensor/d_tensor/misc.py
 colossalai/tensor/d_tensor/sharding_spec.py
 colossalai/tensor/d_tensor/utils.py
 colossalai/tensor/moe_tensor/__init__.py
 colossalai/tensor/moe_tensor/api.py
 colossalai/tensor/moe_tensor/moe_info.py
+colossalai/tensor/padded_tensor/__init__.py
+colossalai/tensor/padded_tensor/api.py
 colossalai/testing/__init__.py
 colossalai/testing/comparison.py
 colossalai/testing/pytest_wrapper.py
 colossalai/testing/random.py
 colossalai/testing/utils.py
 colossalai/utils/__init__.py
 colossalai/utils/common.py
@@ -830,15 +836,15 @@
 extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h
 extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu
 extensions/csrc/cuda/type_shim.h
 extensions/csrc/cuda/include/block_reduce.h
 extensions/flash_attention/__init__.py
 extensions/flash_attention/flash_attention_dao_cuda.py
 extensions/flash_attention/flash_attention_npu.py
-extensions/flash_attention/flash_attention_xformers_cuda.py
+extensions/flash_attention/flash_attention_sdpa_cuda.py
 extensions/layernorm/__init__.py
 extensions/layernorm/layernorm_cuda.py
 extensions/moe/__init__.py
 extensions/moe/moe_cuda.py
 extensions/optimizer/__init__.py
 extensions/optimizer/fused_optimizer_cuda.py
 extensions/softmax/__init__.py
@@ -938,14 +944,15 @@
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py
 tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py
 tests/test_shardformer/__init__.py
+tests/test_shardformer/test_flash_attention.py
 tests/test_shardformer/test_shard_utils.py
 tests/test_shardformer/test_with_torch_ddp.py
 tests/test_shardformer/test_model/__init__.py
 tests/test_shardformer/test_model/_utils.py
 tests/test_shardformer/test_model/test_shard_bert.py
 tests/test_shardformer/test_model/test_shard_blip2.py
 tests/test_shardformer/test_model/test_shard_bloom.py
```

### Comparing `colossalai-nightly-2024.3.9/examples/language/data_utils.py` & `colossalai-nightly-2024.5.4/examples/language/data_utils.py`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -117,8 +117,8 @@
         return self.num_samples
 
     def __getitem__(self, idx):
         return {
             "input_ids": self.input_ids[idx],
             "attention_mask": self.attention_mask[idx],
             "labels": self.input_ids[idx],
-        }
+        }
```

### Comparing `colossalai-nightly-2024.3.9/examples/language/model_utils.py` & `colossalai-nightly-2024.5.4/examples/language/model_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/examples/language/performance_evaluator.py` & `colossalai-nightly-2024.5.4/examples/language/performance_evaluator.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/__init__.py` & `colossalai-nightly-2024.5.4/extensions/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,36 +1,32 @@
 from .cpu_adam import CpuAdamArmExtension, CpuAdamX86Extension
-from .flash_attention import (
-    FlashAttentionDaoCudaExtension,
-    FlashAttentionNpuExtension,
-    FlashAttentionXformersCudaExtension,
-)
+from .flash_attention import FlashAttentionDaoCudaExtension, FlashAttentionNpuExtension, FlashAttentionSdpaCudaExtension
 from .layernorm import LayerNormCudaExtension
 from .moe import MoeCudaExtension
 from .optimizer import FusedOptimizerCudaExtension
 from .softmax import ScaledMaskedSoftmaxCudaExtension, ScaledUpperTriangleMaskedSoftmaxCudaExtension
 
 ALL_EXTENSIONS = [
     CpuAdamArmExtension,
     CpuAdamX86Extension,
     LayerNormCudaExtension,
     MoeCudaExtension,
     FusedOptimizerCudaExtension,
     ScaledMaskedSoftmaxCudaExtension,
     ScaledUpperTriangleMaskedSoftmaxCudaExtension,
     FlashAttentionDaoCudaExtension,
-    FlashAttentionXformersCudaExtension,
+    FlashAttentionSdpaCudaExtension,
     FlashAttentionNpuExtension,
 ]
 
 __all__ = [
     "CpuAdamArmExtension",
     "CpuAdamX86Extension",
     "LayerNormCudaExtension",
     "MoeCudaExtension",
     "FusedOptimizerCudaExtension",
     "ScaledMaskedSoftmaxCudaExtension",
     "ScaledUpperTriangleMaskedSoftmaxCudaExtension",
     "FlashAttentionDaoCudaExtension",
-    "FlashAttentionXformersCudaExtension",
+    "FlashAttentionSdpaCudaExtension",
     "FlashAttentionNpuExtension",
 ]
```

### Comparing `colossalai-nightly-2024.3.9/extensions/base_extension.py` & `colossalai-nightly-2024.5.4/extensions/base_extension.py`

 * *Files 2% similar despite different names*

```diff
@@ -54,21 +54,21 @@
         # concat
         home_directory = os.path.expanduser("~")
         extension_directory = f".cache/colossalai/torch_extensions/torch{torch_version_major}.{torch_version_minor}_{device_name}-{device_version}-{hash_suffix}"
         cache_directory = os.path.join(home_directory, extension_directory)
         return cache_directory
 
     @abstractmethod
-    def is_hardware_available(self) -> bool:
+    def is_available(self) -> bool:
         """
         Check if the hardware required by the kernel is available.
         """
 
     @abstractmethod
-    def assert_hardware_compatible(self) -> None:
+    def assert_compatible(self) -> None:
         """
         Check if the hardware required by the kernel is compatible.
         """
 
     @abstractmethod
     def build_aot(self) -> Union["CppExtension", "CUDAExtension"]:
         pass
```

### Comparing `colossalai-nightly-2024.3.9/extensions/cpp_extension.py` & `colossalai-nightly-2024.5.4/extensions/cpp_extension.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/cpu_adam/cpu_adam_arm.py` & `colossalai-nightly-2024.5.4/extensions/cpu_adam/cpu_adam_arm.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,19 +3,19 @@
 from ..cpp_extension import _CppExtension
 
 
 class CpuAdamArmExtension(_CppExtension):
     def __init__(self):
         super().__init__(name="cpu_adam_arm")
 
-    def is_hardware_available(self) -> bool:
+    def is_available(self) -> bool:
         # only arm allowed
         return platform.machine() == "aarch64"
 
-    def assert_hardware_compatible(self) -> None:
+    def assert_compatible(self) -> None:
         arch = platform.machine()
         assert (
             arch == "aarch64"
         ), f"[extension] The {self.name} kernel requires the CPU architecture to be aarch64 but got {arch}"
 
     # necessary 4 functions
     def sources_files(self):
```

### Comparing `colossalai-nightly-2024.3.9/extensions/cpu_adam/cpu_adam_x86.py` & `colossalai-nightly-2024.5.4/extensions/cpu_adam/cpu_adam_x86.py`

 * *Files 7% similar despite different names*

```diff
@@ -4,23 +4,23 @@
 from ..utils import append_nvcc_threads
 
 
 class CpuAdamX86Extension(_CudaExtension):
     def __init__(self):
         super().__init__(name="cpu_adam_x86")
 
-    def is_hardware_available(self) -> bool:
-        return platform.machine() == "x86_64" and super().is_hardware_available()
+    def is_available(self) -> bool:
+        return platform.machine() == "x86_64" and super().is_available()
 
-    def assert_hardware_compatible(self) -> None:
+    def assert_compatible(self) -> None:
         arch = platform.machine()
         assert (
             arch == "x86_64"
         ), f"[extension] The {self.name} kernel requires the CPU architecture to be x86_64 but got {arch}"
-        super().assert_hardware_compatible()
+        super().assert_compatible()
 
     # necessary 4 functions
     def sources_files(self):
         ret = [
             self.csrc_abs_path("cuda/cpu_adam.cpp"),
         ]
         return ret
```

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/arm/cpu_adam_arm.cpp` & `colossalai-nightly-2024.5.4/extensions/csrc/arm/cpu_adam_arm.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/arm/cpu_adam_arm.h` & `colossalai-nightly-2024.5.4/extensions/csrc/arm/cpu_adam_arm.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/colossal_C_frontend.cpp` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/colossal_C_frontend.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/cpu_adam.cpp` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/cpu_adam.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/cpu_adam.h` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/cpu_adam.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/include/block_reduce.h` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/include/block_reduce.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/layer_norm_cuda.cpp` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/layer_norm_cuda.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/layer_norm_cuda_kernel.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/layer_norm_cuda_kernel.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/moe_cuda.cpp` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/moe_cuda.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/moe_cuda_kernel.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/moe_cuda_kernel.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/multi_tensor_adam.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_adam.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/multi_tensor_apply.cuh` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_apply.cuh`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_l2norm_kernel.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/multi_tensor_lamb.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_lamb.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/multi_tensor_scale_kernel.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_scale_kernel.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/multi_tensor_sgd_kernel.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/scaled_masked_softmax.cpp` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_masked_softmax.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/scaled_masked_softmax.h` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_masked_softmax.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/scaled_masked_softmax_cuda.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_masked_softmax_cuda.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.cpp`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_upper_triang_masked_softmax.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/scaled_upper_triang_masked_softmax_cuda.cu`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/cuda/type_shim.h` & `colossalai-nightly-2024.5.4/extensions/csrc/cuda/type_shim.h`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/csrc/scaled_softmax.py` & `colossalai-nightly-2024.5.4/extensions/csrc/scaled_softmax.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/cuda_extension.py` & `colossalai-nightly-2024.5.4/extensions/cuda_extension.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,25 +18,25 @@
 class _CudaExtension(_CppExtension):
     @abstractmethod
     def nvcc_flags(self) -> List[str]:
         """
         This function should return a list of nvcc compilation flags for extensions.
         """
 
-    def is_hardware_available(self) -> bool:
+    def is_available(self) -> bool:
         # cuda extension can only be built if cuda is available
         try:
             import torch
 
             cuda_available = torch.cuda.is_available()
         except:
             cuda_available = False
         return cuda_available
 
-    def assert_hardware_compatible(self) -> None:
+    def assert_compatible(self) -> None:
         from torch.utils.cpp_extension import CUDA_HOME
 
         if not CUDA_HOME:
             raise AssertionError(
                 "[extension] CUDA_HOME is not found. You need to export CUDA_HOME environment variable or install CUDA Toolkit first in order to build/load CUDA extensions"
             )
         check_system_pytorch_cuda_match(CUDA_HOME)
```

### Comparing `colossalai-nightly-2024.3.9/extensions/layernorm/layernorm_cuda.py` & `colossalai-nightly-2024.5.4/extensions/layernorm/layernorm_cuda.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/moe/moe_cuda.py` & `colossalai-nightly-2024.5.4/extensions/moe/moe_cuda.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/optimizer/fused_optimizer_cuda.py` & `colossalai-nightly-2024.5.4/extensions/optimizer/fused_optimizer_cuda.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/softmax/scaled_masked_softmax_cuda.py` & `colossalai-nightly-2024.5.4/extensions/softmax/scaled_masked_softmax_cuda.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py` & `colossalai-nightly-2024.5.4/extensions/softmax/scaled_upper_triangle_masked_softmax_cuda.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/triton_extension.py` & `colossalai-nightly-2024.5.4/extensions/triton_extension.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/extensions/utils.py` & `colossalai-nightly-2024.5.4/extensions/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/setup.py` & `colossalai-nightly-2024.5.4/setup.py`

 * *Files 2% similar despite different names*

```diff
@@ -76,29 +76,29 @@
     from extensions import ALL_EXTENSIONS
 
     op_names = []
     ext_modules = []
 
     for ext_cls in ALL_EXTENSIONS:
         ext = ext_cls()
-        if ext.support_aot and ext.is_hardware_available():
-            ext.assert_hardware_compatible()
+        if ext.support_aot and ext.is_available():
+            ext.assert_compatible()
             op_names.append(ext.name)
             ext_modules.append(ext.build_aot())
 
     # show log
     if len(ext_modules) == 0:
         raise RuntimeError("[extension] Could not find any kernel compatible with the current environment.")
     else:
         op_name_list = ", ".join(op_names)
         print(f"[extension] Building extensions{op_name_list}")
 else:
     ext_modules = []
 
-version = "2024.03.09"
+version = "2024.05.04"
 package_name = "colossalai-nightly"
 
 setup(
     name=package_name,
     version=version,
     packages=find_packages(
         exclude=(
```

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/__init__.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,33 +1,33 @@
 import os
+
 from . import custom, diffusers, timm, torchaudio, torchvision, transformers
 from .executor import run_fwd, run_fwd_bwd
 from .registry import model_zoo
 
 # We pick a subset of models for fast testing in order to reduce the total testing time
 COMMON_MODELS = [
-    'custom_hanging_param_model',
-    'custom_nested_model',
-    'custom_repeated_computed_layers',
-    'custom_simple_net',
-    'diffusers_clip_text_model',
-    'diffusers_auto_encoder_kl',
-    'diffusers_unet2d_model',
-    'timm_densenet',
-    'timm_resnet',
-    'timm_swin_transformer',
-    'torchaudio_wav2vec2_base',
-    'torchaudio_conformer',
-    'transformers_bert_for_masked_lm',
-    'transformers_bloom_for_causal_lm',
-    'transformers_falcon_for_causal_lm',
-    'transformers_chatglm_for_conditional_generation',
-    'transformers_llama_for_casual_lm',
-    'transformers_vit_for_masked_image_modeling',
-    'transformers_mistral_for_casual_lm'
+    "custom_hanging_param_model",
+    "custom_nested_model",
+    "custom_repeated_computed_layers",
+    "custom_simple_net",
+    "diffusers_clip_text_model",
+    "diffusers_auto_encoder_kl",
+    "diffusers_unet2d_model",
+    "timm_densenet",
+    "timm_resnet",
+    "timm_swin_transformer",
+    "torchaudio_wav2vec2_base",
+    "torchaudio_conformer",
+    "transformers_bert_for_masked_lm",
+    "transformers_bloom_for_causal_lm",
+    "transformers_falcon_for_causal_lm",
+    "transformers_chatglm_for_conditional_generation",
+    "transformers_llama_for_casual_lm",
+    "transformers_vit_for_masked_image_modeling",
+    "transformers_mistral_for_casual_lm",
 ]
 
-IS_FAST_TEST = os.environ.get('FAST_TEST', '0') == '1'
-
+IS_FAST_TEST = os.environ.get("FAST_TEST", "0") == "1"
 
-__all__ = ["model_zoo", "run_fwd", "run_fwd_bwd", 'COMMON_MODELS', 'IS_FAST_TEST']
 
+__all__ = ["model_zoo", "run_fwd", "run_fwd_bwd", "COMMON_MODELS", "IS_FAST_TEST"]
```

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/custom/base.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/base.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/custom/hanging_param_model.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/hanging_param_model.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/custom/nested_model.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/nested_model.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/custom/repeated_computed_layers.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/repeated_computed_layers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/custom/simple_net.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/custom/simple_net.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/diffusers/diffusers.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/diffusers/diffusers.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/executor.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/executor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/registry.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/registry.py`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -98,8 +98,8 @@
                         new_dict[k] = v
 
         if not allow_empty:
             assert len(new_dict) > 0, f"No model found with keyword {keyword}"
         return new_dict
 
 
-model_zoo = ModelZooRegistry()
+model_zoo = ModelZooRegistry()
```

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/timm/timm.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/timm/timm.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/torchaudio/torchaudio.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchaudio/torchaudio.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/torchrec/torchrec.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchrec/torchrec.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/torchvision/torchvision.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/torchvision/torchvision.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/albert.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/albert.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/bert.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/bert.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/blip2.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/blip2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/bloom.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/bloom.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/chatglm2.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/chatglm2.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import torch
+from torch.nn import init
+from transformers import AutoConfig, AutoModelForCausalLM
 
-from colossalai.shardformer.modeling.chatglm2_6b.configuration_chatglm import ChatGLMConfig
-from colossalai.shardformer.modeling.chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration, ChatGLMModel
 from ..registry import ModelAttribute, model_zoo
 
 # ================================
 # Register single-sentence ChatGLM
 # ================================
 
 
@@ -29,50 +29,57 @@
 
 # define loss function
 loss_fn_for_chatglm_model = lambda x: torch.nn.functional.mse_loss(
     x["last_hidden_state"], torch.ones_like(x["last_hidden_state"])
 )
 loss_fn = lambda x: x["loss"]
 
-config = ChatGLMConfig(
+config = AutoConfig.from_pretrained(
+    "THUDM/chatglm2-6b",
+    trust_remote_code=True,
     num_layers=2,
     padded_vocab_size=65024,
     hidden_size=64,
+    ffn_hidden_size=214,
     num_attention_heads=8,
     kv_channels=16,
     rmsnorm=True,
     original_rope=True,
     use_cache=True,
+    multi_query_attention=False,
     torch_dtype=torch.float32,
 )
 
-infer_config = ChatGLMConfig(
+
+infer_config = AutoConfig.from_pretrained(
+    "THUDM/chatglm2-6b",
+    trust_remote_code=True,
     num_layers=2,
     padded_vocab_size=65024,
     hidden_size=128,
     num_attention_heads=8,
     multi_query_attention=True,
     multi_query_group_num=2,
     kv_channels=16,
     rmsnorm=True,
     original_rope=True,
     use_cache=True,
     torch_dtype=torch.float32,
 )
 
-model_zoo.register(
-    name="transformers_chatglm",
-    model_fn=lambda: ChatGLMModel(config, empty_init=False),
-    data_gen_fn=data_gen,
-    output_transform_fn=output_transform_fn,
-    loss_fn=loss_fn_for_chatglm_model,
-    model_attribute=ModelAttribute(has_control_flow=True),
-)
+
+def init_chatglm():
+    model = AutoModelForCausalLM.from_config(config, empty_init=False, trust_remote_code=True)
+    for m in model.modules():
+        if m.__class__.__name__ == "RMSNorm":
+            init.ones_(m.weight)
+    return model
+
 
 model_zoo.register(
     name="transformers_chatglm_for_conditional_generation",
-    model_fn=lambda: ChatGLMForConditionalGeneration(config, empty_init=False),
+    model_fn=init_chatglm,
     data_gen_fn=data_gen_for_conditional_generation,
     output_transform_fn=output_transform_fn,
     loss_fn=loss_fn,
     model_attribute=ModelAttribute(has_control_flow=True),
 )
```

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/falcon.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/falcon.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/gpt.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/gptj.py`

 * *Files 24% similar despite different names*

```diff
@@ -9,16 +9,17 @@
 # Register single-sentence GPT
 # ===============================
 
 
 def data_gen():
     # Generated from following code snippet
     #
-    # from transformers import GPT2Tokenizer
+    # from transformers import AutoTokenizer
     # input = 'Hello, my dog is cute is cute' (last two words repeated to satisfy length requirement)
+    # tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")
     # tokenized_input = tokenizer(input, return_tensors='pt')
     # input_ids = tokenized_input['input_ids']
     # attention_mask = tokenized_input['attention_mask']
     input_ids = torch.tensor([[15496, 11, 616, 3290, 318, 13779, 318, 13779]], dtype=torch.int64)
     attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.int64)
     return dict(input_ids=input_ids, attention_mask=attention_mask)
 
@@ -38,122 +39,74 @@
     start_positions = torch.tensor([0], dtype=torch.int64)
     data["start_positions"] = start_positions
     end_positions = torch.tensor([1], dtype=torch.int64)
     data["end_positions"] = end_positions
     return data
 
 
-def data_gen_for_token_classification():
-    # token classification data gen
-    # `labels` is the type not the token id for token classification, 0 or 1
-    data = data_gen()
-    data["labels"] = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 1]], dtype=torch.int64)
-    return data
-
-
 def data_gen_for_sequence_classification():
     # sequence classification data gen
     data = data_gen()
     data["labels"] = torch.tensor([1], dtype=torch.int64)
     return data
 
 
-def date_gen_for_double_heads():
-    num_choices = 2
-    batch_size = 2
-    input_ids = torch.tensor(
-        [[15496, 11, 616, 3290, 318, 13779, 318, 13779], [15496, 11, 616, 3290, 318, 13779, 318, 13779]],
-        dtype=torch.int64,
-    )
-    attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.int64)
-    mc_labels = torch.zeros(input_ids.shape[0], dtype=torch.int64)
-
-    mc_token_ids = torch.arange(0, num_choices, dtype=torch.int64)
-    mc_token_ids = mc_token_ids.expand((batch_size, num_choices))
-    multiple_choice_inputs_ids = input_ids.unsqueeze(1).expand(-1, num_choices, -1).contiguous()
-    multiple_choice_input_mask = attention_mask.unsqueeze(1).expand(-1, num_choices, -1).contiguous()
-
-    inputs = {
-        "input_ids": multiple_choice_inputs_ids,
-        "mc_token_ids": mc_token_ids,
-        "attention_mask": multiple_choice_input_mask,
-        "labels": multiple_choice_inputs_ids,
-        "mc_labels": mc_labels,
-    }
-    return inputs
-
-
 # define output transform function
 output_transform_fn = lambda x: x
 
 # define loss function
-loss_fn_for_gpt2_model = lambda x: torch.nn.functional.mse_loss(
-    x["last_hidden_state"], torch.ones_like(x["last_hidden_state"])
+loss_fn_for_gptj_model = lambda x: torch.nn.functional.mse_loss(
+    x.last_hidden_state, torch.ones_like(x.last_hidden_state)
 )
-loss_fn = lambda x: x["loss"]
+loss_fn = lambda x: x.loss
 
-config = transformers.GPT2Config(
+config = transformers.GPTJConfig(
     n_layer=2,
     n_head=4,
-    n_embd=128,
     vocab_size=50258,
+    n_embd=256,
+    hidden_size=256,
+    n_positions=512,
     attn_pdrop=0,
     embd_pdrop=0,
     resid_pdrop=0,
-    summary_first_dropout=0,
     hidden_dropout=0,
     problem_type="single_label_classification",
     pad_token_id=50256,
 )
 
 config_for_token_classification = copy.deepcopy(config)
 config_for_token_classification.num_labels = 2
 
 # register the following models
 model_zoo.register(
-    name="transformers_gpt",
-    model_fn=lambda: transformers.GPT2Model(config),
+    name="transformers_gptj",
+    model_fn=lambda: transformers.GPTJModel(config),
     data_gen_fn=data_gen,
     output_transform_fn=output_transform_fn,
-    loss_fn=loss_fn_for_gpt2_model,
+    loss_fn=loss_fn_for_gptj_model,
     model_attribute=ModelAttribute(has_control_flow=True),
 )
 model_zoo.register(
-    name="transformers_gpt_lm",
-    model_fn=lambda: transformers.GPT2LMHeadModel(config),
+    name="transformers_gptj_lm",
+    model_fn=lambda: transformers.GPTJForCausalLM(config),
     data_gen_fn=data_gen_for_lm,
     output_transform_fn=output_transform_fn,
     loss_fn=loss_fn,
     model_attribute=ModelAttribute(has_control_flow=True),
 )
 model_zoo.register(
-    name="transformers_gpt_double_heads",
-    model_fn=lambda: transformers.GPT2DoubleHeadsModel(config),
-    data_gen_fn=date_gen_for_double_heads,
-    output_transform_fn=output_transform_fn,
-    loss_fn=lambda x: x.loss + x.mc_loss,
-    model_attribute=ModelAttribute(has_control_flow=True),
-)
-model_zoo.register(
-    name="transformers_gpt_for_question_answering",
-    model_fn=lambda: transformers.GPT2ForQuestionAnswering(config),
+    name="transformers_gptj_for_question_answering",
+    model_fn=lambda: transformers.GPTJForQuestionAnswering(config),
     data_gen_fn=data_gen_for_question_answering,
     output_transform_fn=output_transform_fn,
     loss_fn=loss_fn,
     model_attribute=ModelAttribute(has_control_flow=True),
 )
 model_zoo.register(
-    name="transformers_gpt_for_token_classification",
-    model_fn=lambda: transformers.GPT2ForTokenClassification(config_for_token_classification),
-    data_gen_fn=data_gen_for_token_classification,
-    output_transform_fn=output_transform_fn,
-    loss_fn=loss_fn,
-    model_attribute=ModelAttribute(has_control_flow=True),
-)
-model_zoo.register(
-    name="transformers_gpt_for_sequence_classification",
-    model_fn=lambda: transformers.GPT2ForSequenceClassification(config_for_token_classification),
+    name="transformers_gptj_for_sequence_classification",
+    model_fn=lambda: transformers.GPTJForSequenceClassification(config_for_token_classification),
     data_gen_fn=data_gen_for_sequence_classification,
     output_transform_fn=output_transform_fn,
     loss_fn=loss_fn,
     model_attribute=ModelAttribute(has_control_flow=True),
 )
```

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/gptj.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/mistral.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,112 +1,81 @@
-import copy
-
 import torch
 import transformers
+from transformers import MistralConfig
 
 from ..registry import ModelAttribute, model_zoo
 
 # ===============================
-# Register single-sentence GPT
+# Register single-sentence Mistral
 # ===============================
 
 
 def data_gen():
     # Generated from following code snippet
     #
-    # from transformers import AutoTokenizer
-    # input = 'Hello, my dog is cute is cute' (last two words repeated to satisfy length requirement)
-    # tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-j-6B")
-    # tokenized_input = tokenizer(input, return_tensors='pt')
+    # from transformers import AutoModelForCausalLM, AutoTokenizer
+    # tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
+    # input = 'My favourite condiment is vinegar' (last two words repeated to satisfy length requirement)
+    # tokenized_input = tokenizer([input], return_tensors="pt")
     # input_ids = tokenized_input['input_ids']
     # attention_mask = tokenized_input['attention_mask']
-    input_ids = torch.tensor([[15496, 11, 616, 3290, 318, 13779, 318, 13779]], dtype=torch.int64)
+    input_ids = torch.tensor([[1, 1984, 16020, 2076, 2487, 349, 21375, 4749]], dtype=torch.int64)
     attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.int64)
     return dict(input_ids=input_ids, attention_mask=attention_mask)
 
 
 def data_gen_for_lm():
     # LM data gen
     # the `labels` of LM is the token of the output, cause no padding, use `input_ids` as `labels`
     data = data_gen()
     data["labels"] = data["input_ids"].clone()
     return data
 
 
-def data_gen_for_question_answering():
-    # question answering data gen
-    # `labels` is the type not the token id for token classification, 0 or 1
-    data = data_gen()
-    start_positions = torch.tensor([0], dtype=torch.int64)
-    data["start_positions"] = start_positions
-    end_positions = torch.tensor([1], dtype=torch.int64)
-    data["end_positions"] = end_positions
-    return data
-
-
 def data_gen_for_sequence_classification():
     # sequence classification data gen
     data = data_gen()
     data["labels"] = torch.tensor([1], dtype=torch.int64)
     return data
 
 
 # define output transform function
 output_transform_fn = lambda x: x
 
 # define loss function
-loss_fn_for_gptj_model = lambda x: torch.nn.functional.mse_loss(
+loss_fn_for_mistral_model = lambda x: torch.nn.functional.mse_loss(
     x.last_hidden_state, torch.ones_like(x.last_hidden_state)
 )
 loss_fn = lambda x: x.loss
+loss_fn_for_seq_classification = lambda output: output.logits.mean()
 
-config = transformers.GPTJConfig(
-    n_layer=2,
-    n_head=4,
-    vocab_size=50258,
-    n_embd=256,
-    hidden_size=256,
-    n_positions=512,
-    attn_pdrop=0,
-    embd_pdrop=0,
-    resid_pdrop=0,
-    hidden_dropout=0,
-    problem_type="single_label_classification",
-    pad_token_id=50256,
+config = MistralConfig(
+    hidden_size=256, intermediate_size=256, num_attention_heads=64, num_hidden_layers=2, vocab_size=50258
 )
 
-config_for_token_classification = copy.deepcopy(config)
-config_for_token_classification.num_labels = 2
+if hasattr(config, "pad_token_id"):
+    config.pad_token_id = config.eos_token_id
 
-# register the following models
 model_zoo.register(
-    name="transformers_gptj",
-    model_fn=lambda: transformers.GPTJModel(config),
+    name="transformers_mistral",
+    model_fn=lambda: transformers.MistralModel(config),
     data_gen_fn=data_gen,
     output_transform_fn=output_transform_fn,
-    loss_fn=loss_fn_for_gptj_model,
+    loss_fn=loss_fn_for_mistral_model,
     model_attribute=ModelAttribute(has_control_flow=True),
 )
 model_zoo.register(
-    name="transformers_gptj_lm",
-    model_fn=lambda: transformers.GPTJForCausalLM(config),
+    name="transformers_mistral_for_casual_lm",
+    model_fn=lambda: transformers.MistralForCausalLM(config),
     data_gen_fn=data_gen_for_lm,
     output_transform_fn=output_transform_fn,
     loss_fn=loss_fn,
     model_attribute=ModelAttribute(has_control_flow=True),
 )
 model_zoo.register(
-    name="transformers_gptj_for_question_answering",
-    model_fn=lambda: transformers.GPTJForQuestionAnswering(config),
-    data_gen_fn=data_gen_for_question_answering,
-    output_transform_fn=output_transform_fn,
-    loss_fn=loss_fn,
-    model_attribute=ModelAttribute(has_control_flow=True),
-)
-model_zoo.register(
-    name="transformers_gptj_for_sequence_classification",
-    model_fn=lambda: transformers.GPTJForSequenceClassification(config_for_token_classification),
+    name="transformers_mistral_for_sequence_classification",
+    model_fn=lambda: transformers.MistralForSequenceClassification(config),
     data_gen_fn=data_gen_for_sequence_classification,
     output_transform_fn=output_transform_fn,
-    loss_fn=loss_fn,
+    loss_fn=loss_fn_for_seq_classification,
     model_attribute=ModelAttribute(has_control_flow=True),
 )
```

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/llama.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/opt.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,90 +1,92 @@
 import torch
 import transformers
 
 from ..registry import ModelAttribute, model_zoo
 
-try:
-    from transformers import LlamaConfig
-
-    HAS_LLAMA = True
-except ImportError:
-    HAS_LLAMA = False
-
-if HAS_LLAMA:
-    # ===============================
-    # Register LLaMA
-    # ===============================
-
-    def data_gen():
-        # the input ids are corresponding to the sentence
-        # 'Hello, my dog is cute'
-        #
-        # the code is give below:
-        # -----------------------------------
-        # from transformers import LlamaTokenizerFast
-        # tokenizer = LlamaTokenizerFast.from_pretrained("hf-internal-testing/llama-tokenizer")
-        # input = 'Hello, my dog is cute'
-        # tokenized_input = tokenizer(input, return_tensors='pt').to('cuda')
-        # -----------------------------------
-
-        input_ids = torch.Tensor(
-            [[1, 15043, 29892, 590, 11203, 338, 274, 1082], [1, 15043, 29892, 590, 11203, 338, 274, 1082]]
-        ).long()
-        attention_mask = torch.Tensor([[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]).long()
-        return dict(input_ids=input_ids, attention_mask=attention_mask)
-
-    # label is needed for casual lm
-    def data_gen_for_casual_lm():
-        data = data_gen()
-        labels = data["input_ids"].clone()
-        data["labels"] = labels
-        return data
-
-    # transform the output to a dict
-    output_transform_fn = lambda x: x
-
-    # function to get the loss
-    loss_fn = lambda output: output["last_hidden_state"].mean()
-    loss_fn_for_casual_lm = lambda output: output["loss"]
-    loss_fn_for_seq_classification = lambda output: output["logits"].mean()
-
-    config = LlamaConfig(
-        num_hidden_layers=4,
-        hidden_size=128,
-        intermediate_size=256,
-        num_attention_heads=4,
-        max_position_embeddings=128,
-        num_labels=16,
-    )
-
-    if hasattr(config, "pad_token_id"):
-        config.pad_token_id = config.eos_token_id
-
-    # register the following models
-    # transformers.LlamaModel,
-    # transformers.LlamaForCausalLM,
-    # transformers.LlamaForSequenceClassification,
-    model_zoo.register(
-        name="transformers_llama",
-        model_fn=lambda: transformers.LlamaModel(config),
-        data_gen_fn=data_gen,
-        output_transform_fn=output_transform_fn,
-        loss_fn=loss_fn,
-        model_attribute=ModelAttribute(has_control_flow=True),
-    )
-    model_zoo.register(
-        name="transformers_llama_for_casual_lm",
-        model_fn=lambda: transformers.LlamaForCausalLM(config),
-        data_gen_fn=data_gen_for_casual_lm,
-        output_transform_fn=output_transform_fn,
-        loss_fn=loss_fn_for_casual_lm,
-        model_attribute=ModelAttribute(has_control_flow=True),
-    )
-    model_zoo.register(
-        name="transformers_llama_for_sequence_classification",
-        model_fn=lambda: transformers.LlamaForSequenceClassification(config),
-        data_gen_fn=data_gen,
-        output_transform_fn=output_transform_fn,
-        loss_fn=loss_fn_for_seq_classification,
-        model_attribute=ModelAttribute(has_control_flow=True),
-    )
+# ===============================
+# Register single-sentence OPT
+# ===============================
+BATCH_SIZE = 2
+SEQ_LENGTH = 16
+
+
+def data_gen():
+    input_ids = torch.Tensor([[1, 15043, 29892, 590, 11203, 338, 274, 1082]]).long()
+    attention_mask = torch.Tensor([[1, 1, 1, 1, 1, 1, 1, 1]]).long()
+    return dict(input_ids=input_ids, attention_mask=attention_mask)
+
+
+def data_gen_for_causal_lm():
+    # LM data gen
+    # the `labels` of LM is the token of the output, cause no padding, use `input_ids` as `labels`
+    data = data_gen()
+    labels = data["input_ids"].clone()
+    data["labels"] = labels
+    return data
+
+
+def data_gen_for_sequence_classification():
+    # LM data gen
+    # the `labels` of LM is the token of the output, cause no padding, use `input_ids` as `labels`
+    data = data_gen()
+    data["input_ids"].clone()
+    data["labels"] = torch.tensor([1])
+    return data
+
+
+def data_gen_for_question_answering():
+    # LM data gen
+    # the `labels` of LM is the token of the output, cause no padding, use `input_ids` as `labels`
+    data = data_gen()
+    data["start_positions"] = torch.tensor([0])
+    data["end_positions"] = torch.tensor([1])
+    return data
+
+
+output_transform_fn = lambda x: x
+loss_fn_for_opt_model = lambda x: torch.nn.functional.mse_loss(
+    x["last_hidden_state"], torch.ones_like(x["last_hidden_state"])
+)
+loss_fn_for_lm = lambda x: x["loss"]
+config = transformers.OPTConfig(
+    hidden_size=128,
+    num_hidden_layers=2,
+    num_attention_heads=4,
+    dropout=0,
+)
+
+# register the following models
+# transformers.OPTModel,
+# transformers.OPTForCausalLM,
+model_zoo.register(
+    name="transformers_opt",
+    model_fn=lambda: transformers.OPTModel(config),
+    data_gen_fn=data_gen,
+    output_transform_fn=output_transform_fn,
+    loss_fn=loss_fn_for_opt_model,
+    model_attribute=ModelAttribute(has_control_flow=True),
+)
+model_zoo.register(
+    name="transformers_opt_for_causal_lm",
+    model_fn=lambda: transformers.OPTForCausalLM(config),
+    data_gen_fn=data_gen_for_causal_lm,
+    output_transform_fn=output_transform_fn,
+    loss_fn=loss_fn_for_lm,
+    model_attribute=ModelAttribute(has_control_flow=True),
+)
+model_zoo.register(
+    name="transformers_opt_for_question_answering",
+    model_fn=lambda: transformers.OPTForQuestionAnswering(config),
+    data_gen_fn=data_gen_for_question_answering,
+    output_transform_fn=output_transform_fn,
+    loss_fn=loss_fn_for_lm,
+    model_attribute=ModelAttribute(has_control_flow=True),
+)
+
+# TODO The loss and gradient check in the test are failing, to be fixed.
+# model_zoo.register(name='transformers_opt_for_sequence_classification',
+#                    model_fn=lambda: transformers.OPTForSequenceClassification(config),
+#                    data_gen_fn=data_gen_for_sequence_classification,
+#                    output_transform_fn=output_transform_fn,
+#                    loss_fn=loss_fn_for_lm,
+#                    model_attribute=ModelAttribute(has_control_flow=True))
```

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/mistral.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/t5.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,78 +1,81 @@
 import torch
 import transformers
-from transformers import MistralConfig
 
 from ..registry import ModelAttribute, model_zoo
 
 # ===============================
-# Register single-sentence Mistral
+# Register single-sentence T5
 # ===============================
 
 
-def data_gen():
+# define data gen function
+def data_gen_for_encoder_only():
     # Generated from following code snippet
     #
-    # from transformers import AutoModelForCausalLM, AutoTokenizer
-    # tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
-    # input = 'My favourite condiment is vinegar' (last two words repeated to satisfy length requirement)
-    # tokenized_input = tokenizer([input], return_tensors="pt")
-    # input_ids = tokenized_input['input_ids']
-    # attention_mask = tokenized_input['attention_mask']
-    input_ids = torch.tensor([[1, 1984, 16020, 2076, 2487, 349, 21375, 4749]], dtype=torch.int64)
-    attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.int64)
+    # from transformers import T5Config, T5Tokenizer
+    # config = T5Config(decoder_start_token_id=0)
+    # tokenizer = T5Tokenizer.from_pretrained("t5-small")
+    # input_ids = tokenizer("translate English to German: The house is wonderful.", return_tensors="pt").input_ids
+    input_ids = torch.Tensor([[13959, 1566, 12, 2968, 10, 37, 629, 19, 1627, 5, 1, 12, 1627, 5, 1, 12]]).long()
+    attention_mask = torch.Tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]).long()
     return dict(input_ids=input_ids, attention_mask=attention_mask)
 
 
-def data_gen_for_lm():
-    # LM data gen
-    # the `labels` of LM is the token of the output, cause no padding, use `input_ids` as `labels`
-    data = data_gen()
-    data["labels"] = data["input_ids"].clone()
+def data_gen_for_conditional_generation():
+    # labels is generated with the following code
+    #
+    # labels = tokenizer("Das Haus ist wunderbar.", return_tensors="pt").input_ids
+    data = data_gen_for_encoder_only()
+    labels = torch.Tensor([[644, 4598, 229, 19250, 5, 1, 644, 4598, 229, 19250, 5, 1, 229, 19250, 5, 1]]).long()
+    data["labels"] = labels
     return data
 
 
-def data_gen_for_sequence_classification():
-    # sequence classification data gen
-    data = data_gen()
-    data["labels"] = torch.tensor([1], dtype=torch.int64)
+def data_gen_for_t5_model():
+    # decoder_inputs_ids is obtained with the following code
+    # decoder_input_ids = model._shift_right(input_ids)
+    data = data_gen_for_encoder_only()
+    decoder_input_ids = torch.Tensor([[0, 13959, 1566, 12, 2968, 10, 37, 629, 19, 1627, 5, 5, 19, 1627, 5, 5]]).long()
+    data["decoder_input_ids"] = decoder_input_ids
     return data
 
 
-# define output transform function
+# output transform function
 output_transform_fn = lambda x: x
 
 # define loss function
-loss_fn_for_mistral_model = lambda x: torch.nn.functional.mse_loss(
-    x.last_hidden_state, torch.ones_like(x.last_hidden_state)
-)
-loss_fn = lambda x: x.loss
-loss_fn_for_seq_classification = lambda output: output.logits.mean()
-
-config = MistralConfig(
-    hidden_size=256, intermediate_size=256, num_attention_heads=64, num_hidden_layers=2, vocab_size=50258
-)
-
+loss_fn_for_t5_model = lambda x: x["last_hidden_state"].mean()
+loss_fn_for_encoder_only = lambda x: x["last_hidden_state"].mean()
+loss_fn_for_conditional_generation = lambda x: x["loss"]
+
+# define model config
+config = transformers.T5Config(d_model=128, num_layers=2, dropout_rate=0, decoder_start_token_id=0)
+
+# register the following models
+# transformers.T5Model,
+# transformers.T5ForConditionalGeneration,
+# transformers.T5EncoderModel,
 model_zoo.register(
-    name="transformers_mistral",
-    model_fn=lambda: transformers.MistralModel(config),
-    data_gen_fn=data_gen,
+    name="transformers_t5",
+    model_fn=lambda: transformers.T5Model(config),
+    data_gen_fn=data_gen_for_t5_model,
     output_transform_fn=output_transform_fn,
-    loss_fn=loss_fn_for_mistral_model,
+    loss_fn=loss_fn_for_t5_model,
     model_attribute=ModelAttribute(has_control_flow=True),
 )
 model_zoo.register(
-    name="transformers_mistral_for_casual_lm",
-    model_fn=lambda: transformers.MistralForCausalLM(config),
-    data_gen_fn=data_gen_for_lm,
+    name="transformers_t5_for_conditional_generation",
+    model_fn=lambda: transformers.T5ForConditionalGeneration(config),
+    data_gen_fn=data_gen_for_conditional_generation,
     output_transform_fn=output_transform_fn,
-    loss_fn=loss_fn,
+    loss_fn=loss_fn_for_conditional_generation,
     model_attribute=ModelAttribute(has_control_flow=True),
 )
 model_zoo.register(
-    name="transformers_mistral_for_sequence_classification",
-    model_fn=lambda: transformers.MistralForSequenceClassification(config),
-    data_gen_fn=data_gen_for_sequence_classification,
+    name="transformers_t5_encoder_model",
+    model_fn=lambda: transformers.T5EncoderModel(config),
+    data_gen_fn=data_gen_for_encoder_only,
     output_transform_fn=output_transform_fn,
-    loss_fn=loss_fn_for_seq_classification,
+    loss_fn=loss_fn_for_encoder_only,
     model_attribute=ModelAttribute(has_control_flow=True),
 )
```

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/sam.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/sam.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/vit.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/vit.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/kit/model_zoo/transformers/whisper.py` & `colossalai-nightly-2024.5.4/tests/kit/model_zoo/transformers/whisper.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_analyzer/test_fx/test_bias_addition.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_bias_addition.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_analyzer/test_fx/test_mod_dir.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_mod_dir.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_analyzer/test_fx/test_nested_ckpt.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_nested_ckpt.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_analyzer/test_fx/test_shape_prop.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_shape_prop.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_analyzer/test_fx/test_symbolic_profile.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/test_symbolic_profile.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_analyzer/test_fx/zoo.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_fx/zoo.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_analyzer/test_subclasses/test_aten.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_aten.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_analyzer/test_subclasses/test_flop_tensor.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_flop_tensor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_analyzer/test_subclasses/test_meta_mode.py` & `colossalai-nightly-2024.5.4/tests/test_analyzer/test_subclasses/test_meta_mode.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_pass/test_node_converting_pass.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/test_node_converting_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_pass/test_size_value_converting_pass.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_bias_addition_forward.py`

 * *Files 2% similar despite different names*

```diff
@@ -38,15 +38,15 @@
         x = x * 2
 
         return x
 
 
 def check_linear_module(rank, world_size, port):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = LinearModel(4, 8).cuda()
     input = torch.rand(4, 4).cuda()
     output_compare = model(input)
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     # [[0, 1]
     #  [2, 3]]
@@ -55,15 +55,15 @@
     gm = initialize_model(model, meta_args=meta_args, device_mesh=device_mesh)
     output = gm(input)
     assert_close(output, output_compare)
 
 
 def check_conv_module(rank, world_size, port):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = ConvModel(3, 6, 2).cuda()
     input = torch.rand(4, 3, 64, 64).cuda()
     output_compare = model(input)
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     # [[0, 1]
     #  [2, 3]]
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_broadcast.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_broadcast.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_checkpoint.py`

 * *Files 1% similar despite different names*

```diff
@@ -35,15 +35,15 @@
         hidden_states = self.act(hidden_states)
 
         return hidden_states
 
 
 def check_act_ckpt(rank, world_size, port):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = GPT2MLPWithCkpt(intermediate_size=4 * HIDDEN_SIZE, hidden_size=HIDDEN_SIZE)
     torch.rand(1, 64, HIDDEN_SIZE)
     input_sample = {
         "hidden_states": torch.rand(1, 64, HIDDEN_SIZE).to("meta"),
     }
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_ddp.py`

 * *Files 4% similar despite different names*

```diff
@@ -28,15 +28,15 @@
         x = self.linear_2(x)
 
         return x
 
 
 def check_compatibility_with_ddp(rank, world_size, port):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = MLP(4).cuda()
     if rank in [0, 1]:
         input = torch.arange(0, 16, dtype=torch.float).reshape(4, 4).cuda()
     elif rank in [2, 3]:
         input = torch.arange(16, 32, dtype=torch.float).reshape(4, 4).cuda()
     input_compare = torch.arange(0, 32, dtype=torch.float).reshape(8, 4).cuda()
     output_compare = model(input_compare)
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_compatibility_with_gemini.py`

 * *Files 1% similar despite different names*

```diff
@@ -30,15 +30,15 @@
         x = self.linear_2(x)
 
         return x
 
 
 def check_auto_parallel_with_gemini(rank, world_size, port):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = MLP(4).half().cuda()
     if rank in [0, 1]:
         input = torch.arange(0, 16).reshape(4, 4).half().cuda()
     elif rank in [2, 3]:
         input = torch.arange(16, 32).reshape(4, 4).half().cuda()
     input_compare = torch.arange(0, 32).reshape(8, 4).half().cuda()
     output_compare = model(input_compare)
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_find_repeat_block.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/gpt_modules.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_runtime_with_gpt_modules.py`

 * *Files 2% similar despite different names*

```diff
@@ -69,15 +69,15 @@
                 avg_diff = difference.abs().sum() / difference.numel()
                 assert avg_diff < 0.001
                 print(f"{name} param has {avg_diff} average difference")
 
 
 def check_attention_layer(rank, model_cls, world_size, port):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
 
     config = transformers.GPT2Config(n_position=64, n_layer=2, n_head=16, n_embd=HIDDEN_DIM)
 
     if model_cls == GPT2MLP:
         model = model_cls(intermediate_size=4 * config.hidden_size, config=config).to("cuda")
     else:
         model = model_cls(config=config).to("cuda")
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_gpt/test_solver_with_gpt_module.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_liveness_analysis.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addbmm_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -36,15 +36,15 @@
         else:
             output = torch.addbmm(bias, x1, x2)
         return output
 
 
 def check_2d_device_mesh(rank, world_size, port, module, bias_shape, using_kwargs):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = module(using_kwargs).cuda()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     x1 = torch.rand(4, 8, 16).cuda()
     x2 = torch.rand(4, 16, 8).cuda()
     bias = torch.rand(bias_shape).cuda()
@@ -146,15 +146,15 @@
         assert input_sharding_spec.sharding_sequence[0] == output_sharding_spec.sharding_sequence[0]
         assert other_sharding_spec.sharding_sequence[1] == input_sharding_spec.sharding_sequence[-1]
         assert other_sharding_spec.sharding_sequence[-1] == output_sharding_spec.sharding_sequence[-1]
 
 
 def check_1d_device_mesh(rank, module, bias_shape, using_kwargs, world_size, port):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (1, 4)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     model = module(using_kwargs).cuda()
     x1 = torch.rand(4, 8, 16).cuda()
     x2 = torch.rand(4, 16, 8).cuda()
     bias = torch.rand(bias_shape).cuda()
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addmm_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_addmm_handler.py`

 * *Files 0% similar despite different names*

```diff
@@ -36,15 +36,15 @@
     def forward(self, m1):
         x = torch.addmm(self.bias, m1, self.weight, beta=3, alpha=2)
         return x
 
 
 def check_addmm_function_handler(rank, world_size, port, input_shape, model_cls):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     if model_cls == AddmmModel:
         model = AddmmModel().cuda()
     else:
         model = AddmmModel_with_param(weight_shape=(8, 16), bias_shape=input_shape).cuda()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_batch_norm_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_batch_norm_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 from colossalai.logging import disable_existing_loggers
 from colossalai.testing import rerun_if_address_is_in_use, run_on_environment_flag, spawn
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 def check_bn_module_handler(rank, world_size, port):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = nn.Sequential(nn.BatchNorm2d(16)).cuda()
 
     physical_mesh_id = torch.arange(0, 4)
 
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     input = torch.rand(4, 16, 64, 64).cuda()
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_function_node.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_function_node.py`

 * *Files 0% similar despite different names*

```diff
@@ -30,15 +30,15 @@
     def forward(self, x):
         x = F.linear(x, self.weight, bias=self.bias)
         return x
 
 
 def check_linear_module_handler(rank, world_size, port):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = LinearModule(weight_shape=WEIGHT_SHAPE).cuda()
 
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     input = torch.rand(4, 4, 4, 16).cuda()
     # the index of linear node in computation graph
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_module_node.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bias_linear_module_node.py`

 * *Files 1% similar despite different names*

```diff
@@ -26,15 +26,15 @@
     def forward(self, x):
         x = self.linear(x)
         return x
 
 
 def check_linear_module_handler(rank, world_size, port, bias):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = LinearModule(16, 32, bias=bias).cuda()
 
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     input = torch.rand(4, 4, 4, 16).cuda()
     # the index of linear node in computation graph
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_binary_elementwise_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_binary_elementwise_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 from colossalai.logging import disable_existing_loggers
 from colossalai.testing import parameterize, rerun_if_address_is_in_use, run_on_environment_flag, spawn
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 def check_binary_elementwise_handler_with_tensor(rank, world_size, port, op, other_dim):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
 
     class BinaryElementwiseOpModel(nn.Module):
         def __init__(self, op):
             super().__init__()
             self.op = op
 
         def forward(self, x1, x2):
@@ -141,15 +141,15 @@
     def forward(self, x1):
         out = self.op(x1, self.const)
         return out
 
 
 def check_binary_elementwise_handler_with_int(rank, world_size, port, op, other_dim, model_cls):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
 
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     if model_cls == BEOpModelWithNodeConst:
         model = model_cls(op).cuda()
     else:
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bmm_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_bmm_handler.py`

 * *Files 1% similar despite different names*

```diff
@@ -22,15 +22,15 @@
 class BMMTorchFunctionModule(nn.Module):
     def forward(self, x1, x2):
         return torch.bmm(x1, x2)
 
 
 def check_2d_device_mesh(rank, module, world_size, port):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = module().cuda()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     x1 = torch.rand(4, 8, 16).cuda()
     x2 = torch.rand(4, 16, 8).cuda()
     # the index of bmm node in computation graph
@@ -117,15 +117,15 @@
         assert input_sharding_spec.sharding_sequence[:-1] == output_sharding_spec.sharding_sequence[:-1]
         assert other_sharding_spec.sharding_sequence[1] == input_sharding_spec.sharding_sequence[-1]
         assert other_sharding_spec.sharding_sequence[-1] == output_sharding_spec.sharding_sequence[-1]
 
 
 def check_1d_device_mesh(rank, module, world_size, port):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = module().cuda()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (1, 4)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     x1 = torch.rand(4, 8, 16).cuda()
     x2 = torch.rand(4, 16, 8).cuda()
     # the index of bmm node in computation graph
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_conv_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_conv_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 from colossalai.logging import disable_existing_loggers
 from colossalai.testing import rerun_if_address_is_in_use, run_on_environment_flag, spawn
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 def check_conv_module_handler(rank, world_size, port, bias):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = nn.Sequential(nn.Conv2d(4, 16, 3, padding=1, bias=bias)).cuda()
     # graph():
     #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
     #     %_0 : [#users=1] = call_module[target=0](args = (%input_1,), kwargs = {})
     #     return _0
     input = torch.rand(4, 4, 64, 64).cuda()
 
@@ -149,15 +149,15 @@
     def forward(self, input, others, bias=None):
         x = nn.functional.conv2d(input, others, bias=bias, padding=1)
         return x
 
 
 def check_conv_function_handler(rank, world_size, port, bias):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = ConvModel().cuda()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     input = torch.rand(4, 4, 64, 64).cuda()
     others = torch.rand(16, 4, 3, 3).cuda()
     input_args = [input, others]
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_default_reshape_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_default_reshape_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_embedding_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_embedding_handler.py`

 * *Files 2% similar despite different names*

```diff
@@ -29,15 +29,15 @@
     def forward(self, input):
         x = self.embedding(input)
         return x
 
 
 def check_embedding_module_handler(rank, world_size, port):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = EmbeddingModule(num_embeddings=NUM_EMBEDDINGS, embedding_dims=EMBEDDING_DIMS).cuda()
     # graph():
     #     %input_1 : torch.Tensor [#users=1] = placeholder[target=input]
     #     %embedding : [#users=1] = call_module[target=embedding](args = (%input_1,), kwargs = {})
     #     return embedding
     input = torch.rand(4, 16, 16) * NUM_EMBEDDINGS
     input = input.to(torch.int64).cuda()
@@ -146,15 +146,15 @@
     def forward(self, input, others):
         x = nn.functional.embedding(input, others)
         return x
 
 
 def check_embedding_function_handler(rank, world_size, port):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = EmbeddingFunction().cuda()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     input = torch.rand(4, 16, 16) * NUM_EMBEDDINGS
     input = input.to(torch.int64).cuda()
     others = torch.rand(NUM_EMBEDDINGS, EMBEDDING_DIMS).cuda()
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getattr_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getattr_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getitem_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_getitem_handler.py`

 * *Files 1% similar despite different names*

```diff
@@ -27,15 +27,15 @@
         linear_node = nn.functional.linear(input, other, bias=None)
         x = linear_node[self.getitem_index]
         return x
 
 
 def check_getitem_from_tensor_handler(rank, getitem_index, world_size, port):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
 
     model = GetItemFromTensorModel(getitem_index=getitem_index)
 
     input = torch.rand(8, 16, 64, 32).to("cuda")
     other = torch.rand(64, 32).to("cuda")
     # index of linear node in computation graph
     node_index = 2
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_layer_norm_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_layer_norm_handler.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 from colossalai.testing import rerun_if_address_is_in_use, spawn
 from colossalai.testing.pytest_wrapper import run_on_environment_flag
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 def check_ln_module_handler(rank, world_size, port):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = nn.Sequential(nn.LayerNorm(16)).cuda()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     input = torch.rand(4, 16).cuda()
     # the index of bn node in computation graph
     node_index = 1
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_linear_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_linear_handler.py`

 * *Files 1% similar despite different names*

```diff
@@ -19,15 +19,15 @@
 from colossalai.testing.pytest_wrapper import run_on_environment_flag
 from colossalai.testing.utils import parameterize
 from tests.test_auto_parallel.test_tensor_shard.test_node_handler.utils import numerical_test_for_node_strategy
 
 
 def check_linear_module_handler(rank, world_size, port, bias, input_shape):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = nn.Sequential(nn.Linear(16, 32, bias=bias)).cuda()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
     input = torch.rand(input_shape).cuda()
     # the index of linear node in computation graph
     node_index = 1
@@ -167,15 +167,15 @@
     def forward(self, input, others, bias=None):
         x = nn.functional.linear(input, others, bias=bias)
         return x
 
 
 def check_linear_function_handler(rank, world_size, port, bias, input_shape):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = LinearModel().cuda()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
 
     input = torch.rand(input_shape).cuda()
     other = torch.rand(32, 16).cuda()
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_matmul_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_matmul_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_norm_pooling_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_norm_pooling_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_output_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_output_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_permute_and_transpose_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_permute_and_transpose_handler.py`

 * *Files 1% similar despite different names*

```diff
@@ -47,15 +47,15 @@
         else:
             permute_node = self.call_function(linear_node, *self.reshape_dims)
         return permute_node
 
 
 def check_view_handler(rank, world_size, port, call_function, reshape_dims, model_cls):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     if call_function == torch.permute:
         reshape_dims = reshape_dims[0]
     elif call_function == torch.transpose:
         reshape_dims = reshape_dims[1]
     model = model_cls(reshape_dims, call_function).cuda()
 
     if model_cls.__name__ == "ConvReshapeModel":
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_placeholder_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_placeholder_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_shard_option.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_shard_option.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_softmax_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_softmax_handler.py`

 * *Files 0% similar despite different names*

```diff
@@ -25,15 +25,15 @@
         linear_node = F.linear(input, other, bias=None)
         softmax_node = F.softmax(linear_node, self.softmax_dim)
         return softmax_node
 
 
 def check_split_handler(rank, world_size, port, softmax_dim, model_cls):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = model_cls(softmax_dim=softmax_dim).cuda()
 
     input = torch.rand(8, 16, 64, 32).to("cuda")
     other = torch.rand(64, 32).to("cuda")
     # index of linear node in computation graph
     node_index = 2
     # total number of linear strategies
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_split_handler.py`

 * *Files 0% similar despite different names*

```diff
@@ -38,15 +38,15 @@
         linear_node = nn.functional.linear(input, other, bias=None)
         split_node = linear_node.split(self.split_size, dim=self.split_dim)
         return split_node
 
 
 def check_split_handler(rank, world_size, port, split_size, split_dim, model_cls):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = model_cls(split_size=split_size, split_dim=split_dim).cuda()
 
     if model_cls.__name__ == "ConvSplitModel":
         input = torch.rand(8, 8, 66, 66).to("cuda")
         other = torch.rand(16, 8, 3, 3).to("cuda")
         # index of conv node in computation graph
         node_index = 2
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_sum_handler.py`

 * *Files 0% similar despite different names*

```diff
@@ -28,15 +28,15 @@
         else:
             sum_node = torch.sum(linear_node, keepdim=self.keepdim)
         return sum_node
 
 
 def check_sum_handler(rank, world_size, port, sum_dims, keepdim):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = LinearSumModel(sum_dims=sum_dims, keepdim=keepdim).cuda()
     physical_mesh_id = torch.arange(0, 4)
     mesh_shape = (2, 2)
     device_mesh = DeviceMesh(physical_mesh_id, mesh_shape, init_process_group=True)
 
     input = torch.rand(8, 16, 64, 32).to("cuda")
     other = torch.rand(64, 32).to("cuda")
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_tensor_constructor.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_unary_element_wise_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_view_handler.py`

 * *Files 0% similar despite different names*

```diff
@@ -37,15 +37,15 @@
         linear_node = nn.functional.linear(input, other, bias=None)
         reshape_node = linear_node.view(*self.tgt_shape)
         return reshape_node
 
 
 def check_view_handler(rank, tgt_shape, model_cls, world_size, port):
     disable_existing_loggers()
-    launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     model = model_cls(tgt_shape).cuda()
 
     if model_cls.__name__ == "ConvViewModel":
         input = torch.rand(8, 8, 66, 66).to("cuda")
         other = torch.rand(16, 8, 3, 3).to("cuda")
         # index of conv node in computation graph
         node_index = 2
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/test_where_handler.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_node_handler/utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py` & `colossalai-nightly-2024.5.4/tests/test_auto_parallel/test_tensor_shard/test_solver_with_resnet_v2.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/_utils.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 import copy
-import math
 from contextlib import nullcontext
 from typing import Any, Callable, Dict, List, Optional
 
 import torch
 import torch.distributed as dist
 from torch import Tensor
 from torch import distributed as dist
@@ -11,30 +10,33 @@
 from torch.nn import Module
 from torch.optim import Adam, Optimizer
 from torch.testing import assert_close
 
 from colossalai.booster import Booster
 from colossalai.booster.plugin import HybridParallelPlugin
 from colossalai.booster.plugin.hybrid_parallel_plugin import HybridParallelModule
+from colossalai.checkpoint_io.utils import gather_distributed_param
 from colossalai.lazy import LazyInitContext
 from colossalai.pipeline.stage_manager import PipelineStageManager
 from colossalai.shardformer import ShardConfig, ShardFormer
 from colossalai.shardformer._utils import getattr_
 from colossalai.shardformer.policies.auto_policy import Policy
 from colossalai.tensor.d_tensor.api import is_customized_distributed_tensor, is_distributed_tensor
+from colossalai.tensor.padded_tensor.api import is_padded_tensor, to_unpadded_tensor
 
 
 def build_model(
     model_fn,
     enable_fused_normalization=True,
     enable_tensor_parallelism=True,
     enable_flash_attention=False,
     enable_jit_fused=False,
     enable_sequence_parallelism=False,
     use_lazy_init: bool = False,
+    dtype=torch.float32,
 ):
     # create new model
     ctx = LazyInitContext() if use_lazy_init else nullcontext()
     with ctx:
         # create new model
         org_model = model_fn()
         model_copy = copy.deepcopy(org_model)
@@ -47,15 +49,15 @@
         enable_flash_attention=enable_flash_attention,
         enable_jit_fused=enable_jit_fused,
         enable_sequence_parallelism=enable_sequence_parallelism,
     )
     model_copy = copy.deepcopy(org_model)
     shard_former = ShardFormer(shard_config=shard_config)
     sharded_model, shared_params = shard_former.optimize(model_copy)
-    return org_model.cuda(), sharded_model.cuda()
+    return org_model.cuda().to(dtype), sharded_model.cuda().to(dtype)
 
 
 def build_pipeline_model(
     model_fn,
     stage_manager=None,
     enable_fused_normalization=False,
     enable_tensor_parallelism=False,
@@ -118,25 +120,31 @@
 
     ctx = LazyInitContext() if use_lazy_init else nullcontext()
     with ctx:
         org_model = model_fn()
         sharded_model = copy.deepcopy(org_model)
     if use_lazy_init:
         ctx.materialize(org_model)
-
     org_model = org_model.cuda()
     org_optimizer = Adam(org_model.parameters(), lr=1e-3)
     sharded_optimizer = Adam(sharded_model.parameters(), lr=1e-3)
     criterion = loss_fn
 
     plugin = HybridParallelPlugin(**test_config)
     booster = Booster(plugin=plugin)
 
     sharded_model, sharded_optimizer, criterion, _, _ = booster.boost(sharded_model, sharded_optimizer, criterion)
-    return org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster
+    return (
+        org_model,
+        org_optimizer,
+        sharded_model,
+        sharded_optimizer,
+        criterion,
+        booster,
+    )
 
 
 def run_forward_backward_with_hybrid_plugin(
     org_model: Module,
     sharded_model: Module,
     sharded_optimizer: Optimizer,
     data_gen_fn: Callable,
@@ -150,73 +158,80 @@
     def _criterion(outputs, inputs):
         outputs = output_transform_fn(outputs)
         loss = criterion(outputs)
         return loss
 
     data = data_gen_fn()
 
-    if booster.plugin.shard_config.enable_sequence_parallelism and booster.plugin.tp_size != 0:
-        seq_len = data["input_ids"].shape[-1]
-        lcm = booster.plugin.tp_size * seq_len // math.gcd(booster.plugin.tp_size, seq_len)
-        times = lcm // seq_len
-        input_shape = data["input_ids"].shape
-        for k, v in data.items():
-            if v.shape == input_shape:
-                data[k] = v.repeat((1,) * (v.dim() - 1) + (times,))
+    shard_test_data = {}
+    for k, v in data.items():
+        shard_test_data[k] = data[k].clone()
+    unshard_test_data = {}
+    for k, v in data.items():
+        unshard_test_data[k] = data[k].clone()
 
     sharded_model.train()
     if booster.plugin.stage_manager is not None:
-        for k, v in data.items():
+        for k, v in shard_test_data.items():
             if torch.is_tensor(v) or "Tensor" in v.__class__.__name__:
                 new_shape = [1] * v.dim()
                 new_shape[0] = 4
-                data[k] = v.to("cuda").repeat(*new_shape)
+                shard_test_data[k] = v.to("cuda").repeat(*new_shape)
 
-        data_iter = iter([data])
+        data_iter = iter([shard_test_data])
         sharded_output = booster.execute_pipeline(
-            data_iter, sharded_model, _criterion, sharded_optimizer, return_loss=True, return_outputs=True
+            data_iter,
+            sharded_model,
+            _criterion,
+            sharded_optimizer,
+            return_loss=True,
+            return_outputs=True,
         )
         sharded_loss = sharded_output["loss"]
-    else:
-        data = {k: v.cuda() for k, v in data.items()}
-        sharded_output = sharded_model(**data)
 
+    else:
+        shard_test_data = {k: v.cuda() for k, v in shard_test_data.items()}
+        sharded_output = sharded_model(**shard_test_data)
         sharded_loss = criterion(sharded_output)
         sharded_optimizer.backward(sharded_loss)
 
     org_model.train()
-    data = {k: v.cuda() for k, v in data.items()}
-    org_output = org_model(**data)
-
+    if booster.plugin.stage_manager is not None:
+        for k, v in unshard_test_data.items():
+            if torch.is_tensor(v) or "Tensor" in v.__class__.__name__:
+                new_shape = [1] * v.dim()
+                new_shape[0] = 4
+                unshard_test_data[k] = v.to("cuda").repeat(*new_shape)
+    unshard_test_data = {k: v.cuda() for k, v in unshard_test_data.items()}
+    org_output = org_model(**unshard_test_data)
     org_loss = criterion(org_output)
     org_loss.backward()
 
     return org_loss, org_output, sharded_loss, sharded_output
 
 
 def check_output_hidden_state(
     org_output: Tensor,
     sharded_output: Tensor,
     stage_manager: Optional[PipelineStageManager] = None,
     atol: float = 1e-5,
     rtol: float = 1e-3,
-    dim: int = 0,
 ):
     org_hidden_state = org_output.last_hidden_state
 
     if stage_manager and stage_manager.is_last_stage(ignore_chunk=True):
         sharded_hidden_state = sharded_output["outputs"]["last_hidden_state"]
     else:
         sharded_hidden_state = sharded_output.last_hidden_state
 
     assert_close(org_hidden_state.float(), sharded_hidden_state.float(), atol=atol, rtol=rtol)
 
 
 def check_loss(org_loss: Tensor, sharded_loss: Tensor, atol: float = 1e-5, rtol: float = 1e-3):
-    assert torch.allclose(org_loss.float(), sharded_loss.float(), atol=atol, rtol=rtol)
+    assert_close(org_loss.float(), sharded_loss.float(), atol=atol, rtol=rtol)
 
 
 def check_weight(
     org_model: Module,
     sharded_model: Module,
     layer_suffix: List[str],
     tp_group: Optional[ProcessGroup] = None,
@@ -230,19 +245,18 @@
         sharded_weight = getattr_(sharded_model, suffix).weight
 
         # skip if layer is not held by this process
         if sharded_weight is None:
             continue
 
         if is_distributed_tensor(sharded_weight) or is_customized_distributed_tensor(sharded_weight):
-            sharded_weight_list = [
-                torch.zeros_like(sharded_weight).to("cuda") for _ in range(dist.get_world_size(tp_group))
-            ]
-            dist.all_gather(sharded_weight_list, sharded_weight, tp_group)
-            sharded_weight = torch.cat(sharded_weight_list, dim=dim)
+            sharded_weight = gather_distributed_param(sharded_weight, keep_vars=False)
+
+        if is_padded_tensor(sharded_weight):
+            sharded_weight = to_unpadded_tensor(sharded_weight)
 
         if verbose and dist.get_rank() == 0:
             print(f"'{suffix}' weight: {org_weight}, {sharded_weight}")
 
         assert_close(org_weight.float(), sharded_weight.float(), atol=atol, rtol=rtol)
 
 
@@ -309,15 +323,17 @@
         if verbose and dist.get_rank() == 0:
             print(f"'{suffix}' grad: {org_grad}, {shard_grad}")
 
         assert_close(org_grad.float(), shard_grad.float(), rtol=rtol, atol=atol)
 
 
 def unwrap_model(
-    module: Module, base_model_class_name: Optional[str] = None, base_model_attribute_name: Optional[str] = None
+    module: Module,
+    base_model_class_name: Optional[str] = None,
+    base_model_attribute_name: Optional[str] = None,
 ):
     if isinstance(module, HybridParallelModule):
         module = module.unwrap()
     if base_model_class_name is None:
         return module
     if module.__class__.__name__ == base_model_class_name:
         return module
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_bert.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_gptj.py`

 * *Files 7% similar despite different names*

```diff
@@ -21,219 +21,263 @@
 
 def check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config):
     org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
         model_fn, loss_fn, test_config
     )
 
     org_loss, org_output, sharded_loss, sharded_output = run_forward_backward_with_hybrid_plugin(
-        org_model, sharded_model, sharded_optimizer, data_gen_fn, output_transform_fn, criterion, booster
+        org_model,
+        sharded_model,
+        sharded_optimizer,
+        data_gen_fn,
+        output_transform_fn,
+        criterion,
+        booster,
     )
 
     stage_manager = booster.plugin.stage_manager
     tp_group = booster.plugin.tp_group
 
-    bert = unwrap_model(org_model, "BertModel", "bert")
-    sharded_bert = unwrap_model(sharded_model, "BertModel", "bert")
+    # unwrap model
+    gptj = unwrap_model(org_model, "GPTJModel", "transformer")
+    sharded_gptj = unwrap_model(sharded_model, "GPTJModel", "transformer")
 
-    norm_layer_for_check = ["encoder.layer[0].attention.output.LayerNorm", "embeddings.LayerNorm"]
-    col_layer_for_check = ["encoder.layer[0].output.dense"]
-    row_layer_for_check = ["embeddings.word_embeddings", "encoder.layer[0].intermediate.dense"]
-    weight_layer_for_check = ["encoder.layer[0].output.dense", "encoder.layer[1].output.dense"]
+    col_layer_for_check = ["h[0].attn.k_proj"]
+    row_layer_for_check = ["h[0].mlp.fc_out"]  # use dim=0 for wte get_grad_tensors_for_check
 
-    # Save gradient tensors for comparison between the original model and the sharded model before optimizer step.
+    # Save gradient tensors for comparison between the original model and the sharded model.
     grads_to_check = {}
-    if test_config["precision"] == "fp32":
-        atol, rtol = 1e-4, 1e-3
-    else:
-        atol, rtol = 5e-3, 5e-3
-    if (stage_manager is None or stage_manager.is_first_stage(ignore_chunk=True)) and booster.plugin.zero_stage == 0:
+    if (stage_manager is None or stage_manager.is_first_stage()) and booster.plugin.zero_stage == 0:
+        if test_config["precision"] == "fp32":
+            atol, rtol = 1e-4, 1e-3
+        else:
+            atol, rtol = 5e-3, 5e-3
         col_layer_grads = get_grad_tensors_for_check(
-            bert, sharded_bert, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
-        )
-        row_layer_grads = get_grad_tensors_for_check(
-            bert, sharded_bert, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0, verbose=False
+            gptj,
+            sharded_gptj,
+            col_layer_for_check,
+            tp_group,
+            atol=atol,
+            rtol=rtol,
+            dim=0,
+            verbose=False,
         )
 
-        norm_layer_grads = get_grad_tensors_for_check(
-            bert,
-            sharded_bert,
-            norm_layer_for_check,
+        row_layer_grads = get_grad_tensors_for_check(
+            gptj,
+            sharded_gptj,
+            row_layer_for_check,
             tp_group,
             atol=atol,
             rtol=rtol,
             dim=1,
             verbose=False,
         )
-
         grads_to_check.update(col_layer_grads)
         grads_to_check.update(row_layer_grads)
-        grads_to_check.update(norm_layer_grads)
 
     # optimizer executes step
     org_optimizer.step()
     sharded_optimizer.step()
 
     # check last hidden state & loss
-    if stage_manager is None or stage_manager.is_last_stage(ignore_chunk=True):
+    if stage_manager is None or stage_manager.is_last_stage():
         if test_config["precision"] == "fp32":
             atol, rtol = 1e-5, 1e-3
         else:
             atol, rtol = 5e-3, 5e-3
-        if org_model.__class__.__name__ == "BertModel":
+
+        if org_model.__class__.__name__ == "GPTJModel":
             check_output_hidden_state(org_output, sharded_output, stage_manager, atol=atol, rtol=rtol)
 
         check_loss(org_loss, sharded_loss, atol=atol, rtol=rtol)
 
     # check weights
-    if test_config["precision"] == "fp32":
-        atol, rtol = 5e-3, 1e-3
-    else:
-        atol, rtol = 5e-3, 5e-3
-    if stage_manager is None or stage_manager.is_first_stage(ignore_chunk=True):
-        check_weight(bert, sharded_bert, weight_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1)
+    if stage_manager is None or stage_manager.is_first_stage():
+        if test_config["precision"] == "fp32":
+            atol, rtol = 5e-3, 1e-3
+        else:
+            atol, rtol = 5e-3, 5e-3
+        check_weight(
+            gptj,
+            sharded_gptj,
+            col_layer_for_check,
+            tp_group,
+            atol=atol,
+            rtol=rtol,
+            dim=0,
+            verbose=False,
+        )
 
     # check grads
     check_all_grad_tensors(grads_to_check)
 
+    Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
 @parameterize(
     "test_config",
     [
         {
             "tp_size": 2,
-            "pp_size": 1,
+            "pp_size": 2,
+            "num_microbatches": 4,
             "enable_all_optimization": True,
-            "use_lazy_init": True,
-            "precision": "fp32",
+            #'use_lazy_init': True,  GPTJ currently do not support lazy init; model training has issue even without sharding
+            "precision": "fp16",
+            "initial_scale": 1,
         },
         {
             "tp_size": 1,
             "pp_size": 2,
             "num_microbatches": 4,
-            "use_lazy_init": True,
-            "precision": "fp32",
-        },
-        {
-            "tp_size": 2,
-            "pp_size": 2,
-            "num_microbatches": 2,
             "enable_all_optimization": True,
-            "use_lazy_init": True,
+            #'use_lazy_init': True,
             "precision": "fp16",
             "initial_scale": 1,
         },
         {
             "tp_size": 4,
             "pp_size": 1,
-            "enable_all_optimization": True,
+            "enable_all_optimization": False,
+            "use_lazy_init": False,
+            "precision": "fp32",
+        },
+        {
+            "tp_size": 2,
+            "pp_size": 1,
+            "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
         },
-        {"tp_size": 2, "pp_size": 1, "enable_all_optimization": True, "use_lazy_init": False, "precision": "fp32"},
+        {
+            "tp_size": 2,
+            "pp_size": 2,
+            "num_microbatches": 4,
+            "enable_all_optimization": False,
+            #'use_lazy_init': True,
+            "precision": "fp32",
+        },
         {
             "tp_size": 2,
             "pp_size": 1,
             "enable_all_optimization": True,
-            "use_lazy_init": True,
+            #'use_lazy_init': True,
             "zero_stage": 2,
             "precision": "fp16",
             "initial_scale": 1,
         },
         {
             "tp_size": 1,
             "pp_size": 2,
             "num_microbatches": 2,
             "enable_all_optimization": True,
-            "use_lazy_init": True,
+            #'use_lazy_init': True,
             "zero_stage": 1,
             "precision": "fp16",
             "initial_scale": 1,
         },
     ],
 )
-def run_bert_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_bert")
+@clear_cache_before_run()
+def run_gptj_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_gptj")
 
-    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
+    for name, (
+        model_fn,
+        data_gen_fn,
+        output_transform_fn,
+        loss_fn,
+        _,
+    ) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
-    Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
 @parameterize(
     "test_config",
     [
         {
             "tp_size": 2,
             "pp_size": 2,
             "num_microbatches": 4,
             "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
-        },
-        {
-            "tp_size": 2,
-            "pp_size": 2,
-            "num_microbatches": 4,
-            "enable_all_optimization": False,
-            "use_lazy_init": False,
-            "precision": "fp16",
-            "zero_stage": 1,
             "initial_scale": 1,
         },
         {
             "tp_size": 2,
             "pp_size": 2,
-            "pp_style": "interleaved",
-            "num_model_chunks": 2,
             "num_microbatches": 4,
             "enable_all_optimization": False,
+            "use_lazy_init": False,
             "precision": "fp16",
             "zero_stage": 1,
             "initial_scale": 1,
         },
     ],
 )
-def run_bert_3d_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_bert")
+@clear_cache_before_run()
+def run_gptj_3d_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_gptj")
 
-    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
+    for name, (
+        model_fn,
+        data_gen_fn,
+        output_transform_fn,
+        loss_fn,
+        _,
+    ) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
-    Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
-def check_bert(rank, world_size, port):
+def check_gptj(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_bert_test()
+    colossalai.launch(
+        config={},
+        rank=rank,
+        world_size=world_size,
+        host="localhost",
+        port=port,
+        backend="nccl",
+    )
+    run_gptj_test()
 
 
-def check_bert_3d(rank, world_size, port):
+def check_gptj_3d(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_bert_3d_test()
+    colossalai.launch(
+        config={},
+        rank=rank,
+        world_size=world_size,
+        host="localhost",
+        port=port,
+        backend="nccl",
+    )
+    run_gptj_3d_test()
 
 
+@pytest.mark.skip("TODO check_gptj has something wrong.")
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_bert():
-    spawn(check_bert, 4)
+def test_gptj():
+    spawn(check_gptj, 4)
 
 
 @pytest.mark.largedist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_bert_3d():
-    spawn(check_bert_3d, 8)
+def test_gptj_3d():
+    spawn(check_gptj_3d, 8)
 
 
 if __name__ == "__main__":
-    test_bert()
-    test_bert_3d()
+    test_gptj()
+    test_gptj_3d()
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_blip2.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_sam.py`

 * *Files 12% similar despite different names*

```diff
@@ -15,67 +15,58 @@
 
 
 def check_forward_backward(org_model, sharded_model, data_gen_fn, output_transform_fn, loss_fn):
     # check forward
     org_output, org_loss, shard_output, shard_loss = run_forward(
         org_model, sharded_model, data_gen_fn, output_transform_fn, loss_fn
     )
-    assert_hf_output_close(org_output, shard_output, ignore_keys=["past_key_values"])
+    assert_hf_output_close(org_output, shard_output, ignore_keys=["pred_masks"])
 
     # do backward
     org_loss.backward()
     shard_loss.backward()
 
     assert torch.allclose(
         org_loss, shard_loss, atol=1e-5
     ), f"shard model loss is not equal to orgin model loss\n{org_loss}\n{shard_loss}"
 
     # check grad
 
-    blip2 = org_model
-    sharded_blip2 = sharded_model
+    sam = org_model
+    sharded_sam = sharded_model
 
     # check grad
-    col_layer_for_check = [
-        "vision_model.encoder.layers[0].self_attn.qkv",
-        "qformer.encoder.layer[0].attention.attention.query",
-        "language_model.model.decoder.layers[0].self_attn.k_proj",
-    ]
-    row_layer_for_check = [
-        "vision_model.encoder.layers[0].self_attn.projection",
-        "qformer.encoder.layer[0].attention.output.dense",
-        "language_model.model.decoder.layers[0].self_attn.out_proj",
-    ]
-    check_grad(blip2, sharded_blip2, col_layer_for_check, atol=1e-6, rtol=1e-5, dim=0, verbose=False)
-    check_grad(blip2, sharded_blip2, row_layer_for_check, atol=1e-6, rtol=1e-5, dim=1, verbose=False)
+    col_layer_for_check = ["mask_decoder.transformer.layers[0].self_attn.q_proj", "vision_encoder.layers[0].mlp.lin1"]
+    row_layer_for_check = ["mask_decoder.transformer.layers[0].self_attn.out_proj", "vision_encoder.layers[0].mlp.lin2"]
+    check_grad(sam, sharded_sam, col_layer_for_check, atol=1e-5, rtol=1e-3, dim=0, verbose=False)
+    check_grad(sam, sharded_sam, row_layer_for_check, atol=1e-3, rtol=1e-3, dim=1, verbose=False)
 
 
 @parameterize("enable_fused_normalization", [True, False])
 @parameterize("enable_tensor_parallelism", [True, False])
 @parameterize("enable_flash_attention", [True, False])
-@parameterize("enable_jit_fused", [True, False])
-def run_blip2_test(enable_fused_normalization, enable_tensor_parallelism, enable_flash_attention, enable_jit_fused):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_blip2")
+def run_sam_test(enable_fused_normalization, enable_tensor_parallelism, enable_flash_attention):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_sam")
     for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
         org_model, sharded_model = build_model(
-            model_fn, enable_fused_normalization, enable_tensor_parallelism, enable_flash_attention, enable_jit_fused
+            model_fn, enable_fused_normalization, enable_tensor_parallelism, enable_flash_attention
         )
         check_forward_backward(org_model, sharded_model, data_gen_fn, output_transform_fn, loss_fn)
 
     torch.cuda.empty_cache()
 
 
-def check_blip2(rank, world_size, port):
+def check_sam(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_blip2_test()
+    colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    run_sam_test()
 
 
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_blip2():
-    spawn(check_blip2, 2)
+def test_sam():
+    spawn(check_sam, 2)
 
 
 if __name__ == "__main__":
-    test_blip2()
+    test_sam()
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_bloom.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_bloom.py`

 * *Files 5% similar despite different names*

```diff
@@ -96,14 +96,36 @@
     torch.cuda.empty_cache()
 
 
 @parameterize(
     "test_config",
     [
         {
+            "tp_size": 4,
+            "pp_size": 1,
+            "num_microbatches": 1,
+            "enable_sequence_parallelism": True,
+            "sequence_parallelism_mode": "ring",
+            "enable_flash_attention": False,
+            "use_lazy_init": True,
+            "precision": "fp32",
+            "initial_scale": 1,
+        },
+        {
+            "tp_size": 4,
+            "pp_size": 1,
+            "num_microbatches": 1,
+            "enable_sequence_parallelism": True,
+            "sequence_parallelism_mode": "split_gather",
+            "enable_flash_attention": False,
+            "use_lazy_init": True,
+            "precision": "fp16",
+            "initial_scale": 1,
+        },
+        {
             "tp_size": 2,
             "pp_size": 2,
             "num_microbatches": 4,
             "enable_all_optimization": True,
             "use_lazy_init": True,
             "precision": "fp16",
             "initial_scale": 1,
@@ -183,21 +205,21 @@
     clear_layout_converter()
     Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
 def check_bloom(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     run_bloom_test()
 
 
 def check_bloom_3d(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     run_bloom_3d_test()
 
 
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
 def test_bloom():
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_chatglm2.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_chatglm2.py`

 * *Files 10% similar despite different names*

```diff
@@ -21,26 +21,35 @@
 
 def check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config):
     org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
         model_fn, loss_fn, test_config
     )
 
     org_loss, org_output, sharded_loss, sharded_output = run_forward_backward_with_hybrid_plugin(
-        org_model, sharded_model, sharded_optimizer, data_gen_fn, output_transform_fn, criterion, booster
+        org_model,
+        sharded_model,
+        sharded_optimizer,
+        data_gen_fn,
+        output_transform_fn,
+        criterion,
+        booster,
     )
 
     stage_manager = booster.plugin.stage_manager
     tp_group = booster.plugin.tp_group
 
     # unwrap model
     chatglm_model = unwrap_model(org_model, "ChatGLMModel", "transformer")
     shard_chatglm_model = unwrap_model(sharded_model, "ChatGLMModel", "transformer")
 
     norm_layer_for_check = ["encoder.layers[0].input_layernorm"]
-    row_layer_for_check = ["encoder.layers[0].self_attention.query_key_value", "embedding.word_embeddings"]
+    row_layer_for_check = [
+        "encoder.layers[0].self_attention.query_key_value",
+        "embedding.word_embeddings",
+    ]
     col_layer_for_check = ["encoder.layers[0].self_attention.dense"]
 
     # Save gradient tensors for comparison between the original model and the sharded model.
     grads_to_check = {}
     if (stage_manager is None or stage_manager.is_first_stage()) and booster.plugin.zero_stage == 0:
         if test_config["precision"] == "fp32":
             atol, rtol = 1e-6, 1e-3
@@ -90,14 +99,15 @@
     # check last hidden state & loss
     if stage_manager is None or stage_manager.is_last_stage():
         if test_config["precision"] == "fp32":
             atol, rtol = 1e-5, 1e-3
         else:
             atol, rtol = 5e-3, 5e-3
 
+        # TODO: ChatGLMModel output is [S, B, H], merging batch of pipeline is wrong
         if org_model.__class__.__name__ == "ChatGLMModel":
             check_output_hidden_state(org_output, sharded_output, stage_manager, atol=atol, rtol=rtol, dim=1)
 
         check_loss(org_loss, sharded_loss, atol=atol, rtol=rtol)
 
     # check weights
     if stage_manager is None or stage_manager.is_first_stage():
@@ -123,14 +133,36 @@
     torch.cuda.empty_cache()
 
 
 @parameterize(
     "test_config",
     [
         {
+            "tp_size": 4,
+            "pp_size": 1,
+            "num_microbatches": 1,
+            "enable_sequence_parallelism": True,
+            "sequence_parallelism_mode": "ring",
+            "enable_flash_attention": False,
+            "use_lazy_init": True,
+            "precision": "fp32",
+            "initial_scale": 1,
+        },
+        {
+            "tp_size": 4,
+            "pp_size": 1,
+            "num_microbatches": 1,
+            "enable_sequence_parallelism": True,
+            "sequence_parallelism_mode": "split_gather",
+            "enable_flash_attention": False,
+            "use_lazy_init": True,
+            "precision": "fp16",
+            "initial_scale": 1,
+        },
+        {
             "tp_size": 2,
             "pp_size": 2,
             "num_microbatches": 4,
             "enable_all_optimization": True,
             "use_lazy_init": True,
             "precision": "fp16",
             "initial_scale": 1,
@@ -139,31 +171,49 @@
             "tp_size": 1,
             "pp_size": 2,
             "num_microbatches": 4,
             "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
         },
-        {"tp_size": 4, "pp_size": 1, "enable_all_optimization": True, "use_lazy_init": False, "precision": "fp32"},
-        {"tp_size": 2, "pp_size": 1, "enable_all_optimization": True, "use_lazy_init": False, "precision": "fp32"},
+        {
+            "tp_size": 4,
+            "pp_size": 1,
+            "enable_all_optimization": False,
+            "use_lazy_init": False,
+            "precision": "fp32",
+        },
+        {
+            "tp_size": 2,
+            "pp_size": 1,
+            "enable_all_optimization": False,
+            "use_lazy_init": False,
+            "precision": "fp32",
+        },
         {
             "tp_size": 2,
             "pp_size": 1,
             "enable_all_optimization": True,
             "use_lazy_init": True,
             "zero_stage": 2,
             "precision": "fp16",
             "initial_scale": 1,
         },
     ],
 )
 def run_chatglm_test(test_config):
     sub_model_zoo = model_zoo.get_sub_registry("transformers_chatglm")
 
-    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
+    for name, (
+        model_fn,
+        data_gen_fn,
+        output_transform_fn,
+        loss_fn,
+        _,
+    ) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
     torch.cuda.empty_cache()
 
 
 @parameterize(
@@ -189,30 +239,48 @@
             "initial_scale": 1,
         },
     ],
 )
 def run_chatglm_3d_test(test_config):
     sub_model_zoo = model_zoo.get_sub_registry("transformers_chatglm")
 
-    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
+    for name, (
+        model_fn,
+        data_gen_fn,
+        output_transform_fn,
+        loss_fn,
+        _,
+    ) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
     torch.cuda.empty_cache()
 
 
 def check_chatglm(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    colossalai.launch(
+        rank=rank,
+        world_size=world_size,
+        host="localhost",
+        port=port,
+        backend="nccl",
+    )
     run_chatglm_test()
 
 
 def check_chatglm_3d(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    colossalai.launch(
+        rank=rank,
+        world_size=world_size,
+        host="localhost",
+        port=port,
+        backend="nccl",
+    )
     run_chatglm_3d_test()
 
 
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
 def test_chatglm():
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_falcon.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_falcon.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import pytest
 import torch
+import torch.distributed as dist
 
 import colossalai
 from colossalai.logging import disable_existing_loggers
 from colossalai.shardformer.layer.utils import Randomizer
 from colossalai.tensor.d_tensor.api import clear_layout_converter
 from colossalai.testing import clear_cache_before_run, parameterize, rerun_if_address_is_in_use, spawn
 from tests.kit.model_zoo import model_zoo
@@ -68,14 +69,16 @@
             check_output_hidden_state(org_output, sharded_output, stage_manager, atol=atol, rtol=rtol)
 
         check_loss(org_loss, sharded_loss, atol=atol, rtol=rtol)
 
     if stage_manager is None or stage_manager.is_first_stage():
         if test_config["precision"] == "fp32":
             atol, rtol = 2e-4, 1e-3
+            if dist.get_world_size() > 4:
+                atol, rtol = 4e-4, 3e-2
         else:
             atol, rtol = 5e-3, 5e-3
         check_weight(falcon, sharded_falcon, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False)
 
     # check grads
     check_all_grad_tensors(grads_to_check)
 
@@ -169,21 +172,21 @@
     clear_layout_converter()
     Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
 def check_falcon(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     run_falcon_test()
 
 
 def check_falcon_3d(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     run_falcon_3d_test()
 
 
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
 def test_falcon():
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_gpt2.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_opt.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,7 +1,9 @@
+import os
+
 import pytest
 import torch
 
 import colossalai
 from colossalai.logging import disable_existing_loggers
 from colossalai.shardformer.layer.utils import Randomizer
 from colossalai.tensor.d_tensor.api import clear_layout_converter
@@ -14,87 +16,106 @@
     check_output_hidden_state,
     check_weight,
     get_grad_tensors_for_check,
     run_forward_backward_with_hybrid_plugin,
     unwrap_model,
 )
 
+os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "true"
+
 
 def check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config):
     org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
         model_fn, loss_fn, test_config
     )
 
     org_loss, org_output, sharded_loss, sharded_output = run_forward_backward_with_hybrid_plugin(
-        org_model, sharded_model, sharded_optimizer, data_gen_fn, output_transform_fn, criterion, booster
+        org_model,
+        sharded_model,
+        sharded_optimizer,
+        data_gen_fn,
+        output_transform_fn,
+        criterion,
+        booster,
     )
 
     stage_manager = booster.plugin.stage_manager
     tp_group = booster.plugin.tp_group
 
     # unwrap model
-    gpt2 = unwrap_model(org_model, "GPT2Model", "transformer")
-    sharded_gpt2 = unwrap_model(sharded_model, "GPT2Model", "transformer")
+    opt_model = unwrap_model(org_model, "OPTModel", "model")
+    shard_opt_model = unwrap_model(sharded_model, "OPTModel", "model")
 
-    norm_layer_for_check = ["h[0].ln_1", "h[0].ln_2"]
-    col_layer_for_check = ["h[0].mlp.c_fc"]
-    row_layer_for_check = ["wte", "h[0].mlp.c_proj"]
+    row_layer_for_check = [
+        "decoder.layers[0].self_attn.q_proj",
+        "decoder.embed_tokens",
+    ]  # 'decoder.embed_tokens'
+    col_layer_for_check = ["decoder.layers[0].self_attn.out_proj"]
 
     # Save gradient tensors for comparison between the original model and the sharded model.
     grads_to_check = {}
     if (stage_manager is None or stage_manager.is_first_stage()) and booster.plugin.zero_stage == 0:
         if test_config["precision"] == "fp32":
-            atol, rtol = 1e-4, 1e-3
+            atol, rtol = 1e-6, 1e-3
         else:
-            atol, rtol = 5e-3, 5e-3
-        col_layer_grads = get_grad_tensors_for_check(
-            gpt2, sharded_gpt2, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
-        )
+            atol, rtol = 4e-2, 4e-2
         row_layer_grads = get_grad_tensors_for_check(
-            gpt2, sharded_gpt2, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0, verbose=False
+            opt_model,
+            shard_opt_model,
+            row_layer_for_check,
+            tp_group,
+            atol=atol,
+            rtol=rtol,
+            dim=0,
+            verbose=False,
         )
-
-        norm_layer_grads = get_grad_tensors_for_check(
-            gpt2,
-            sharded_gpt2,
-            norm_layer_for_check,
+        col_layer_grads = get_grad_tensors_for_check(
+            opt_model,
+            shard_opt_model,
+            col_layer_for_check,
             tp_group,
             atol=atol,
             rtol=rtol,
             dim=1,
             verbose=False,
         )
-
         grads_to_check.update(col_layer_grads)
         grads_to_check.update(row_layer_grads)
-        grads_to_check.update(norm_layer_grads)
 
     # optimizer executes step
     org_optimizer.step()
     sharded_optimizer.step()
 
     # check last hidden state & loss
     if stage_manager is None or stage_manager.is_last_stage():
         if test_config["precision"] == "fp32":
             atol, rtol = 1e-5, 1e-3
         else:
             atol, rtol = 5e-3, 5e-3
-
-        if org_model.__class__.__name__ == "GPT2Model":
+        if org_model.__class__.__name__ == "OPTModel":
             check_output_hidden_state(org_output, sharded_output, stage_manager, atol=atol, rtol=rtol)
 
         check_loss(org_loss, sharded_loss, atol=atol, rtol=rtol)
 
     # check weights
     if stage_manager is None or stage_manager.is_first_stage():
         if test_config["precision"] == "fp32":
-            atol, rtol = 5e-3, 1e-3
+            atol, rtol = 1e-3, 1e-3
         else:
             atol, rtol = 5e-3, 5e-3
-        check_weight(gpt2, sharded_gpt2, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False)
+        check_weight(
+            opt_model,
+            shard_opt_model,
+            col_layer_for_check,
+            tp_group,
+            atol=atol,
+            rtol=rtol,
+            dim=1,
+            verbose=False,
+        )
 
     # check grads
     check_all_grad_tensors(grads_to_check)
 
     Randomizer.reset_index()
     torch.cuda.empty_cache()
 
@@ -111,43 +132,34 @@
             "precision": "fp16",
             "initial_scale": 1,
         },
         {
             "tp_size": 1,
             "pp_size": 2,
             "num_microbatches": 4,
-            "enable_all_optimization": True,
-            "use_lazy_init": True,
-            "precision": "fp16",
-            "initial_scale": 1,
+            "enable_all_optimization": False,
+            "use_lazy_init": False,
+            "precision": "fp32",
         },
         {
             "tp_size": 4,
             "pp_size": 1,
-            "enable_all_optimization": True,
+            "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
         },
         {
             "tp_size": 2,
             "pp_size": 1,
-            "enable_all_optimization": True,
+            "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
         },
         {
             "tp_size": 2,
-            "pp_size": 2,
-            "num_microbatches": 4,
-            "enable_all_optimization": True,
-            "use_lazy_init": True,
-            "precision": "fp32",
-        },
-        {
-            "tp_size": 2,
             "pp_size": 1,
             "enable_all_optimization": True,
             "use_lazy_init": True,
             "zero_stage": 2,
             "precision": "fp16",
             "initial_scale": 1,
         },
@@ -159,19 +171,23 @@
             "use_lazy_init": True,
             "zero_stage": 1,
             "precision": "fp16",
             "initial_scale": 1,
         },
     ],
 )
-@clear_cache_before_run()
-def run_gpt2_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_gpt", exclude="transformers_gptj")
-
-    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
+def run_opt_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_opt")
+    for name, (
+        model_fn,
+        data_gen_fn,
+        output_transform_fn,
+        loss_fn,
+        _,
+    ) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
     torch.cuda.empty_cache()
 
 
 @parameterize(
@@ -194,47 +210,64 @@
             "use_lazy_init": False,
             "precision": "fp16",
             "zero_stage": 1,
             "initial_scale": 1,
         },
     ],
 )
-@clear_cache_before_run()
-def run_gpt2_3d_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_gpt", exclude="transformers_gptj")
+def run_opt_3d_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_opt")
 
-    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
+    for name, (
+        model_fn,
+        data_gen_fn,
+        output_transform_fn,
+        loss_fn,
+        _,
+    ) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
     torch.cuda.empty_cache()
 
 
-def check_gpt2(rank, world_size, port):
+def check_OPTModel(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_gpt2_test()
+    colossalai.launch(
+        rank=rank,
+        world_size=world_size,
+        host="localhost",
+        port=port,
+        backend="nccl",
+    )
+    run_opt_test()
 
 
-def check_gpt2_3d(rank, world_size, port):
+def check_opt_3d(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_gpt2_3d_test()
+    colossalai.launch(
+        rank=rank,
+        world_size=world_size,
+        host="localhost",
+        port=port,
+        backend="nccl",
+    )
+    run_opt_3d_test()
 
 
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_gpt2():
-    spawn(check_gpt2, 4)
+def test_OPTModel():
+    spawn(check_OPTModel, 4)
 
 
 @pytest.mark.largedist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_gpt2_3d():
-    spawn(check_gpt2_3d, 8)
+def test_opt_3d():
+    spawn(check_opt_3d, 8)
 
 
 if __name__ == "__main__":
-    test_gpt2()
-    test_gpt2_3d()
+    test_OPTModel()
+    test_opt_3d()
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_gptj.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_t5.py`

 * *Files 10% similar despite different names*

```diff
@@ -21,207 +21,242 @@
 
 def check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config):
     org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
         model_fn, loss_fn, test_config
     )
 
     org_loss, org_output, sharded_loss, sharded_output = run_forward_backward_with_hybrid_plugin(
-        org_model, sharded_model, sharded_optimizer, data_gen_fn, output_transform_fn, criterion, booster
+        org_model,
+        sharded_model,
+        sharded_optimizer,
+        data_gen_fn,
+        output_transform_fn,
+        criterion,
+        booster,
     )
 
     stage_manager = booster.plugin.stage_manager
     tp_group = booster.plugin.tp_group
 
     # unwrap model
-    gptj = unwrap_model(org_model, "GPTJModel", "transformer")
-    sharded_gptj = unwrap_model(sharded_model, "GPTJModel", "transformer")
+    t5 = unwrap_model(org_model)
+    sharded_t5 = unwrap_model(sharded_model)
 
-    col_layer_for_check = ["h[0].attn.k_proj"]
-    row_layer_for_check = ["h[0].mlp.fc_out"]  # use dim=0 for wte get_grad_tensors_for_check
+    row_layer_for_check = ["shared", "encoder.block[0].layer[0].SelfAttention.q"]
 
-    # Save gradient tensors for comparison between the original model and the sharded model.
+    # Save gradient tensors for comparison between the original model and the sharded model before optimizer step.
     grads_to_check = {}
+    if test_config["precision"] == "fp32":
+        atol, rtol = 1e-5, 1e-3
+    else:
+        atol, rtol = 5e-3, 5e-3
     if (stage_manager is None or stage_manager.is_first_stage()) and booster.plugin.zero_stage == 0:
-        if test_config["precision"] == "fp32":
-            atol, rtol = 1e-4, 1e-3
-        else:
-            atol, rtol = 5e-3, 5e-3
-        col_layer_grads = get_grad_tensors_for_check(
-            gptj, sharded_gptj, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0, verbose=False
-        )
-
         row_layer_grads = get_grad_tensors_for_check(
-            gptj, sharded_gptj, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
+            t5, sharded_t5, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0
         )
-        grads_to_check.update(col_layer_grads)
         grads_to_check.update(row_layer_grads)
 
     # optimizer executes step
     org_optimizer.step()
     sharded_optimizer.step()
 
     # check last hidden state & loss
     if stage_manager is None or stage_manager.is_last_stage():
         if test_config["precision"] == "fp32":
             atol, rtol = 1e-5, 1e-3
         else:
             atol, rtol = 5e-3, 5e-3
 
-        if org_model.__class__.__name__ == "GPTJModel":
+        if org_model.__class__.__name__ != "T5ForConditionalGeneration":
             check_output_hidden_state(org_output, sharded_output, stage_manager, atol=atol, rtol=rtol)
 
         check_loss(org_loss, sharded_loss, atol=atol, rtol=rtol)
 
     # check weights
+    if test_config["precision"] == "fp32":
+        # TODO he precision in weight checking is too significant.
+        atol, rtol = 1e-3, 1e-3
+    else:
+        atol, rtol = 5e-3, 5e-3
     if stage_manager is None or stage_manager.is_first_stage():
-        if test_config["precision"] == "fp32":
-            atol, rtol = 5e-3, 1e-3
-        else:
-            atol, rtol = 5e-3, 5e-3
-        check_weight(gptj, sharded_gptj, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0, verbose=False)
+        check_weight(
+            t5,
+            sharded_t5,
+            row_layer_for_check,
+            tp_group,
+            atol=atol,
+            rtol=rtol,
+            dim=0,
+            verbose=False,
+        )
 
     # check grads
     check_all_grad_tensors(grads_to_check)
 
-    Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
 @parameterize(
     "test_config",
     [
         {
             "tp_size": 2,
             "pp_size": 2,
-            "num_microbatches": 4,
+            "num_microbatches": 2,
+            "enable_metadata_cache": False,
             "enable_all_optimization": True,
-            #'use_lazy_init': True,  GPTJ currently do not support lazy init; model training has issue even without sharding
+            "use_lazy_init": True,
             "precision": "fp16",
             "initial_scale": 1,
         },
         {
             "tp_size": 1,
             "pp_size": 2,
             "num_microbatches": 4,
-            "enable_all_optimization": True,
-            #'use_lazy_init': True,
+            "enable_metadata_cache": False,
+            "use_lazy_init": False,
             "precision": "fp16",
             "initial_scale": 1,
         },
         {
             "tp_size": 4,
             "pp_size": 1,
-            "enable_all_optimization": True,
-            "use_lazy_init": False,
-            "precision": "fp32",
-        },
-        {
-            "tp_size": 2,
-            "pp_size": 1,
-            "enable_all_optimization": True,
+            "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
         },
         {
-            "tp_size": 2,
-            "pp_size": 2,
+            "tp_size": 1,
+            "pp_size": 4,
             "num_microbatches": 4,
-            "enable_all_optimization": True,
-            #'use_lazy_init': True,
+            "enable_metadata_cache": False,
+            "enable_all_optimization": False,
+            "use_lazy_init": False,
             "precision": "fp32",
         },
         {
             "tp_size": 2,
             "pp_size": 1,
             "enable_all_optimization": True,
-            #'use_lazy_init': True,
+            "use_lazy_init": True,
             "zero_stage": 2,
             "precision": "fp16",
             "initial_scale": 1,
         },
         {
             "tp_size": 1,
             "pp_size": 2,
             "num_microbatches": 2,
+            "enable_metadata_cache": False,
             "enable_all_optimization": True,
-            #'use_lazy_init': True,
+            "use_lazy_init": True,
             "zero_stage": 1,
             "precision": "fp16",
             "initial_scale": 1,
         },
     ],
 )
 @clear_cache_before_run()
-def run_gptj_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_gptj")
+def run_t5_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_t5")
+
+    for name, (
+        model_fn,
+        data_gen_fn,
+        output_transform_fn,
+        loss_fn,
+        _,
+    ) in sub_model_zoo.items():
+        # skip 4-stage pp test for t5_encoder
+        if test_config["pp_size"] > 2 and name == "transformers_t5_encoder_model":
+            continue
 
-    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
+    Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
 @parameterize(
     "test_config",
     [
         {
             "tp_size": 2,
             "pp_size": 2,
             "num_microbatches": 4,
+            "enable_metadata_cache": False,
             "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
             "initial_scale": 1,
         },
         {
             "tp_size": 2,
             "pp_size": 2,
             "num_microbatches": 4,
+            "enable_metadata_cache": False,
             "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp16",
             "zero_stage": 1,
             "initial_scale": 1,
         },
     ],
 )
-@clear_cache_before_run()
-def run_gptj_3d_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_gptj")
+def run_t5_3d_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_t5")
 
-    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
+    for name, (
+        model_fn,
+        data_gen_fn,
+        output_transform_fn,
+        loss_fn,
+        _,
+    ) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
     torch.cuda.empty_cache()
 
 
-def check_gptj(rank, world_size, port):
+def check_t5(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_gptj_test()
+    colossalai.launch(
+        rank=rank,
+        world_size=world_size,
+        host="localhost",
+        port=port,
+        backend="nccl",
+    )
+    run_t5_test()
 
 
-def check_gptj_3d(rank, world_size, port):
+def check_t5_3d(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_gptj_3d_test()
+    colossalai.launch(
+        rank=rank,
+        world_size=world_size,
+        host="localhost",
+        port=port,
+        backend="nccl",
+    )
+    run_t5_3d_test()
+
 
-@pytest.mark.skip("TODO check_gptj has something wrong.")
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_gptj():
-    spawn(check_gptj, 4)
+def test_t5():
+    spawn(check_t5, 4)
 
 
 @pytest.mark.largedist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_gptj_3d():
-    spawn(check_gptj_3d, 8)
+def test_t5_3d():
+    spawn(check_t5_3d, 8)
 
 
 if __name__ == "__main__":
-    test_gptj()
-    test_gptj_3d()
+    test_t5()
+    test_t5_3d()
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_llama.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_mistral.py`

 * *Files 18% similar despite different names*

```diff
@@ -32,203 +32,154 @@
         org_model, sharded_model, sharded_optimizer, data_gen_fn, output_transform_fn, criterion, booster
     )
 
     stage_manager = booster.plugin.stage_manager
     tp_group = booster.plugin.tp_group
 
     # unwrap model
-    llama_model = unwrap_model(org_model, "LlamaModel", "model")
-    shard_llama_model = unwrap_model(sharded_model, "LlamaModel", "model")
+    mistral_model = unwrap_model(org_model, "MistralModel", "model")
+    shard_mistral_model = unwrap_model(sharded_model, "MistralModel", "model")
 
     row_layer_for_check = ["layers[0].self_attn.q_proj", "embed_tokens"]
     col_layer_for_check = ["layers[0].self_attn.o_proj"]
 
     # Save gradient tensors for comparison between the original model and the sharded model before optimizer step.
     grads_to_check = {}
-    if (stage_manager is None or stage_manager.is_first_stage(ignore_chunk=True)) and booster.plugin.zero_stage == 0:
+    if (stage_manager is None or stage_manager.is_first_stage()) and booster.plugin.zero_stage == 0:
         if test_config["precision"] == "fp32":
-            atol, rtol = 1e-6, 1e-4
+            atol, rtol = 5e-5, 1e-4
         else:
             atol, rtol = 5e-3, 5e-3
         row_layer_grads = get_grad_tensors_for_check(
-            llama_model, shard_llama_model, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0, verbose=False
+            mistral_model,
+            shard_mistral_model,
+            row_layer_for_check,
+            tp_group,
+            atol=atol,
+            rtol=rtol,
+            dim=0,
+            verbose=False,
         )
         col_layer_grads = get_grad_tensors_for_check(
-            llama_model, shard_llama_model, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
+            mistral_model,
+            shard_mistral_model,
+            col_layer_for_check,
+            tp_group,
+            atol=atol,
+            rtol=rtol,
+            dim=1,
+            verbose=False,
         )
         grads_to_check.update(col_layer_grads)
         grads_to_check.update(row_layer_grads)
 
     # optimizer executes step
     org_optimizer.step()
     sharded_optimizer.step()
 
     # check last hidden state & loss
-    if stage_manager is None or stage_manager.is_last_stage(ignore_chunk=True):
+    if stage_manager is None or stage_manager.is_last_stage():
         if test_config["precision"] == "fp32":
             atol, rtol = 1e-5, 1e-3
         else:
             atol, rtol = 5e-3, 5e-3
 
-        if org_model.__class__.__name__ == "LlamaModel":
+        if org_model.__class__.__name__ == "MistralModel":
             check_output_hidden_state(org_output, sharded_output, stage_manager, atol=atol, rtol=rtol)
 
         check_loss(org_loss, sharded_loss, atol=atol, rtol=rtol)
 
     # check weights
-    if stage_manager is None or stage_manager.is_first_stage(ignore_chunk=True):
+    if stage_manager is None or stage_manager.is_first_stage():
         if test_config["precision"] == "fp32":
-            atol, rtol = 1e-4, 1e-3
+            atol, rtol = 2e-4, 1e-3
         else:
             atol, rtol = 5e-3, 5e-3
         check_weight(
-            llama_model, shard_llama_model, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
+            mistral_model,
+            shard_mistral_model,
+            col_layer_for_check,
+            tp_group,
+            atol=atol,
+            rtol=rtol,
+            dim=1,
+            verbose=False,
         )
 
     # check grads
     check_all_grad_tensors(grads_to_check)
 
     torch.cuda.empty_cache()
 
 
 @parameterize(
     "test_config",
     [
         {
-            "tp_size": 2,
+            "tp_size": 1,
             "pp_size": 2,
             "num_microbatches": 2,
             "enable_all_optimization": True,
-            "use_lazy_init": True,
+            "use_lazy_init": False,
             "precision": "fp16",
             "initial_scale": 1,
         },
         {
-            "tp_size": 1,
+            "tp_size": 2,
             "pp_size": 2,
-            "num_microbatches": 4,
-            "use_lazy_init": False,
-            "precision": "fp32",
+            "num_microbatches": 2,
+            "enable_all_optimization": True,
+            "use_lazy_init": True,
+            "precision": "fp16",
+            "initial_scale": 1,
         },
         {
             "tp_size": 4,
             "pp_size": 1,
             "enable_all_optimization": True,
             "use_lazy_init": False,
             "precision": "fp32",
         },
         {
-            "tp_size": 1,
-            "pp_size": 4,
-            "num_microbatches": 4,
-            "enable_all_optimization": False,
+            "tp_size": 2,
+            "pp_size": 1,
+            "enable_all_optimization": True,
             "use_lazy_init": False,
             "precision": "fp32",
         },
-        {"tp_size": 2, "pp_size": 1, "enable_all_optimization": True, "use_lazy_init": False, "precision": "fp32"},
         {
             "tp_size": 2,
             "pp_size": 1,
             "enable_all_optimization": True,
             "use_lazy_init": True,
             "zero_stage": 2,
             "precision": "fp16",
             "initial_scale": 1,
         },
-        {
-            "tp_size": 1,
-            "pp_size": 2,
-            "num_microbatches": 2,
-            "enable_all_optimization": True,
-            "use_lazy_init": True,
-            "zero_stage": 1,
-            "precision": "fp16",
-            "initial_scale": 1,
-        },
     ],
 )
-def run_llama_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_llama")
+def run_mistral_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_mistral")
 
     for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
     Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
-@parameterize(
-    "test_config",
-    [
-        {
-            "tp_size": 2,
-            "pp_size": 2,
-            "num_microbatches": 4,
-            "enable_all_optimization": False,
-            "use_lazy_init": False,
-            "precision": "fp32",
-            "initial_scale": 1,
-        },
-        {
-            "tp_size": 2,
-            "pp_size": 2,
-            "num_microbatches": 4,
-            "enable_all_optimization": False,
-            "use_lazy_init": False,
-            "precision": "fp16",
-            "zero_stage": 1,
-            "initial_scale": 1,
-        },
-        {
-            "tp_size": 2,
-            "pp_size": 2,
-            "pp_style": "interleaved",
-            "num_model_chunks": 2,
-            "num_microbatches": 4,
-            "enable_all_optimization": False,
-            "precision": "fp16",
-            "zero_stage": 1,
-            "initial_scale": 1,
-        },
-    ],
-)
-def run_llama_3d_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_llama")
-
-    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
-        check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
-
-    clear_layout_converter()
-    Randomizer.reset_index()
-    torch.cuda.empty_cache()
-
-
-def check_llama(rank, world_size, port):
-    disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_llama_test()
-
-
-def check_llama_3d(rank, world_size, port):
+def check_mistral(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_llama_3d_test()
+    colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    run_mistral_test()
 
 
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_llama():
-    spawn(check_llama, 4)
-
-
-@pytest.mark.largedist
-@rerun_if_address_is_in_use()
-@clear_cache_before_run()
-def test_llama_3d():
-    spawn(check_llama_3d, 8)
+def test_mistral():
+    spawn(check_mistral, 4)
 
 
 if __name__ == "__main__":
-    test_llama()
-    test_llama_3d()
+    test_mistral()
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_mistral.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_vit.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,9 +1,7 @@
-import os
-
 import pytest
 import torch
 
 import colossalai
 from colossalai.logging import disable_existing_loggers
 from colossalai.shardformer.layer.utils import Randomizer
 from colossalai.tensor.d_tensor.api import clear_layout_converter
@@ -16,153 +14,184 @@
     check_output_hidden_state,
     check_weight,
     get_grad_tensors_for_check,
     run_forward_backward_with_hybrid_plugin,
     unwrap_model,
 )
 
-os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "true"
-
 
 def check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config):
     org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
         model_fn, loss_fn, test_config
     )
 
     org_loss, org_output, sharded_loss, sharded_output = run_forward_backward_with_hybrid_plugin(
         org_model, sharded_model, sharded_optimizer, data_gen_fn, output_transform_fn, criterion, booster
     )
 
     stage_manager = booster.plugin.stage_manager
     tp_group = booster.plugin.tp_group
 
     # unwrap model
-    mistral_model = unwrap_model(org_model, "MistralModel", "model")
-    shard_mistral_model = unwrap_model(sharded_model, "MistralModel", "model")
+    vit_model = unwrap_model(org_model, "ViTModel", "vit")
+    shard_vit_model = unwrap_model(sharded_model, "ViTModel", "vit")
 
-    row_layer_for_check = ["layers[0].self_attn.q_proj", "embed_tokens"]
-    col_layer_for_check = ["layers[0].self_attn.o_proj"]
+    # check grad
+    row_layer_for_check = ["encoder.layer[0].attention.attention.query", "embeddings.patch_embeddings.projection"]
+    col_layer_for_check = ["encoder.layer[0].attention.output.dense"]
 
     # Save gradient tensors for comparison between the original model and the sharded model before optimizer step.
     grads_to_check = {}
     if (stage_manager is None or stage_manager.is_first_stage()) and booster.plugin.zero_stage == 0:
         if test_config["precision"] == "fp32":
-            atol, rtol = 5e-5, 1e-4
+            atol, rtol = 2e-5, 1e-3
         else:
             atol, rtol = 5e-3, 5e-3
         row_layer_grads = get_grad_tensors_for_check(
-            mistral_model,
-            shard_mistral_model,
-            row_layer_for_check,
-            tp_group,
-            atol=atol,
-            rtol=rtol,
-            dim=0,
-            verbose=False,
+            vit_model, shard_vit_model, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0, verbose=False
         )
         col_layer_grads = get_grad_tensors_for_check(
-            mistral_model,
-            shard_mistral_model,
-            col_layer_for_check,
-            tp_group,
-            atol=atol,
-            rtol=rtol,
-            dim=1,
-            verbose=False,
+            vit_model, shard_vit_model, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
         )
         grads_to_check.update(col_layer_grads)
         grads_to_check.update(row_layer_grads)
 
     # optimizer executes step
     org_optimizer.step()
     sharded_optimizer.step()
 
     # check last hidden state & loss
     if stage_manager is None or stage_manager.is_last_stage():
         if test_config["precision"] == "fp32":
-            atol, rtol = 1e-5, 1e-3
+            atol, rtol = 2e-3, 1e-3
         else:
             atol, rtol = 5e-3, 5e-3
 
-        if org_model.__class__.__name__ == "MistralModel":
+        if org_model.__class__.__name__ == "ViTModel":
             check_output_hidden_state(org_output, sharded_output, stage_manager, atol=atol, rtol=rtol)
-
         check_loss(org_loss, sharded_loss, atol=atol, rtol=rtol)
 
     # check weights
     if stage_manager is None or stage_manager.is_first_stage():
         if test_config["precision"] == "fp32":
-            atol, rtol = 1e-4, 1e-3
+            atol, rtol = 5e-3, 1e-3
         else:
             atol, rtol = 5e-3, 5e-3
         check_weight(
-            mistral_model,
-            shard_mistral_model,
-            col_layer_for_check,
-            tp_group,
-            atol=atol,
-            rtol=rtol,
-            dim=1,
-            verbose=False,
+            vit_model, shard_vit_model, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
         )
 
     # check grads
     check_all_grad_tensors(grads_to_check)
 
     torch.cuda.empty_cache()
 
 
+# TODO: num_microbatch size = 2 inf loss
 @parameterize(
     "test_config",
     [
         {
-            "tp_size": 4,
-            "pp_size": 1,
+            "tp_size": 2,
+            "pp_size": 2,
+            "num_microbatches": 4,
             "enable_all_optimization": True,
             "use_lazy_init": False,
-            "precision": "fp32",
+            "precision": "fp16",
+            "initial_scale": 1,
         },
         {
-            "tp_size": 2,
-            "pp_size": 1,
-            "enable_all_optimization": True,
+            "tp_size": 1,
+            "pp_size": 2,
+            "num_microbatches": 4,
+            "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
         },
+        {"tp_size": 4, "pp_size": 1, "enable_all_optimization": True, "use_lazy_init": False, "precision": "fp32"},
+        {"tp_size": 2, "pp_size": 1, "enable_all_optimization": True, "use_lazy_init": False, "precision": "fp32"},
         {
             "tp_size": 2,
             "pp_size": 1,
             "enable_all_optimization": True,
-            "use_lazy_init": True,
+            "use_lazy_init": False,
             "zero_stage": 2,
             "precision": "fp16",
             "initial_scale": 1,
         },
+        {
+            "tp_size": 1,
+            "pp_size": 2,
+            "num_microbatches": 4,
+            "enable_all_optimization": True,
+            "use_lazy_init": False,
+            "zero_stage": 1,
+            "precision": "fp16",
+            "initial_scale": 1,
+        },
     ],
 )
-def run_mistral_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_mistral")
+def run_vit_test(test_config):
+    # TODO: fix bug when settign lazy_init for Conv2D Layers in ViT models
 
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_vit")
     for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
     Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
-def check_mistral(rank, world_size, port):
+@parameterize(
+    "test_config",
+    [
+        {
+            "tp_size": 2,
+            "pp_size": 2,
+            "num_microbatches": 4,
+            "enable_all_optimization": False,
+            "use_lazy_init": False,
+            "precision": "fp32",
+            "initial_scale": 1,
+        },
+    ],
+)
+def run_vit_3d_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_vit")
+
+    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
+        check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
+
+    clear_layout_converter()
+    torch.cuda.empty_cache()
+
+
+def check_vit(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_mistral_test()
+    colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    run_vit_test()
+
+
+def check_vit_3d(rank, world_size, port):
+    disable_existing_loggers()
+    colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    run_vit_3d_test()
 
 
-@pytest.mark.skip("This test should be run on a version of transformers not less than 4.35.2.")
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_mistral():
-    spawn(check_mistral, 4)
+def test_vit():
+    spawn(check_vit, 4)
+
+
+@pytest.mark.largedist
+@rerun_if_address_is_in_use()
+@clear_cache_before_run()
+def test_vit_3d():
+    spawn(check_vit_3d, 8)
 
 
 if __name__ == "__main__":
-    test_mistral()
+    test_vit()
+    test_vit_3d()
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_opt.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_whisper.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,7 @@
-import os
-
 import pytest
 import torch
 
 import colossalai
 from colossalai.logging import disable_existing_loggers
 from colossalai.shardformer.layer.utils import Randomizer
 from colossalai.tensor.d_tensor.api import clear_layout_converter
@@ -13,195 +11,215 @@
     build_model_from_hybrid_plugin,
     check_all_grad_tensors,
     check_loss,
     check_output_hidden_state,
     check_weight,
     get_grad_tensors_for_check,
     run_forward_backward_with_hybrid_plugin,
-    unwrap_model,
 )
 
-os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "true"
-
 
 def check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config):
+    # check forward
     org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
         model_fn, loss_fn, test_config
     )
 
     org_loss, org_output, sharded_loss, sharded_output = run_forward_backward_with_hybrid_plugin(
         org_model, sharded_model, sharded_optimizer, data_gen_fn, output_transform_fn, criterion, booster
     )
 
     stage_manager = booster.plugin.stage_manager
     tp_group = booster.plugin.tp_group
 
-    # unwrap model
-    opt_model = unwrap_model(org_model, "OPTModel", "model")
-    shard_opt_model = unwrap_model(sharded_model, "OPTModel", "model")
-
-    row_layer_for_check = ["decoder.layers[0].self_attn.q_proj", "decoder.embed_tokens"]  # 'decoder.embed_tokens'
-    col_layer_for_check = ["decoder.layers[0].self_attn.out_proj"]
+    # unwarp the model
+    if org_model.__class__.__name__ == "WhisperForConditionalGeneration":
+        whisper = org_model.model
+        sharded_whisper = sharded_model.unwrap().model
+    else:
+        whisper = org_model
+        sharded_whisper = sharded_model.unwrap()
+
+    # check grad
+    if org_model.__class__.__name__ == "WhisperForAudioClassification":
+        col_layer_for_check = ["encoder.layers[0].self_attn.q_proj"]
+        row_layer_for_check = ["encoder.layers[0].self_attn.out_proj"]
+    else:
+        col_layer_for_check = [
+            "encoder.layers[0].self_attn.q_proj",
+            # 'decoder.layers[0].self_attn.q_proj'
+        ]
+        row_layer_for_check = [
+            "encoder.layers[0].self_attn.out_proj",
+            #'decoder.layers[0].self_attn.out_proj'
+        ]
 
-    # Save gradient tensors for comparison between the original model and the sharded model.
+    # Save gradient tensors for comparison between the original model and the sharded model before optimizer step.
     grads_to_check = {}
-    if (stage_manager is None or stage_manager.is_first_stage()) and booster.plugin.zero_stage == 0:
-        if test_config["precision"] == "fp32":
-            atol, rtol = 1e-6, 1e-3
-        else:
-            atol, rtol = 4e-2, 4e-2
+    if test_config["precision"] == "fp32":
+        atol, rtol = 2e-4, 2e-4
+    else:
+        atol, rtol = 5e-3, 5e-3
+
+    if stage_manager is None or stage_manager.is_first_stage():
         row_layer_grads = get_grad_tensors_for_check(
-            opt_model, shard_opt_model, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0, verbose=False
+            whisper, sharded_whisper, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1
         )
         col_layer_grads = get_grad_tensors_for_check(
-            opt_model, shard_opt_model, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
+            whisper, sharded_whisper, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0
         )
         grads_to_check.update(col_layer_grads)
         grads_to_check.update(row_layer_grads)
 
     # optimizer executes step
     org_optimizer.step()
     sharded_optimizer.step()
 
     # check last hidden state & loss
     if stage_manager is None or stage_manager.is_last_stage():
         if test_config["precision"] == "fp32":
-            atol, rtol = 1e-5, 1e-3
+            atol, rtol = 2e-4, 2e-4
         else:
             atol, rtol = 5e-3, 5e-3
-        if org_model.__class__.__name__ == "OPTModel":
+
+        if org_model.__class__.__name__ == "WhisperModel":
             check_output_hidden_state(org_output, sharded_output, stage_manager, atol=atol, rtol=rtol)
 
         check_loss(org_loss, sharded_loss, atol=atol, rtol=rtol)
 
     # check weights
+    if test_config["precision"] == "fp32":
+        atol, rtol = 1e-3, 1e-3
+    else:
+        atol, rtol = 5e-3, 5e-3
     if stage_manager is None or stage_manager.is_first_stage():
-        if test_config["precision"] == "fp32":
-            atol, rtol = 1e-3, 1e-3
-        else:
-            atol, rtol = 5e-3, 5e-3
         check_weight(
-            opt_model, shard_opt_model, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
+            whisper, sharded_whisper, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=1, verbose=False
+        )
+        check_weight(
+            whisper, sharded_whisper, col_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0, verbose=False
         )
 
     # check grads
     check_all_grad_tensors(grads_to_check)
 
-    Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
+# TODO fix WhisperForConditionalGeneration enable jit fused operato
+# TODOjianghai) fix fp16
 @parameterize(
     "test_config",
     [
         {
             "tp_size": 2,
             "pp_size": 2,
-            "num_microbatches": 4,
+            "num_microbatches": 2,
+            "enable_metadata_cache": False,
             "enable_all_optimization": True,
-            "use_lazy_init": True,
-            "precision": "fp16",
+            "use_lazy_init": False,
+            "precision": "fp32",
             "initial_scale": 1,
         },
         {
             "tp_size": 1,
             "pp_size": 2,
             "num_microbatches": 4,
-            "enable_all_optimization": False,
+            "enable_metadata_cache": False,
             "use_lazy_init": False,
             "precision": "fp32",
+            "initial_scale": 1,
         },
-        {"tp_size": 4, "pp_size": 1, "enable_all_optimization": True, "use_lazy_init": False, "precision": "fp32"},
-        {"tp_size": 2, "pp_size": 1, "enable_all_optimization": True, "use_lazy_init": False, "precision": "fp32"},
         {
-            "tp_size": 2,
+            "tp_size": 4,
             "pp_size": 1,
             "enable_all_optimization": True,
-            "use_lazy_init": True,
-            "zero_stage": 2,
-            "precision": "fp16",
-            "initial_scale": 1,
+            "use_lazy_init": False,
+            "precision": "fp32",
         },
         {
             "tp_size": 1,
-            "pp_size": 2,
-            "num_microbatches": 2,
-            "enable_all_optimization": True,
-            "use_lazy_init": True,
-            "zero_stage": 1,
-            "precision": "fp16",
-            "initial_scale": 1,
+            "pp_size": 4,
+            "num_microbatches": 4,
+            "enable_metadata_cache": False,
+            "use_lazy_init": False,
+            "precision": "fp32",
         },
+        # whisper is not supported fp16 for now.
     ],
 )
-def run_opt_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_opt")
+def run_whisper_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_whisper")
     for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
+        if test_config["pp_size"] > 2 and name == "transformers_whisper_for_audio_classification":
+            continue
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
+    Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
 @parameterize(
     "test_config",
     [
         {
             "tp_size": 2,
             "pp_size": 2,
             "num_microbatches": 4,
+            "enable_metadata_cache": False,
             "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
             "initial_scale": 1,
         },
         {
             "tp_size": 2,
             "pp_size": 2,
-            "num_microbatches": 4,
+            "num_microbatches": 2,
+            "enable_metadata_cache": False,
             "enable_all_optimization": False,
             "use_lazy_init": False,
-            "precision": "fp16",
-            "zero_stage": 1,
+            "precision": "fp32",
             "initial_scale": 1,
         },
     ],
 )
-def run_opt_3d_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_opt")
+def run_whisper_3d_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_whisper")
 
     for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
     torch.cuda.empty_cache()
 
 
-def check_OPTModel(rank, world_size, port):
+def check_whisper(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_opt_test()
+    colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    run_whisper_test()
 
 
-def check_opt_3d(rank, world_size, port):
+def check_whisper_3d(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_opt_3d_test()
+    colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    run_whisper_3d_test()
 
 
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_OPTModel():
-    spawn(check_OPTModel, 4)
+def test_whisper():
+    spawn(check_whisper, 4)
 
 
 @pytest.mark.largedist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_opt_3d():
-    spawn(check_opt_3d, 8)
+def test_whisper_3d():
+    spawn(check_whisper_3d, 8)
 
 
 if __name__ == "__main__":
-    test_OPTModel()
-    test_opt_3d()
+    test_whisper()
+    test_whisper_3d()
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_sam.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_blip2.py`

 * *Files 13% similar despite different names*

```diff
@@ -15,58 +15,105 @@
 
 
 def check_forward_backward(org_model, sharded_model, data_gen_fn, output_transform_fn, loss_fn):
     # check forward
     org_output, org_loss, shard_output, shard_loss = run_forward(
         org_model, sharded_model, data_gen_fn, output_transform_fn, loss_fn
     )
-    assert_hf_output_close(org_output, shard_output, ignore_keys=["pred_masks"])
+    assert_hf_output_close(org_output, shard_output, ignore_keys=["past_key_values"])
 
     # do backward
     org_loss.backward()
     shard_loss.backward()
 
     assert torch.allclose(
         org_loss, shard_loss, atol=1e-5
     ), f"shard model loss is not equal to orgin model loss\n{org_loss}\n{shard_loss}"
 
     # check grad
 
-    sam = org_model
-    sharded_sam = sharded_model
+    blip2 = org_model
+    sharded_blip2 = sharded_model
 
     # check grad
-    col_layer_for_check = ["mask_decoder.transformer.layers[0].self_attn.q_proj", "vision_encoder.layers[0].mlp.lin1"]
-    row_layer_for_check = ["mask_decoder.transformer.layers[0].self_attn.out_proj", "vision_encoder.layers[0].mlp.lin2"]
-    check_grad(sam, sharded_sam, col_layer_for_check, atol=1e-5, rtol=1e-3, dim=0, verbose=False)
-    check_grad(sam, sharded_sam, row_layer_for_check, atol=1e-3, rtol=1e-3, dim=1, verbose=False)
+    col_layer_for_check = [
+        "vision_model.encoder.layers[0].self_attn.qkv",
+        "qformer.encoder.layer[0].attention.attention.query",
+        "language_model.model.decoder.layers[0].self_attn.k_proj",
+    ]
+    row_layer_for_check = [
+        "vision_model.encoder.layers[0].self_attn.projection",
+        "qformer.encoder.layer[0].attention.output.dense",
+        "language_model.model.decoder.layers[0].self_attn.out_proj",
+    ]
+    check_grad(
+        blip2,
+        sharded_blip2,
+        col_layer_for_check,
+        atol=1e-6,
+        rtol=1e-5,
+        dim=0,
+        verbose=False,
+    )
+    check_grad(
+        blip2,
+        sharded_blip2,
+        row_layer_for_check,
+        atol=1e-6,
+        rtol=1e-5,
+        dim=1,
+        verbose=False,
+    )
 
 
 @parameterize("enable_fused_normalization", [True, False])
 @parameterize("enable_tensor_parallelism", [True, False])
 @parameterize("enable_flash_attention", [True, False])
-def run_sam_test(enable_fused_normalization, enable_tensor_parallelism, enable_flash_attention):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_sam")
-    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
+@parameterize("enable_jit_fused", [True, False])
+def run_blip2_test(
+    enable_fused_normalization,
+    enable_tensor_parallelism,
+    enable_flash_attention,
+    enable_jit_fused,
+):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_blip2")
+    for name, (
+        model_fn,
+        data_gen_fn,
+        output_transform_fn,
+        loss_fn,
+        _,
+    ) in sub_model_zoo.items():
         org_model, sharded_model = build_model(
-            model_fn, enable_fused_normalization, enable_tensor_parallelism, enable_flash_attention
+            model_fn,
+            enable_fused_normalization,
+            enable_tensor_parallelism,
+            enable_flash_attention,
+            enable_jit_fused,
+            dtype=torch.float,
         )
         check_forward_backward(org_model, sharded_model, data_gen_fn, output_transform_fn, loss_fn)
 
     torch.cuda.empty_cache()
 
 
-def check_sam(rank, world_size, port):
+def check_blip2(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_sam_test()
+    colossalai.launch(
+        rank=rank,
+        world_size=world_size,
+        host="localhost",
+        port=port,
+        backend="nccl",
+    )
+    run_blip2_test()
 
 
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_sam():
-    spawn(check_sam, 2)
+def test_blip2():
+    spawn(check_blip2, 2)
 
 
 if __name__ == "__main__":
-    test_sam()
+    test_blip2()
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_shardformer/test_model/test_shard_t5.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_model/test_shard_gpt2.py`

 * *Files 20% similar despite different names*

```diff
@@ -21,203 +21,295 @@
 
 def check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config):
     org_model, org_optimizer, sharded_model, sharded_optimizer, criterion, booster = build_model_from_hybrid_plugin(
         model_fn, loss_fn, test_config
     )
 
     org_loss, org_output, sharded_loss, sharded_output = run_forward_backward_with_hybrid_plugin(
-        org_model, sharded_model, sharded_optimizer, data_gen_fn, output_transform_fn, criterion, booster
+        org_model,
+        sharded_model,
+        sharded_optimizer,
+        data_gen_fn,
+        output_transform_fn,
+        criterion,
+        booster,
     )
 
     stage_manager = booster.plugin.stage_manager
     tp_group = booster.plugin.tp_group
 
     # unwrap model
-    t5 = unwrap_model(org_model)
-    sharded_t5 = unwrap_model(sharded_model)
+    gpt2 = unwrap_model(org_model, "GPT2Model", "transformer")
+    sharded_gpt2 = unwrap_model(sharded_model, "GPT2Model", "transformer")
 
-    row_layer_for_check = ["shared", "encoder.block[0].layer[0].SelfAttention.q"]
+    norm_layer_for_check = ["h[0].ln_1", "h[0].ln_2"]
+    col_layer_for_check = ["h[0].mlp.c_fc"]
+    row_layer_for_check = ["wte", "h[0].mlp.c_proj"]
 
-    # Save gradient tensors for comparison between the original model and the sharded model before optimizer step.
+    # Save gradient tensors for comparison between the original model and the sharded model.
     grads_to_check = {}
-    if test_config["precision"] == "fp32":
-        atol, rtol = 1e-5, 1e-3
-    else:
-        atol, rtol = 5e-3, 5e-3
     if (stage_manager is None or stage_manager.is_first_stage()) and booster.plugin.zero_stage == 0:
+        if test_config["precision"] == "fp32":
+            atol, rtol = 1e-4, 1e-3
+        else:
+            atol, rtol = 5e-3, 5e-3
+        col_layer_grads = get_grad_tensors_for_check(
+            gpt2,
+            sharded_gpt2,
+            col_layer_for_check,
+            tp_group,
+            atol=atol,
+            rtol=rtol,
+            dim=1,
+            verbose=False,
+        )
         row_layer_grads = get_grad_tensors_for_check(
-            t5, sharded_t5, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0
+            gpt2,
+            sharded_gpt2,
+            row_layer_for_check,
+            tp_group,
+            atol=atol,
+            rtol=rtol,
+            dim=0,
+            verbose=False,
         )
+
+        norm_layer_grads = get_grad_tensors_for_check(
+            gpt2,
+            sharded_gpt2,
+            norm_layer_for_check,
+            tp_group,
+            atol=atol,
+            rtol=rtol,
+            dim=1,
+            verbose=False,
+        )
+
+        grads_to_check.update(col_layer_grads)
         grads_to_check.update(row_layer_grads)
+        grads_to_check.update(norm_layer_grads)
 
     # optimizer executes step
     org_optimizer.step()
     sharded_optimizer.step()
 
     # check last hidden state & loss
     if stage_manager is None or stage_manager.is_last_stage():
         if test_config["precision"] == "fp32":
             atol, rtol = 1e-5, 1e-3
         else:
             atol, rtol = 5e-3, 5e-3
 
-        if org_model.__class__.__name__ != "T5ForConditionalGeneration":
+        if org_model.__class__.__name__ == "GPT2Model":
             check_output_hidden_state(org_output, sharded_output, stage_manager, atol=atol, rtol=rtol)
 
         check_loss(org_loss, sharded_loss, atol=atol, rtol=rtol)
 
     # check weights
-    if test_config["precision"] == "fp32":
-        atol, rtol = 5e-4, 1e-3
-    else:
-        atol, rtol = 5e-3, 5e-3
     if stage_manager is None or stage_manager.is_first_stage():
-        check_weight(t5, sharded_t5, row_layer_for_check, tp_group, atol=atol, rtol=rtol, dim=0, verbose=False)
+        if test_config["precision"] == "fp32":
+            atol, rtol = 5e-3, 1e-3
+        else:
+            atol, rtol = 5e-3, 5e-3
+        check_weight(
+            gpt2,
+            sharded_gpt2,
+            col_layer_for_check,
+            tp_group,
+            atol=atol,
+            rtol=rtol,
+            dim=1,
+            verbose=False,
+        )
 
     # check grads
     check_all_grad_tensors(grads_to_check)
 
+    Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
 @parameterize(
     "test_config",
     [
         {
+            "tp_size": 4,
+            "pp_size": 1,
+            "num_microbatches": 1,
+            "enable_sequence_parallelism": True,
+            "sequence_parallelism_mode": "ring",
+            "enable_flash_attention": False,
+            "use_lazy_init": True,
+            "precision": "fp32",
+            "initial_scale": 1,
+        },
+        {
+            "tp_size": 4,
+            "pp_size": 1,
+            "num_microbatches": 1,
+            "enable_sequence_parallelism": True,
+            "sequence_parallelism_mode": "split_gather",
+            "enable_flash_attention": False,
+            "use_lazy_init": True,
+            "precision": "fp16",
+            "initial_scale": 1,
+        },
+        {
             "tp_size": 2,
             "pp_size": 2,
-            "num_microbatches": 2,
-            "enable_metadata_cache": False,
+            "num_microbatches": 4,
             "enable_all_optimization": True,
             "use_lazy_init": True,
             "precision": "fp16",
             "initial_scale": 1,
         },
         {
             "tp_size": 1,
             "pp_size": 2,
             "num_microbatches": 4,
-            "enable_metadata_cache": False,
-            "use_lazy_init": False,
+            "enable_all_optimization": True,
+            "use_lazy_init": True,
             "precision": "fp16",
             "initial_scale": 1,
         },
         {
             "tp_size": 4,
             "pp_size": 1,
-            "enable_all_optimization": True,
+            "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
         },
         {
-            "tp_size": 1,
-            "pp_size": 4,
-            "num_microbatches": 4,
-            "enable_metadata_cache": False,
+            "tp_size": 2,
+            "pp_size": 1,
             "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
         },
-        {"tp_size": 2, "pp_size": 1, "enable_all_optimization": True, "use_lazy_init": False, "precision": "fp32"},
+        {
+            "tp_size": 2,
+            "pp_size": 2,
+            "num_microbatches": 4,
+            "enable_all_optimization": False,
+            "use_lazy_init": True,
+            "precision": "fp32",
+        },
         {
             "tp_size": 2,
             "pp_size": 1,
             "enable_all_optimization": True,
             "use_lazy_init": True,
             "zero_stage": 2,
             "precision": "fp16",
             "initial_scale": 1,
         },
         {
             "tp_size": 1,
             "pp_size": 2,
             "num_microbatches": 2,
-            "enable_metadata_cache": False,
             "enable_all_optimization": True,
             "use_lazy_init": True,
             "zero_stage": 1,
             "precision": "fp16",
             "initial_scale": 1,
         },
     ],
 )
 @clear_cache_before_run()
-def run_t5_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_t5")
-
-    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
-        # skip 4-stage pp test for t5_encoder
-        if test_config["pp_size"] > 2 and name == "transformers_t5_encoder_model":
-            continue
+def run_gpt2_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_gpt", exclude="transformers_gptj")
 
+    for name, (
+        model_fn,
+        data_gen_fn,
+        output_transform_fn,
+        loss_fn,
+        _,
+    ) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
-    Randomizer.reset_index()
     torch.cuda.empty_cache()
 
 
 @parameterize(
     "test_config",
     [
         {
             "tp_size": 2,
             "pp_size": 2,
             "num_microbatches": 4,
-            "enable_metadata_cache": False,
             "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp32",
             "initial_scale": 1,
         },
         {
             "tp_size": 2,
             "pp_size": 2,
             "num_microbatches": 4,
-            "enable_metadata_cache": False,
             "enable_all_optimization": False,
             "use_lazy_init": False,
             "precision": "fp16",
             "zero_stage": 1,
             "initial_scale": 1,
         },
     ],
 )
-def run_t5_3d_test(test_config):
-    sub_model_zoo = model_zoo.get_sub_registry("transformers_t5")
+@clear_cache_before_run()
+def run_gpt2_3d_test(test_config):
+    sub_model_zoo = model_zoo.get_sub_registry("transformers_gpt", exclude="transformers_gptj")
 
-    for name, (model_fn, data_gen_fn, output_transform_fn, loss_fn, _) in sub_model_zoo.items():
+    for name, (
+        model_fn,
+        data_gen_fn,
+        output_transform_fn,
+        loss_fn,
+        _,
+    ) in sub_model_zoo.items():
         check_forward_backward(model_fn, data_gen_fn, output_transform_fn, loss_fn, test_config)
 
     clear_layout_converter()
     torch.cuda.empty_cache()
 
 
-def check_t5(rank, world_size, port):
+def check_gpt2(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_t5_test()
+    colossalai.launch(
+        rank=rank,
+        world_size=world_size,
+        host="localhost",
+        port=port,
+        backend="nccl",
+    )
+    run_gpt2_test()
 
 
-def check_t5_3d(rank, world_size, port):
+def check_gpt2_3d(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
-    run_t5_3d_test()
+    colossalai.launch(
+        rank=rank,
+        world_size=world_size,
+        host="localhost",
+        port=port,
+        backend="nccl",
+    )
+    run_gpt2_3d_test()
 
 
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_t5():
-    spawn(check_t5, 4)
+def test_gpt2():
+    spawn(check_gpt2, 4)
 
 
 @pytest.mark.largedist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
-def test_t5_3d():
-    spawn(check_t5_3d, 8)
+def test_gpt2_3d():
+    spawn(check_gpt2_3d, 8)
 
 
 if __name__ == "__main__":
-    test_t5()
-    test_t5_3d()
+    test_gpt2()
+    test_gpt2_3d()
```

### Comparing `colossalai-nightly-2024.3.9/tests/test_shardformer/test_shard_utils.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_shard_utils.py`

 * *Files identical despite different names*

### Comparing `colossalai-nightly-2024.3.9/tests/test_shardformer/test_with_torch_ddp.py` & `colossalai-nightly-2024.5.4/tests/test_shardformer/test_with_torch_ddp.py`

 * *Files 2% similar despite different names*

```diff
@@ -67,15 +67,15 @@
         # backward
         loss.backward()
         torch.cuda.empty_cache()
 
 
 def run_dist(rank, world_size, port):
     disable_existing_loggers()
-    colossalai.launch(config={}, rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
+    colossalai.launch(rank=rank, world_size=world_size, host="localhost", port=port, backend="nccl")
     check_shardformer_with_ddp()
 
 
 @pytest.mark.dist
 @rerun_if_address_is_in_use()
 @clear_cache_before_run()
 def test_gpt2():
```

